import os
import random 
import gradio as gr
import numpy as np
import time
import torch, torchaudio
import gc
import warnings
warnings.filterwarnings('ignore')
from zhconv import convert
from LLM import LLM
from TTS import EdgeTTS
from src.cost_time import calculate_time

from configs import *
os.environ["GRADIO_TEMP_DIR"]= './temp'
os.environ["WEBUI"] = "true"
def get_title(title = 'Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker)'):
    description = f"""
    <p style="text-align: center; font-weight: bold;">
        <span style="font-size: 28px;">{title}</span>
        <br>
        <span style="font-size: 18px;" id="paper-info">
            [<a href="https://zhuanlan.zhihu.com/p/671006998" target="_blank">çŸ¥ä¹</a>]
            [<a href="https://www.bilibili.com/video/BV1rN4y1a76x/" target="_blank">bilibili</a>]
            [<a href="https://github.com/Kedreamix/Linly-Talker" target="_blank">GitHub</a>]
            [<a herf="https://kedreamix.github.io/" target="_blank">ä¸ªäººä¸»é¡µ</a>]
        </span>
        <br> 
        <span>Linly-Talkeræ˜¯ä¸€æ¬¾åˆ›æ–°çš„æ•°å­—äººå¯¹è¯ç³»ç»Ÿï¼Œå®ƒèåˆäº†æœ€æ–°çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ğŸ¤–ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ğŸ™ï¸ã€æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆTTSï¼‰ğŸ—£ï¸å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯ğŸ¤ã€‚</span>
    </p>
    """
    return description

# Default system and prompt settings
DEFAULT_SYSTEM = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'
PREFIX_PROMPT = 'è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'
# Default parameters
IMAGE_SIZE = 256
PREPROCESS_TYPE = 'crop'
FACERENDER = 'facevid2vid'
ENHANCER = False
IS_STILL_MODE = False
EXP_WEIGHT = 1
USE_REF_VIDEO = False
REF_VIDEO = None
REF_INFO = 'pose'
USE_IDLE_MODE = False
AUDIO_LENGTH = 5

edgetts = EdgeTTS()

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        gr.Warning("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
    return question

def clear_memory():
    """
    æ¸…ç†PyTorchçš„æ˜¾å­˜å’Œç³»ç»Ÿå†…å­˜ç¼“å­˜ã€‚
    """
    # 1. æ¸…ç†ç¼“å­˜çš„å˜é‡
    gc.collect()  # è§¦å‘Pythonåƒåœ¾å›æ”¶
    torch.cuda.empty_cache()  # æ¸…ç†PyTorchçš„æ˜¾å­˜ç¼“å­˜
    torch.cuda.ipc_collect()  # æ¸…ç†PyTorchçš„è·¨è¿›ç¨‹é€šä¿¡ç¼“å­˜
    
    # 2. æ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼‰
    print(f"Memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB")
    print(f"Max memory allocated: {torch.cuda.max_memory_allocated() / (1024 ** 2):.2f} MB")
    print(f"Cached memory: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB")
    print(f"Max cached memory: {torch.cuda.max_memory_reserved() / (1024 ** 2):.2f} MB")

def generate_seed():
    seed = random.randint(1, 100000000)
    return {"__type__": "update", "value": seed}

def set_all_random_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def change_instruction(mode):
    return instruct_dict.get(mode, 'æœªçŸ¥æ¨¡å¼')

PROMPT_SR, TARGET_SR = 16000, 22050
DEFAULT_DATA = np.zeros(TARGET_SR)

@calculate_time
def TTS_response(text, voice, rate, volume, pitch, am, voc, lang, male,
                ref_audio, prompt_text, prompt_language, text_language,
                cut_method, question_audio, question, use_mic_voice,
                mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, 
                tts_method='Edge-TTS', save_path='answer.wav'):
    if text == '':
        text = 'è¯·è¾“å…¥æ–‡å­—/é—®é¢˜'
    if tts_method == 'Edge-TTS':
        if not edgetts.network:
            gr.Warning("è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚PaddleTTS")
            return None
        try:
            edgetts.predict(text, voice, rate, volume, pitch, save_path, 'answer.vtt')
        except Exception as e:
            os.system(f'edge-tts --text "{text}" --voice {voice} --write-media {save_path} --write-subtitles answer.vtt')
        return save_path
    
    if tts_method == 'PaddleTTS':
        tts.predict(text, am, voc, lang=lang, male=male, save_path=save_path)
        return save_path
    
    if tts_method == 'GPT-SoVITSå…‹éš†å£°éŸ³':
        try:
            vits.predict(ref_wav_path=question_audio if use_mic_voice else ref_audio,
                         prompt_text=question if use_mic_voice else prompt_text,
                         prompt_language=prompt_language,
                         text=text,
                         text_language=text_language,
                         how_to_cut=cut_method,
                         save_path=save_path)
            return save_path
        except Exception as e:
            gr.Warning("æ— å…‹éš†ç¯å¢ƒæˆ–æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
            return None
    elif "CosyVoice" in tts_method:
        if prompt_wav_upload is not None:
            prompt_wav = prompt_wav_upload
        elif prompt_wav_record is not None:
            prompt_wav = prompt_wav_record
        else:
            prompt_wav = None
        if mode_checkbox_group in ['è·¨è¯­ç§å¤åˆ»']:
            if prompt_wav is None:
                gr.Warning('æ‚¨æ­£åœ¨ä½¿ç”¨è·¨è¯­ç§å¤åˆ»æ¨¡å¼, è¯·æä¾›promptéŸ³é¢‘')
                return (TARGET_SR, DEFAULT_DATA)
            gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨è·¨è¯­ç§å¤åˆ»æ¨¡å¼, è¯·ç¡®ä¿åˆæˆæ–‡æœ¬å’Œpromptæ–‡æœ¬ä¸ºä¸åŒè¯­è¨€')
        # if in zero_shot cross_lingual, please make sure that prompt_text and prompt_wav meets requirements
        if mode_checkbox_group in ['3sæé€Ÿå¤åˆ»', 'è·¨è¯­ç§å¤åˆ»']:
            if prompt_wav is None:
                gr.Warning('promptéŸ³é¢‘ä¸ºç©ºï¼Œæ‚¨æ˜¯å¦å¿˜è®°è¾“å…¥promptéŸ³é¢‘ï¼Ÿ')
                return (TARGET_SR, DEFAULT_DATA)
            if torchaudio.info(prompt_wav).sample_rate < PROMPT_SR:
                gr.Warning('promptéŸ³é¢‘é‡‡æ ·ç‡{}ä½äº{}'.format(torchaudio.info(prompt_wav).sample_rate, PROMPT_SR))
                return (TARGET_SR, DEFAULT_DATA)
        # sft mode only use sft_dropdown
        if mode_checkbox_group in ['é¢„è®­ç»ƒéŸ³è‰²']:
            if prompt_wav is not None or prompt_text_cv != '':
                gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨é¢„è®­ç»ƒéŸ³è‰²æ¨¡å¼ï¼Œpromptæ–‡æœ¬/promptéŸ³é¢‘/instructæ–‡æœ¬ä¼šè¢«å¿½ç•¥ï¼')
        # zero_shot mode only use prompt_wav prompt text
        if mode_checkbox_group in ['3sæé€Ÿå¤åˆ»']:
            if prompt_text_cv == '':
                gr.Warning('promptæ–‡æœ¬ä¸ºç©ºï¼Œæ‚¨æ˜¯å¦å¿˜è®°è¾“å…¥promptæ–‡æœ¬ï¼Ÿ')
                return (TARGET_SR, DEFAULT_DATA)
            # if instruct_text != '':
            #     gr.Info('æ‚¨æ­£åœ¨ä½¿ç”¨3sæé€Ÿå¤åˆ»æ¨¡å¼ï¼Œé¢„è®­ç»ƒéŸ³è‰²/instructæ–‡æœ¬ä¼šè¢«å¿½ç•¥ï¼')

        if mode_checkbox_group == 'é¢„è®­ç»ƒéŸ³è‰²':
            set_all_random_seed(seed)
            output = cosyvoice.predict_sft(text, sft_dropdown, speed_factor=speed_factor, save_path=save_path)
        elif mode_checkbox_group == '3sæé€Ÿå¤åˆ»':
            set_all_random_seed(seed)
            output = cosyvoice.predict_zero_shot(text, prompt_text_cv, prompt_wav, speed_factor=speed_factor, save_path=save_path)
        elif mode_checkbox_group == 'è·¨è¯­ç§å¤åˆ»':
            set_all_random_seed(seed)
            output = cosyvoice.predict_cross_lingual(text, prompt_wav, speed_factor=speed_factor, save_path=save_path)
        return output
    else:
        gr.Warning('æœªçŸ¥æ¨¡å‹')
    return None

inference_mode_list = ['é¢„è®­ç»ƒéŸ³è‰²', '3sæé€Ÿå¤åˆ»', 'è·¨è¯­ç§å¤åˆ»']
instruct_dict = {'é¢„è®­ç»ƒéŸ³è‰²': '1. é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²\n2. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                '3sæé€Ÿå¤åˆ»': '1. é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–å½•å…¥promptéŸ³é¢‘ï¼Œæ³¨æ„ä¸è¶…è¿‡30sï¼Œè‹¥åŒæ—¶æä¾›ï¼Œä¼˜å…ˆé€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶\n2. è¾“å…¥promptæ–‡æœ¬\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                'è·¨è¯­ç§å¤åˆ»': '1. é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–å½•å…¥promptéŸ³é¢‘ï¼Œæ³¨æ„ä¸è¶…è¿‡30sï¼Œè‹¥åŒæ—¶æä¾›ï¼Œä¼˜å…ˆé€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶\n2. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                'è‡ªç„¶è¯­è¨€æ§åˆ¶': '1. é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²\n2. è¾“å…¥instructæ–‡æœ¬\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®'}


@calculate_time
def LLM_response(
    question_audio, question,  # è¾“å…¥çš„éŸ³é¢‘å’Œæ–‡æœ¬é—®é¢˜
    voice, rate, volume, pitch,  # è¯­éŸ³åˆæˆå‚æ•°
    am, voc, lang, male,  # TTS æ¨¡å‹å‚æ•°
    ref_audio, prompt_text, prompt_language, text_language,  # æç¤ºéŸ³é¢‘ã€æ–‡æœ¬åŠå…¶è¯­è¨€è®¾ç½®
    cut_method, use_mic_voice, mode_checkbox_group, sft_dropdown,  # å…¶ä»–TTSé€‰é¡¹
    prompt_text_cv, prompt_wav_upload, prompt_wav_record,  # æç¤ºä¿¡æ¯å’ŒéŸ³é¢‘é€‰é¡¹
    seed, speed_factor,  # éšæœºç§å­å’Œè¯­é€Ÿå› å­
    tts_method='Edge-TTS'  # TTS æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨ 'Edge-TTS'
):
    if len(question) == 0:
        gr.Warning("è¯·è¾“å…¥é—®é¢˜")
        return None, None, None

    # ç”Ÿæˆå›ç­”
    answer = llm.generate(question, DEFAULT_SYSTEM)
    print("LLM å›å¤ï¼š", answer)

    # åˆæˆå›ç­”è¯­éŸ³
    tts_audio = TTS_response(
        answer, voice, rate, volume, pitch, am, voc, lang, male,
        ref_audio, prompt_text, prompt_language, text_language, 
        cut_method, question_audio, question, use_mic_voice, 
        mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, 
        prompt_wav_record, seed, speed_factor, tts_method
    )

    # ç”ŸæˆVTTæ–‡ä»¶ï¼ˆå¦‚æœTTSæ–¹æ³•ä¸º'Edge-TTS'ï¼‰
    tts_vtt = 'answer.vtt' if tts_method == 'Edge-TTS' else None
    tts_vtt = None
    return tts_audio, tts_vtt, answer

@calculate_time
def Talker_response_img(question_audio, method, text, voice, rate, volume, pitch,
                        am, voc, lang, male, inp_ref, prompt_text, prompt_language,
                        text_language, how_to_cut, use_mic_voice, 
                        mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, 
                        tts_method, source_image, preprocess_type, is_still_mode, enhancer,
                        batch_size, size_of_image, pose_style, facerender,
                        exp_weight, blink_every, fps, progress=gr.Progress(track_tqdm=True)):

    if enhancer:
        gr.Warning("è¯·å…ˆå®‰è£…GFPGANåº“ (pip install gfpgan)ï¼Œå·²å®‰è£…å¯å¿½ç•¥")

    if not voice:
        gr.Warning("è¯·é€‰æ‹©å£°éŸ³")
        return None
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, voice, rate, volume, pitch,
                                            am, voc, lang, male, inp_ref, prompt_text, prompt_language,
                                            text_language, how_to_cut, use_mic_voice, 
                                            mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, tts_method)

    if driven_audio is None:
        gr.Warning("éŸ³é¢‘æ²¡æœ‰æ­£å¸¸ç”Ÿæˆï¼Œè¯·æ£€æŸ¥TTSæ˜¯å¦æ­£ç¡®")
        return None

    # è§†é¢‘ç”Ÿæˆ
    video = None
    if method == 'SadTalker':
        video = talker.test2(source_image, driven_audio, preprocess_type, is_still_mode, enhancer,
                             batch_size, size_of_image, pose_style, facerender, exp_weight,
                             REF_VIDEO, REF_INFO, USE_IDLE_MODE, AUDIO_LENGTH, blink_every, 
                             fps=fps)
    elif method == 'Wav2Lip':
        video = talker.predict(source_image, driven_audio, batch_size)
    elif method == 'Wav2Lipv2':
        video = talker.run(source_image, driven_audio, batch_size)
    elif method == 'NeRFTalk':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + method)
        return None

    return (video, driven_vtt) if driven_vtt else video

def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history

def modify_system_session(system: str) -> str:
    if system is None or len(system) == 0:
        system = DEFAULT_SYSTEM
    llm.clear_history()
    return system, system, []

def clear_session():
    # clear history
    llm.clear_history()
    return '', []


def human_response(source_image, history, question_audio, talker_method, voice, rate, volume, pitch,
                   am, voc, lang, male, inp_ref, prompt_text, prompt_language, text_language, cut_method, use_mic_voice, 
                   mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, 
                   tts_method, character, preprocess_type, is_still_mode,
                   enhancer, batch_size, size_of_image, pose_style, facerender, exp_weight,
                   blink_every, fps=20, progress=gr.Progress(track_tqdm=True)):
    response = history[-1][1]
    question = history[-1][0]

    # è§’è‰²ä¿¡æ¯è®¾ç½®
    if character == 'å¥³æ€§è§’è‰²':
        source_image = pic_path = crop_pic_path = first_coeff_path = r'./inputs/girl.png'
        crop_info = ((403, 403), (19, 30, 502, 513), [40.06, 40.17, 443.79, 443.90])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        source_image = pic_path = crop_pic_path = first_coeff_path = r'./inputs/boy.png'
        crop_info = ((876, 747), (0, 0, 886, 838), [10.38, 0, 886, 747.71])
        default_voice = 'zh-CN-YunyangNeural'
    elif character == 'è‡ªå®šä¹‰è§’è‰²':
        if source_image is None:
            gr.Error("è‡ªå®šä¹‰è§’è‰²éœ€è¦ä¸Šä¼ æ­£ç¡®çš„å›¾ç‰‡")
            return None
        default_voice = 'zh-CN-XiaoxiaoNeural'
    else:
        gr.Error("æœªçŸ¥è§’è‰²")
        return None

    voice = default_voice if not voice else voice

    # TTSå“åº”ç”Ÿæˆ
    driven_audio = TTS_response(response, voice, rate, volume, pitch, am, voc, lang, male,
                                            inp_ref, prompt_text, prompt_language, text_language,
                                            cut_method, question_audio, question, use_mic_voice, 
                                            mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor,tts_method)
    driven_vtt = 'answer.vtt' if tts_method == 'Edge-TTS' else None
    driven_vtt = None
    if driven_audio is None:
        gr.Warning("éŸ³é¢‘æ²¡æœ‰æ­£å¸¸ç”Ÿæˆï¼Œè¯·æ£€æŸ¥TTSæ˜¯å¦æ­£ç¡®")
        return None

    # è§†é¢‘ç”Ÿæˆ
    video = None
    if talker_method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test2(source_image, driven_audio, preprocess_type, is_still_mode, enhancer,
                        batch_size, size_of_image, pose_style, facerender, exp_weight,
                        REF_VIDEO, REF_INFO, USE_IDLE_MODE, AUDIO_LENGTH, blink_every, 
                        fps=fps)
    elif talker_method == 'Wav2Lip':
        video = talker.predict(crop_pic_path, driven_audio, batch_size, enhancer)
    elif talker_method == 'Wav2Lipv2':
        video = talker.run(crop_pic_path, driven_audio, batch_size, enhancer)
    elif talker_method == 'NeRFTalk':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + talker_method)
        return None

    return video, driven_vtt if driven_vtt else video


@calculate_time
def MuseTalker_response(source_video, bbox_shift, question_audio, text, voice,
                        rate, volume, pitch, am, voc, lang, male, 
                        ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice,
                        mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor,  
                        tts_method='Edge-TTS', batch_size=4, progress=gr.Progress(track_tqdm=True)):
    default_voice = None
    voice = default_voice if not voice else voice

    if not voice:
        gr.Warning('è¯·é€‰æ‹©å£°éŸ³')
        return None

    # LLMå“åº”ç”Ÿæˆ
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, voice, rate, volume, pitch,
                                               am, voc, lang, male, ref_audio, prompt_text, prompt_language,
                                               text_language, cut_method, use_mic_voice, 
                                               mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, 
                                               tts_method)

    if driven_audio is None:
        gr.Warning("éŸ³é¢‘æ²¡æœ‰æ­£å¸¸ç”Ÿæˆï¼Œè¯·æ£€æŸ¥TTSæ˜¯å¦æ­£ç¡®")
        return None

    # MuseTalker è§†é¢‘ç”Ÿæˆ
    video = musetalker.inference_noprepare(driven_audio, source_video, bbox_shift, batch_size, fps=25)

    return (video, driven_vtt) if driven_vtt else video

GPT_SoVITS_ckpt = "GPT_SoVITS/pretrained_models"
def load_vits_model(gpt_path, sovits_path, progress=gr.Progress(track_tqdm=True)):
    global vits
    print("æ¨¡å‹åŠ è½½ä¸­...", gpt_path, sovits_path)
    all_gpt_path = os.path.join(GPT_SoVITS_ckpt, gpt_path)
    all_sovits_path = os.path.join(GPT_SoVITS_ckpt, sovits_path)
    vits.load_model(all_gpt_path, all_sovits_path)
    gr.Info("æ¨¡å‹åŠ è½½æˆåŠŸ")
    return gpt_path, sovits_path

def character_change(character):
    if character == 'å¥³æ€§è§’è‰²':
        return r'./inputs/girl.png'
    elif character == 'ç”·æ€§è§’è‰²':
        return r'./inputs/boy.png'
    elif character == 'è‡ªå®šä¹‰è§’è‰²':
        return None
    else:
        gr.Warning("ä¸æ”¯æŒçš„è§’è‰²ç±»å‹ï¼š" + character)
        return None

def webui_setting(talk=False):
    if not talk:
        with gr.Tabs():
            with gr.TabItem('æ•°å­—äººå½¢è±¡è®¾å®š'):
                source_image = gr.Image(label="Source image", type="filepath")
    else:
        source_image = None
    with gr.Tabs("TTS Method"):
        with gr.Accordion("TTS Methodè¯­éŸ³æ–¹æ³•è°ƒèŠ‚ ", open=True):
            with gr.Tab("Edge-TTS"):
                voice = gr.Dropdown(edgetts.SUPPORTED_VOICE, value='zh-CN-XiaoxiaoNeural', label="Voice å£°éŸ³é€‰æ‹©")
                rate = gr.Slider(minimum=-100, maximum=100, value=0, step=1.0, label='Rate é€Ÿç‡')
                volume = gr.Slider(minimum=0, maximum=100, value=100, step=1, label='Volume éŸ³é‡')
                pitch = gr.Slider(minimum=-100, maximum=100, value=0, step=1, label='Pitch éŸ³è°ƒ')
            with gr.Tab("PaddleTTS"):
                am = gr.Dropdown(["FastSpeech2"], label="å£°å­¦æ¨¡å‹é€‰æ‹©", value='FastSpeech2')
                voc = gr.Dropdown(["PWGan", "HifiGan"], label="å£°ç å™¨é€‰æ‹©", value='PWGan')
                lang = gr.Dropdown(["zh", "en", "mix", "canton"], label="è¯­è¨€é€‰æ‹©", value='zh')
                male = gr.Checkbox(label="ç”·å£°(Male)", value=False)
            with gr.Tab('GPT-SoVITS'):
                with gr.Row():
                    gpt_path = gr.FileExplorer(root=GPT_SoVITS_ckpt, glob="*.ckpt", value="s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt", file_count='single', label="GPTæ¨¡å‹è·¯å¾„")
                    sovits_path = gr.FileExplorer(root=GPT_SoVITS_ckpt, glob="*.pth", value="s2G488k.pth", file_count='single', label="SoVITSæ¨¡å‹è·¯å¾„")
                button = gr.Button("åŠ è½½æ¨¡å‹")
                button.click(fn=load_vits_model, inputs=[gpt_path, sovits_path], outputs=[gpt_path, sovits_path])
                with gr.Row():
                    ref_audio = gr.Audio(label="è¯·ä¸Šä¼ 3~10ç§’å†…å‚è€ƒéŸ³é¢‘ï¼Œè¶…è¿‡ä¼šæŠ¥é”™ï¼", sources=["microphone", "upload"], type="filepath")
                    use_mic_voice = gr.Checkbox(label="ä½¿ç”¨è¯­éŸ³é—®ç­”çš„éº¦å…‹é£")
                    prompt_text = gr.Textbox(label="å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬", value="")
                    prompt_language = gr.Dropdown(label="å‚è€ƒéŸ³é¢‘çš„è¯­ç§", choices=["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡"], value="ä¸­æ–‡")
                asr_button = gr.Button("è¯­éŸ³è¯†åˆ« - å…‹éš†å‚è€ƒéŸ³é¢‘")
                asr_button.click(fn=Asr, inputs=[ref_audio], outputs=[prompt_text])
                with gr.Row():
                    text_language = gr.Dropdown(label="éœ€è¦åˆæˆçš„è¯­ç§", choices=["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "ä¸­è‹±æ··åˆ", "æ—¥è‹±æ··åˆ", "å¤šè¯­ç§æ··åˆ"], value="ä¸­æ–‡")
                    cut_method = gr.Dropdown(label="æ€ä¹ˆåˆ‡", choices=["ä¸åˆ‡", "å‡‘å››å¥ä¸€åˆ‡", "å‡‘50å­—ä¸€åˆ‡", "æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡", "æŒ‰è‹±æ–‡å¥å·.åˆ‡", "æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡"], value="å‡‘å››å¥ä¸€åˆ‡", interactive=True)
            
            with gr.Tab('CosyVoice'):
                # tts_text = gr.Textbox(label="è¾“å…¥åˆæˆæ–‡æœ¬", lines=1, value="æˆ‘æ˜¯é€šä¹‰å®éªŒå®¤è¯­éŸ³å›¢é˜Ÿå…¨æ–°æ¨å‡ºçš„ç”Ÿæˆå¼è¯­éŸ³å¤§æ¨¡å‹ï¼Œæä¾›èˆ’é€‚è‡ªç„¶çš„è¯­éŸ³åˆæˆèƒ½åŠ›ã€‚")
                speed_factor = gr.Slider(minimum=0.25, maximum=4, step=0.05, label="è¯­é€Ÿè°ƒèŠ‚", value=1.0, interactive=True)
                with gr.Row():
                    mode_checkbox_group = gr.Radio(choices=inference_mode_list, label='é€‰æ‹©æ¨ç†æ¨¡å¼', value=inference_mode_list[0])
                    instruction_text = gr.Text(label="æ“ä½œæ­¥éª¤", lines=3, value=instruct_dict[inference_mode_list[0]], scale=0.5)
                    sft_dropdown = gr.Dropdown(choices=['ä¸­æ–‡å¥³', 'ä¸­æ–‡ç”·', 'æ—¥è¯­ç”·', 'ç²¤è¯­å¥³', 'è‹±æ–‡å¥³', 'è‹±æ–‡ç”·', 'éŸ©è¯­å¥³'], label='é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²', value="ä¸­æ–‡å¥³", scale=0.25)
                with gr.Row():
                    seed_button = gr.Button(value="\U0001F3B2")
                    seed = gr.Number(value=0, label="éšæœºæ¨ç†ç§å­")
                with gr.Row():
                    prompt_wav_upload = gr.Audio(sources='upload', type='filepath', label='é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæ³¨æ„é‡‡æ ·ç‡ä¸ä½äº16khz')
                    prompt_wav_record = gr.Audio(sources='microphone', type='filepath', label='å½•åˆ¶promptéŸ³é¢‘æ–‡ä»¶')
                prompt_text_cv = gr.Textbox(label="è¾“å…¥promptæ–‡æœ¬", lines=1, placeholder="è¯·è¾“å…¥promptæ–‡æœ¬ï¼Œéœ€ä¸promptéŸ³é¢‘å†…å®¹ä¸€è‡´ï¼Œæš‚æ—¶ä¸æ”¯æŒè‡ªåŠ¨è¯†åˆ«...", value='')
                # instruct_text = gr.Textbox(label="è¾“å…¥instructæ–‡æœ¬", lines=1, placeholder="è¯·è¾“å…¥instructæ–‡æœ¬.", value='')
                seed_button.click(generate_seed, inputs=[], outputs=seed)
                mode_checkbox_group.change(fn=change_instruction, inputs=[mode_checkbox_group], outputs=[instruction_text])
            generate_button = gr.Button("ç”ŸæˆéŸ³é¢‘")
            audio_output = gr.Audio(label="åˆæˆéŸ³é¢‘")
            
            with gr.Column(variant='panel'):
                batch_size = gr.Slider(minimum=1, maximum=10, value=2, step=1, label='Talker Batch size')
    if not talk:
        character = gr.Radio(['å¥³æ€§è§’è‰²', 'ç”·æ€§è§’è‰²', 'è‡ªå®šä¹‰è§’è‰²'], label="è§’è‰²é€‰æ‹©", value='è‡ªå®šä¹‰è§’è‰²')
        character.change(fn=character_change, inputs=[character], outputs=[source_image])
        talker_method = gr.Radio(choices=['SadTalker', 'Wav2Lip', 'Wav2Lipv2', 'NeRFTalk', 'Comming Soon!!!'], value='SadTalker', label='æ•°å­—äººæ¨¡å‹é€‰æ‹©')
        talker_method.change(fn=talker_model_change, inputs=[talker_method], outputs=[talker_method])
    else:
        character = None
        talker_method = None
    tts_method = gr.Radio(['Edge-TTS', 'PaddleTTS', 'GPT-SoVITSå…‹éš†å£°éŸ³', 'CosyVoice-SFTæ¨¡å¼', 'CosyVoice-å…‹éš†ç¿»è¯‘æ¨¡å¼', 'Comming Soon!!!'], label="Text To Speech Method", value='Edge-TTS')
    tts_method.change(fn=tts_model_change, inputs=[tts_method], outputs=[tts_method])
    asr_method = gr.Radio(choices=['Whisper-tiny', 'Whisper-base', 'FunASR', 'OmniSenseVoice-quantize', 'OmniSenseVoice', 'Comming Soon!!!'], value='Whisper-base', label='è¯­éŸ³è¯†åˆ«æ¨¡å‹é€‰æ‹©')
    asr_method.change(fn=asr_model_change, inputs=[asr_method], outputs=[asr_method])
    llm_method = gr.Dropdown(choices=['Qwen', 'Qwen2', 'Linly', 'Gemini', 'ChatGLM', 'ChatGPT', 'GPT4Free', 'QAnything', 'ç›´æ¥å›å¤ Direct Reply', 'Comming Soon!!!'], value='ç›´æ¥å›å¤ Direct Reply', label='LLM æ¨¡å‹é€‰æ‹©')
    llm_method.change(fn=llm_model_change, inputs=[llm_method], outputs=[llm_method])
    return (source_image, voice, rate, volume, pitch, am, voc, lang, male, 
            ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice, tts_method, 
            batch_size, character, talker_method, asr_method, llm_method, generate_button, audio_output, 
            mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor)

def exmaple_setting(asr, text, character, talk, tts, voice, llm):
    # é»˜è®¤textçš„Example
    examples = [
        ['Whisper-base', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-XiaoxiaoNeural', 'ç›´æ¥å›å¤ Direct Reply'],
        ['Whisper-tiny', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'PaddleTTS', 'None', 'ç›´æ¥å›å¤ Direct Reply'],
        ['Whisper-base', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-XiaoxiaoNeural', 'Qwen'],
        ['FunASR', 'å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ', 'ç”·æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-YunyangNeural', 'Qwen'],
        ['Whisper-tiny', 'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ', 'å¥³æ€§è§’è‰²', 'Wav2Lip', 'PaddleTTS', 'None', 'Qwen'],
        ['Whisper-tiny', 'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ', 'å¥³æ€§è§’è‰²', 'Wav2Lipv2', 'Edge-TTS', 'None', 'Qwen'],
    ]
    with gr.Row(variant='panel'):
        with gr.Column(variant='panel'):
            gr.Markdown("## Test Examples")
            gr.Examples(
                examples = examples,
                inputs = [asr, text, character, talk , tts, voice, llm],
            )
def app_multi():
    with gr.Blocks(analytics_enabled=False, title='Linly-Talker') as inference:
        # æ˜¾ç¤ºæ ‡é¢˜
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) å¤šè½®GPTå¯¹è¯"))
        
        with gr.Row():
            with gr.Column():
                # åŠ è½½ Web UI è®¾ç½®
                (source_image, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice,
                 tts_method, batch_size, character, talker_method, asr_method, llm_method, generate_button, audio_output,
                 mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor) = webui_setting()

                
                # æ•°å­—äººé—®ç­”è§†é¢‘æ˜¾ç¤º
                video = gr.Video(label='æ•°å­—äººé—®ç­”', scale=0.5)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯¹è¯åï¼‰", variant='primary')
            
            with gr.Column():
                with gr.Tabs(elem_id="sadtalker_checkbox"):
                    with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings", open=False):
                            gr.Markdown("SadTalker: need help? please visit our [best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md) for more details")
                            with gr.Column(variant='panel'):
                                # æ•°å­—äººå‚æ•°è®¾ç½®
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0)
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1)
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)
                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster")
                                    preprocess_type = gr.Radio(['crop', 'resize','full', 'extcrop', 'extfull'], value='crop', label='preprocess', info="How to handle input image?")
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                with gr.Row():
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value=20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")
                
                # System è®¾å®šåŠæ¸…é™¤å†å²å¯¹è¯
                with gr.Row():
                    with gr.Column(scale=3):
                        system_input = gr.Textbox(value=DEFAULT_SYSTEM, lines=1, label='System (è®¾å®šè§’è‰²)')
                    with gr.Column(scale=1):
                        modify_system = gr.Button("ğŸ› ï¸ è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯", scale=2)
                    system_state = gr.Textbox(value=DEFAULT_SYSTEM, visible=False)
                
                # èŠå¤©æœºå™¨äººç•Œé¢
                chatbot = gr.Chatbot(height=400, show_copy_button=True)
                
                # è¯­éŸ³è¾“å…¥åŠè¯†åˆ«æŒ‰é’®
                with gr.Group():
                    question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label='è¯­éŸ³å¯¹è¯', autoplay=False)
                    asr_btn = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                
                # æ–‡æœ¬è¾“å…¥æ¡†
                msg = gr.Textbox(label="è¾“å…¥æ–‡å­—/é—®é¢˜", lines=3, placeholder='è¯·è¾“å…¥æ–‡æœ¬æˆ–é—®é¢˜ï¼ŒåŒæ—¶å¯ä»¥è®¾ç½®LLMæ¨¡å‹ã€‚é»˜è®¤ä½¿ç”¨ç›´æ¥å›å¤ã€‚')
                asr_btn.click(fn=Asr, inputs=[question_audio], outputs=[msg])
                

                generate_button.click(fn=TTS_response, 
                                      inputs=[msg, voice, rate, volume, pitch, am, voc, lang, male,
                                                ref_audio, prompt_text, prompt_language, text_language,
                                                cut_method, question_audio, prompt_text, use_mic_voice, 
                                                mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, tts_method, ],
                                      outputs=[audio_output])

                # æ¸…é™¤å†å²è®°å½•å’Œæäº¤æŒ‰é’®
                with gr.Row():
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    sumbit = gr.Button("ğŸš€ å‘é€", variant='primary')
                    
                # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶
                sumbit.click(chat_response, inputs=[system_input, msg, chatbot], outputs=[msg, chatbot])
                clear_history.click(fn=clear_session, outputs=[msg, chatbot])
                modify_system.click(fn=modify_system_session, inputs=[system_input], outputs=[system_state, system_input, chatbot])
                video_button.click(fn=human_response, inputs=[source_image, chatbot, question_audio, talker_method, voice, rate, volume, pitch,
                                                             am, voc, lang, male, 
                                                             ref_audio, prompt_text, prompt_language, text_language, cut_method,  use_mic_voice, 
                                                             mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor,
                                                             tts_method, character, preprocess_type, 
                                                             is_still_mode, enhancer, batch_size, size_of_image,
                                                             pose_style, facerender, exp_weight, blink_every, fps], outputs=[video])

        # ç¤ºä¾‹è®¾ç½®
        exmaple_setting(asr_method, msg, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_img():
    with gr.Blocks(analytics_enabled=False, title='Linly-Talker') as inference:
        # æ˜¾ç¤ºæ ‡é¢˜
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) ä¸ªæ€§åŒ–è§’è‰²äº’åŠ¨"))
        
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'):
                # åŠ è½½ Web UI è®¾ç½®
                (source_image, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice,
                 tts_method, batch_size, character, talker_method, asr_method, llm_method, generate_button, audio_output,
                 mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor) = webui_setting()
            
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone', 'upload'], type="filepath", label='è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="è¾“å…¥æ–‡å­—/é—®é¢˜", lines=3, placeholder='è¯·è¾“å…¥æ–‡æœ¬æˆ–é—®é¢˜ï¼ŒåŒæ—¶å¯ä»¥è®¾ç½®LLMæ¨¡å‹ã€‚é»˜è®¤ä½¿ç”¨ç›´æ¥å›å¤ã€‚')
                            asr_btn = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_btn.click(fn=Asr, inputs=[question_audio], outputs=[input_text])
                generate_button.click(fn=TTS_response, 
                                      inputs=[input_text, voice, rate, volume, pitch, am, voc, lang, male,
                                                ref_audio, prompt_text, prompt_language, text_language,
                                                cut_method, question_audio, prompt_text, use_mic_voice, 
                                                mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, tts_method, ],
                                      outputs=[audio_output])
                with gr.Tabs(elem_id="text_examples"): 
                    gr.Markdown("## Text Examples")
                    examples = [
                        ['åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ'],
                        ['å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ'],
                        ['ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ'],
                    ]
                    gr.Examples(examples=examples, inputs=[input_text])
                
                with gr.Tabs(elem_id="sadtalker_checkbox"):
                    with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings", open=False):
                            gr.Markdown("SadTalker: need help? please visit our [best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md) for more details")
                            with gr.Column(variant='panel'):
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0)
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1)
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)
                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster")
                                    preprocess_type = gr.Radio(['crop', 'resize', 'full', 'extcrop', 'extfull'], value='crop', label='preprocess', info="How to handle input image?")
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                with gr.Row():
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value=20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")
                
                with gr.Tabs(elem_id="sadtalker_genearted"):
                    gen_video = gr.Video(label="æ•°å­—äººè§†é¢‘", format="mp4")

                submit = gr.Button('ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘', elem_id="sadtalker_generate", variant='primary')
                submit.click(
                    fn=Talker_response_img,
                    inputs=[question_audio, talker_method, input_text, voice, rate, volume, pitch,
                            am, voc, lang, male, ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice,
                            mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, 
                            tts_method, source_image, preprocess_type, is_still_mode, enhancer, batch_size, size_of_image,
                            pose_style, facerender, exp_weight, blink_every, fps], 
                    outputs=[gen_video]
                )
        
        with gr.Row():
            examples = [
                ['examples/source_image/full_body_2.png', 'SadTalker', 'crop', False, False],
                ['examples/source_image/full_body_1.png', 'Wav2Lipv2', 'full', False, False],
                ['examples/source_image/full_body_2.png', 'Wav2Lipv2', 'full', False, False],
                ['examples/source_image/full_body_1.png', 'Wav2Lip', 'full', True, False],
                ['examples/source_image/full_body_1.png', 'SadTalker', 'full', True, False],
                ['examples/source_image/full4.jpeg', 'SadTalker', 'crop', False, True],
            ]
            gr.Examples(
                examples=examples,
                inputs=[source_image, talker_method, preprocess_type, is_still_mode, enhancer],
                outputs=[gen_video],
                # cache_examples=True,
            )
    return inference


def load_musetalk_model():
    """åŠ è½½MuseTalkæ¨¡å‹ï¼Œæ˜¾ç¤ºåŠ è½½çŠ¶æ€å’Œç»“æœä¿¡æ¯ã€‚"""
    gr.Warning("è‹¥æ˜¾å­˜ä¸è¶³ï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–æ¨¡å‹æˆ–è€…æ¢å…¶ä»–è®¾å¤‡ã€‚")
    gr.Info("MuseTalkæ¨¡å‹å¯¼å…¥ä¸­...")
    musetalker.init_model()
    gr.Info("MuseTalkæ¨¡å‹å¯¼å…¥æˆåŠŸ")
    return "MuseTalkæ¨¡å‹å¯¼å…¥æˆåŠŸ"

def musetalk_prepare_material(source_video, bbox_shift):
    """å‡†å¤‡MuseTalkæ‰€éœ€çš„ç´ æï¼Œæ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½ã€‚"""
    if musetalker.load is False:
        gr.Warning("è¯·å…ˆåŠ è½½MuseTalkæ¨¡å‹åé‡æ–°ä¸Šä¼ æ–‡ä»¶")
        return source_video, None
    return musetalker.prepare_material(source_video, bbox_shift)

def app_muse():
    """å®šä¹‰MuseTalkåº”ç”¨çš„UIå’Œé€»è¾‘ã€‚"""
    with gr.Blocks(analytics_enabled=False, title='Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) MuseTalkeræ•°å­—äººå®æ—¶å¯¹è¯"))

        # ä¸Šä¼ å‚è€ƒè§†é¢‘å’Œè°ƒæ•´bbox_shift
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'):
                with gr.TabItem('MuseV Video'):
                    gr.Markdown("MuseV: éœ€è¦å¸®åŠ©ï¼Ÿè¯·è®¿é—® [MuseVDemo](https://huggingface.co/spaces/AnchorFake/MuseVDemo) ç”Ÿæˆè§†é¢‘ã€‚")
                    source_video = gr.Video(label="Reference Video", sources=['upload'])
                    gr.Markdown(
                        "BBox_shift æ¨èå€¼ä¸‹é™ï¼Œåœ¨ç”Ÿæˆåˆå§‹ç»“æœåç”Ÿæˆç›¸åº”çš„ bbox èŒƒå›´ã€‚"
                        "ä¸€èˆ¬æ¥è¯´ï¼Œæ­£å€¼ï¼ˆå‘ä¸‹åŠéƒ¨åˆ†ç§»åŠ¨ï¼‰é€šå¸¸ä¼šå¢åŠ å˜´å·´çš„å¼ å¼€åº¦ï¼Œ"
                        "è€Œè´Ÿå€¼ï¼ˆå‘ä¸ŠåŠéƒ¨åˆ†ç§»åŠ¨ï¼‰é€šå¸¸ä¼šå‡å°‘å˜´å·´çš„å¼ å¼€åº¦ã€‚"
                        "ç”¨æˆ·å¯æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´æ­¤å‚æ•°ã€‚"
                    )
                    bbox_shift = gr.Number(label="BBox_shift value, px", value=0)
                    bbox_shift_scale = gr.Textbox(label="bbox_shift_scale", value="", interactive=False)
                
                # åŠ è½½MuseTalkæ¨¡å‹æŒ‰é’®
                load_musetalk = gr.Button("åŠ è½½MuseTalkæ¨¡å‹(ä¼ å…¥è§†é¢‘å‰å…ˆåŠ è½½)", variant='primary')
                load_musetalk.click(fn=load_musetalk_model, outputs=bbox_shift_scale)

                # åŠ è½½ Web UI è®¾ç½®
                (_, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice,
                 tts_method, batch_size, _, _, asr_method, llm_method, generate_button, audio_output,
                 mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor) = webui_setting(talk=True)
            
            # å¤„ç†source_videoå˜åŒ–
            source_video.change(fn=musetalk_prepare_material, inputs=[source_video, bbox_shift], outputs=[source_video, bbox_shift_scale])

            # é—®é¢˜è¾“å…¥å’ŒASRè¯†åˆ«
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone', 'upload'], type="filepath", label='è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="è¾“å…¥æ–‡å­—/é—®é¢˜", lines=3, placeholder='è¯·è¾“å…¥æ–‡æœ¬æˆ–é—®é¢˜ï¼ŒåŒæ—¶å¯ä»¥è®¾ç½®LLMæ¨¡å‹ã€‚é»˜è®¤ä½¿ç”¨ç›´æ¥å›å¤ã€‚')
                            asr_btn = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_btn.click(fn=Asr, inputs=[question_audio], outputs=[input_text]) 
                    generate_button.click(fn=TTS_response, 
                                      inputs=[input_text, voice, rate, volume, pitch, am, voc, lang, male,
                                                ref_audio, prompt_text, prompt_language, text_language,
                                                cut_method, question_audio, prompt_text, use_mic_voice, 
                                                mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, tts_method, ],
                                      outputs=[audio_output])

                # ç”ŸæˆMuseTalkè§†é¢‘
                with gr.TabItem("MuseTalk Video"):
                    gen_video = gr.Video(label="æ•°å­—äººè§†é¢‘", format="mp4")
                submit = gr.Button('Generate', elem_id="sadtalker_generate", variant='primary')
                # examples = [os.path.join('Musetalk/data/video', video) for video in os.listdir("Musetalk/data/video")]
                
                gr.Markdown("## MuseV Video Examples")
                gr.Examples(
                    examples=[
                        ['Musetalk/data/video/yongen_musev.mp4', 5],
                        ['Musetalk/data/video/musk_musev.mp4', 5],
                        ['Musetalk/data/video/monalisa_musev.mp4', 5],
                        ['Musetalk/data/video/sun_musev.mp4', 5],
                        ['Musetalk/data/video/seaside4_musev.mp4', 5],
                        ['Musetalk/data/video/sit_musev.mp4', 5],
                        ['Musetalk/data/video/man_musev.mp4', 5]
                    ],
                    inputs=[source_video, bbox_shift], 
                )

            # æäº¤æŒ‰é’®ç‚¹å‡»äº‹ä»¶
            submit.click(
                fn=MuseTalker_response,
                inputs=[
                    source_video, bbox_shift, question_audio, input_text, 
                    voice, rate, volume, pitch, am, voc, lang, male, 
                    ref_audio, prompt_text, prompt_language, text_language, cut_method, use_mic_voice, 
                    mode_checkbox_group, sft_dropdown, prompt_text_cv, prompt_wav_upload, prompt_wav_record, seed, speed_factor, 
                    tts_method, batch_size
                ],
                outputs=[gen_video]
            )

    return inference
def asr_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    """æ ¹æ®é€‰æ‹©çš„æ¨¡å‹åç§°æ›´æ¢ASRæ¨¡å‹ã€‚"""
    global asr
    clear_memory()  # æ¸…ç†æ˜¾å­˜

    try:
        if model_name == "Whisper-tiny":
            asr_path = 'Whisper/tiny.pt' if os.path.exists('Whisper/tiny.pt') else 'tiny'
            asr = WhisperASR(asr_path)
            gr.Info("Whisper-tinyæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == "Whisper-base":
            asr_path = 'Whisper/base.pt' if os.path.exists('Whisper/base.pt') else 'base'
            asr = WhisperASR(asr_path)
            gr.Info("Whisper-baseæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'FunASR':
            from ASR import FunASR
            asr = FunASR()
            gr.Info("FunASRæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'OmniSenseVoice-quantize':
            from ASR import OmniSenseVoice
            asr = OmniSenseVoice(quantize=True)
            gr.Info("OmniSenseVoice-quantizeæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'OmniSenseVoice':
            from ASR import OmniSenseVoice
            asr = OmniSenseVoice(quantize=False)
            gr.Info("OmniSenseVoiceæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        else:
            gr.Warning("æœªçŸ¥ASRæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    except Exception as e:
        gr.Warning(f"{model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    return model_name

def llm_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    """æ›´æ¢LLMæ¨¡å‹ï¼Œå¹¶æ ¹æ®é€‰æ‹©çš„æ¨¡å‹åŠ è½½ç›¸åº”èµ„æºã€‚"""
    global llm
    gemini_apikey = ""  # Geminiæ¨¡å‹çš„APIå¯†é’¥
    openai_apikey = ""  # OpenAIçš„APIå¯†é’¥
    proxy_url = None  # ä»£ç†URL

    # æ¸…ç†æ˜¾å­˜ï¼Œé‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜ä»¥ä¾¿åŠ è½½æ–°æ¨¡å‹
    clear_memory()

    try:
        if model_name == 'Linly':
            llm = llm_class.init_model('Linly', 'Linly-AI/Chinese-LLaMA-2-7B-hf', prefix_prompt=PREFIX_PROMPT)
            gr.Info("Linlyæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'Qwen':
            llm = llm_class.init_model('Qwen', 'Qwen/Qwen-1_8B-Chat', prefix_prompt=PREFIX_PROMPT)
            gr.Info("Qwenæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'Qwen2':
            llm = llm_class.init_model('Qwen2', 'Qwen/Qwen1.5-0.5B-Chat', prefix_prompt=PREFIX_PROMPT)
            gr.Info("Qwen2æ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'Gemini':
            if gemini_apikey:
                llm = llm_class.init_model('Gemini', 'gemini-pro', gemini_apikey, proxy_url)
                gr.Info("Geminiæ¨¡å‹å¯¼å…¥æˆåŠŸ")
            else:
                gr.Warning("è¯·å¡«å†™Geminiçš„APIå¯†é’¥")
        elif model_name == 'ChatGLM':
            llm = llm_class.init_model('ChatGLM', 'THUDM/chatglm3-6b', prefix_prompt=PREFIX_PROMPT)
            gr.Info("ChatGLMæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'ChatGPT':
            if openai_apikey:
                llm = llm_class.init_model('ChatGPT', api_key=openai_apikey, proxy_url=proxy_url, prefix_prompt=PREFIX_PROMPT)
                gr.Info("ChatGPTæ¨¡å‹å¯¼å…¥æˆåŠŸ")
            else:
                gr.Warning("è¯·å¡«å†™OpenAIçš„APIå¯†é’¥")
        elif model_name == 'ç›´æ¥å›å¤ Direct Reply':
            llm = llm_class.init_model(model_name)
            gr.Info("ç›´æ¥å›å¤ï¼Œä¸ä½¿ç”¨LLMæ¨¡å‹")
        elif model_name == 'GPT4Free':
            llm = llm_class.init_model('GPT4Free', prefix_prompt=PREFIX_PROMPT)
            gr.Info("GPT4Freeæ¨¡å‹å¯¼å…¥æˆåŠŸï¼Œè¯·æ³¨æ„è¯¥æ¨¡å‹å¯èƒ½ä¸ç¨³å®š")
        elif model_name == 'QAnything':
            llm = llm_class.init_model('QAnything')
            gr.Info("QAnythingæ¨¡å‹æ¥å£åŠ è½½æˆåŠŸ")
        else:
            gr.Warning("æœªçŸ¥LLMæ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°æˆ–æå‡ºIssue")
    except Exception as e:
        gr.Warning(f"{model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    return model_name
def talker_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    """æ›´æ¢æ•°å­—äººå¯¹è¯æ¨¡å‹ï¼Œå¹¶æ ¹æ®é€‰æ‹©çš„æ¨¡å‹åŠ è½½ç›¸åº”èµ„æºã€‚"""
    global talker

    # æ¸…ç†æ˜¾å­˜ï¼Œé‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜ä»¥ä¾¿åŠ è½½æ–°æ¨¡å‹
    clear_memory()

    if model_name not in ['SadTalker', 'Wav2Lip', 'Wav2Lipv2', 'NeRFTalk']:
        gr.Warning("å…¶ä»–æ¨¡å‹æš‚æœªé›†æˆï¼Œè¯·ç­‰å¾…æ›´æ–°")
        return model_name

    try:
        if model_name == 'SadTalker':
            from TFG import SadTalker
            talker = SadTalker(lazy_load=True)
            gr.Info("SadTalkeræ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'Wav2Lip':
            from TFG import Wav2Lip
            clear_memory()
            talker = Wav2Lip("checkpoints/wav2lip_gan.pth")
            gr.Info("Wav2Lipæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        elif model_name == 'Wav2Lipv2':
            from TFG import Wav2Lipv2
            clear_memory()
            talker = Wav2Lipv2('checkpoints/wav2lipv2.pth')
            gr.Info("Wav2Lipv2æ¨¡å‹å¯¼å…¥æˆåŠŸï¼Œèƒ½å¤Ÿç”Ÿæˆæ›´é«˜è´¨é‡çš„ç»“æœ")
        elif model_name == 'NeRFTalk':
            from TFG import NeRFTalk
            talker = NeRFTalk()
            talker.init_model('checkpoints/Obama_ave.pth', 'checkpoints/Obama.json')
            gr.Info("NeRFTalkæ¨¡å‹å¯¼å…¥æˆåŠŸ")
            gr.Warning("NeRFTalkæ¨¡å‹ä»…é’ˆå¯¹å•ä¸ªäººè®­ç»ƒï¼Œå†…ç½®å¥¥å·´é©¬æ¨¡å‹ï¼Œä¸Šä¼ å…¶ä»–å›¾ç‰‡æ— æ•ˆ")
    except Exception as e:
        gr.Warning(f"{model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    return model_name

def tts_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    """æ›´æ¢TTSæ¨¡å‹ï¼Œå¹¶æ ¹æ®é€‰æ‹©çš„æ¨¡å‹åŠ è½½ç›¸åº”èµ„æºã€‚"""
    global tts
    global cosyvoice
    # æ¸…ç†æ˜¾å­˜ï¼Œé‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜ä»¥ä¾¿åŠ è½½æ–°æ¨¡å‹
    clear_memory()

    try:
        if model_name == 'Edge-TTS':
            # tts = EdgeTTS()  # Uncomment when implementation available
            if edgetts.network:
                gr.Info("EdgeTTSæ¨¡å‹å¯¼å…¥æˆåŠŸ")
            else:
                gr.Warning("EdgeTTSæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
        elif model_name == 'PaddleTTS':
            from TTS import PaddleTTS
            tts = PaddleTTS()
            gr.Info("PaddleTTSæ¨¡å‹å¯¼å…¥æˆåŠŸ, æ•ˆæœæœ‰é™ï¼Œä¸å»ºè®®ä½¿ç”¨")
        elif model_name == 'GPT-SoVITSå…‹éš†å£°éŸ³':
            gpt_path = "GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
            sovits_path = "GPT_SoVITS/pretrained_models/s2G488k.pth"
            vits.load_model(gpt_path, sovits_path)
            gr.Info("GPT-SoVITSæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¯·ä¸Šä¼ å‚è€ƒéŸ³é¢‘è¿›è¡Œå…‹éš†")
        elif model_name == 'CosyVoice-SFTæ¨¡å¼':
            from VITS import CosyVoiceTTS
            model_path = 'checkpoints/CosyVoice_ckpt/CosyVoice-300M-SFT'
            cosyvoice = CosyVoiceTTS(model_path)
            gr.Info("CosyVoiceæ¨¡å‹å¯¼å…¥æˆåŠŸï¼Œé€‚åˆä½¿ç”¨SFTæ¨¡å¼ï¼Œç”¨å¾®è°ƒåæ•°æ®")
        elif model_name == 'CosyVoice-å…‹éš†ç¿»è¯‘æ¨¡å¼':
            from VITS import CosyVoiceTTS
            model_path = 'checkpoints/CosyVoice_ckpt/CosyVoice-300M'
            cosyvoice = CosyVoiceTTS(model_path)
            gr.Info("CosyVoiceæ¨¡å‹å¯¼å…¥æˆåŠŸï¼Œæ›´é€‚åˆè¿›è¡Œå…‹éš†å£°éŸ³å’Œç¿»è¯‘å£°éŸ³")
        else:
            gr.Warning("æœªçŸ¥TTSæ¨¡å‹ï¼Œè¯·æ£€æŸ¥æ¨¡å‹åç§°æˆ–æå‡ºIssue")
    except Exception as e:
        gr.Warning(f"{model_name}æ¨¡å‹åŠ è½½å¤±è´¥: {e}")

    return model_name

def success_print(text):
    """è¾“å‡ºç»¿è‰²æ–‡æœ¬ï¼Œè¡¨ç¤ºæˆåŠŸä¿¡æ¯ã€‚"""
    print(f"\033[1;32m{text}\033[0m")

def error_print(text):
    """è¾“å‡ºçº¢è‰²æ–‡æœ¬ï¼Œè¡¨ç¤ºé”™è¯¯ä¿¡æ¯ã€‚"""
    print(f"\033[1;31m{text}\033[0m")
    
if __name__ == "__main__":
    # åˆå§‹åŒ–LLMç±»
    llm_class = LLM(mode='offline')
    llm = llm_class.init_model('ç›´æ¥å›å¤ Direct Reply')
    success_print("é»˜è®¤ä¸ä½¿ç”¨LLMæ¨¡å‹ï¼Œç›´æ¥å›å¤é—®é¢˜ï¼ŒåŒæ—¶å‡å°‘æ˜¾å­˜å ç”¨ï¼")

    # å°è¯•åŠ è½½GPT-SoVITSæ¨¡å—
    try:
        from VITS import *
        vits = GPT_SoVITS()
        success_print("Success! GPT-SoVITSæ¨¡å—åŠ è½½æˆåŠŸï¼Œè¯­éŸ³å…‹éš†é»˜è®¤ä½¿ç”¨GPT-SoVITSæ¨¡å‹")
    except Exception as e:
        error_print(f"GPT-SoVITS åŠ è½½å¤±è´¥: {e}")
        error_print("å¦‚æœä½¿ç”¨VITSï¼Œè¯·å…ˆä¸‹è½½GPT-SoVITSæ¨¡å‹å¹¶å®‰è£…ç¯å¢ƒ")

    # å°è¯•åŠ è½½SadTalkeræ¨¡å—
    try:
        from TFG import SadTalker
        talker = SadTalker(lazy_load=True)
        success_print("Success! SadTalkeræ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨SadTalkeræ¨¡å‹")
    except Exception as e:
        error_print(f"SadTalker åŠ è½½å¤±è´¥: {e}")
        error_print("å¦‚æœä½¿ç”¨SadTalkerï¼Œè¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")

    # å°è¯•åŠ è½½Whisper ASRæ¨¡å—
    try:
        from ASR import WhisperASR
        asr = WhisperASR('base')
        success_print("Success! WhisperASRæ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨Whisper-baseæ¨¡å‹")
    except Exception as e:
        error_print(f"WhisperASR åŠ è½½å¤±è´¥: {e}")
        error_print("å¦‚æœä½¿ç”¨FunASRï¼Œè¯·å…ˆä¸‹è½½WhisperASRæ¨¡å‹å¹¶å®‰è£…ç¯å¢ƒ")

    # æ£€æŸ¥GPUæ˜¾å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert bytes to GB
        if gpu_memory < 8:
            error_print("è­¦å‘Š: æ‚¨çš„æ˜¾å¡æ˜¾å­˜å°äº8GBï¼Œä¸å»ºè®®ä½¿ç”¨MuseTalkåŠŸèƒ½")

    # å°è¯•åŠ è½½MuseTalkæ¨¡å—
    try:
        from TFG import MuseTalk_RealTime
        musetalker = MuseTalk_RealTime()
        success_print("Success! MuseTalkæ¨¡å—åŠ è½½æˆåŠŸ")
    except Exception as e:
        error_print(f"MuseTalk åŠ è½½å¤±è´¥: {e}")
        error_print("å¦‚æœä½¿ç”¨MuseTalkï¼Œè¯·å…ˆä¸‹è½½MuseTalkæ¨¡å‹")

    # å°è¯•åŠ è½½EdgeTTSæ¨¡å—
    try:
        tts = edgetts
        if not tts.network:
            error_print("EdgeTTSæ¨¡å—åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    except Exception as e:
        error_print(f"EdgeTTS åŠ è½½å¤±è´¥: {e}")

    # Gradio UIçš„åˆå§‹åŒ–å’Œå¯åŠ¨
    gr.close_all()
    demo_img = app_img()
    demo_multi = app_multi()
    demo_muse = app_muse()
    demo = gr.TabbedInterface(
        interface_list=[demo_img, demo_multi, demo_muse],
        tab_names=["ä¸ªæ€§åŒ–è§’è‰²äº’åŠ¨", "æ•°å­—äººå¤šè½®æ™ºèƒ½å¯¹è¯", "MuseTalkæ•°å­—äººå®æ—¶å¯¹è¯"],
        title="Linly-Talker WebUI"
    )
    demo.queue(max_size=4, default_concurrency_limit=2)
    demo.launch(
        server_name=ip,  # æœ¬åœ°localhost:127.0.0.1 æˆ– "0.0.0.0" è¿›è¡Œå…¨å±€ç«¯å£è½¬å‘
        server_port=port,
        # ssl_certfile=ssl_certfile,  # SSLè¯ä¹¦æ–‡ä»¶
        # ssl_keyfile=ssl_keyfile,  # SSLå¯†é’¥æ–‡ä»¶
        # ssl_verify=False,
        # share=True,
        debug=True,
    )