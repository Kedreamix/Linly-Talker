# æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€

[English](./README_en.md) [ç®€ä½“ä¸­æ–‡](./README.md)

**2023.12 æ›´æ–°** ğŸ“†

**ç”¨æˆ·å¯ä»¥ä¸Šä¼ ä»»æ„å›¾ç‰‡è¿›è¡Œå¯¹è¯**

**2024.01 æ›´æ–°** ğŸ“†

- **ä»¤äººå…´å¥‹çš„æ¶ˆæ¯ï¼æˆ‘ç°åœ¨å·²ç»å°†å¼ºå¤§çš„GeminiProå’ŒQwenå¤§æ¨¡å‹èå…¥åˆ°æˆ‘ä»¬çš„å¯¹è¯åœºæ™¯ä¸­ã€‚ç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨å¯¹è¯ä¸­ä¸Šä¼ ä»»ä½•å›¾ç‰‡ï¼Œä¸ºæˆ‘ä»¬çš„äº’åŠ¨å¢æ·»äº†å…¨æ–°çš„å±‚é¢ã€‚**
- **æ›´æ–°äº†FastAPIçš„éƒ¨ç½²è°ƒç”¨æ–¹æ³•ã€‚** 
- **æ›´æ–°äº†å¾®è½¯TTSçš„é«˜çº§è®¾ç½®é€‰é¡¹ï¼Œå¢åŠ å£°éŸ³ç§ç±»çš„å¤šæ ·æ€§ï¼Œä»¥åŠåŠ å…¥è§†é¢‘å­—å¹•åŠ å¼ºå¯è§†åŒ–ã€‚**
- **æ›´æ–°äº†GPTå¤šè½®å¯¹è¯ç³»ç»Ÿï¼Œä½¿å¾—å¯¹è¯æœ‰ä¸Šä¸‹æ–‡è”ç³»ï¼Œæé«˜æ•°å­—äººçš„äº¤äº’æ€§å’ŒçœŸå®æ„Ÿ**

## ä»‹ç»

Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚

![The system architecture of multimodal humanâ€“computer interaction.](docs/HOI.png)

## TO DO LIST

- [x] åŸºæœ¬å®Œæˆå¯¹è¯ç³»ç»Ÿæµç¨‹ï¼Œèƒ½å¤Ÿ`è¯­éŸ³å¯¹è¯`
- [x] åŠ å…¥äº†LLMå¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬`Linly`ï¼Œ`Qwen`å’Œ`GeminiPro`çš„ä½¿ç”¨
- [x] å¯ä¸Šä¼ `ä»»æ„æ•°å­—äººç…§ç‰‡`è¿›è¡Œå¯¹è¯
- [x] LinlyåŠ å…¥`FastAPI`è°ƒç”¨æ–¹å¼
- [x] åˆ©ç”¨å¾®è½¯`TTS`åŠ å…¥é«˜çº§é€‰é¡¹ï¼Œå¯è®¾ç½®å¯¹åº”äººå£°ä»¥åŠéŸ³è°ƒç­‰å‚æ•°ï¼Œå¢åŠ å£°éŸ³çš„å¤šæ ·æ€§
- [x] è§†é¢‘ç”ŸæˆåŠ å…¥`å­—å¹•`ï¼Œèƒ½å¤Ÿæ›´å¥½çš„è¿›è¡Œå¯è§†åŒ–
- [x] GPT`å¤šè½®å¯¹è¯`ç³»ç»Ÿï¼ˆæé«˜æ•°å­—äººçš„äº¤äº’æ€§å’ŒçœŸå®æ„Ÿï¼Œå¢å¼ºæ•°å­—äººçš„æ™ºèƒ½ï¼‰
- [ ] `è¯­éŸ³å…‹éš†`æŠ€æœ¯ï¼ˆè¯­éŸ³å…‹éš†åˆæˆè‡ªå·±å£°éŸ³ï¼Œæé«˜æ•°å­—äººåˆ†èº«çš„çœŸå®æ„Ÿå’Œäº’åŠ¨ä½“éªŒï¼‰
- [ ] åŠ å…¥`Langchain`çš„æ¡†æ¶ï¼Œå»ºç«‹æœ¬åœ°çŸ¥è¯†åº“
- [ ] `å®æ—¶`è¯­éŸ³è¯†åˆ«ï¼ˆäººä¸æ•°å­—äººä¹‹é—´å°±å¯ä»¥é€šè¿‡è¯­éŸ³è¿›è¡Œå¯¹è¯äº¤æµ)

ğŸ”† è¯¥é¡¹ç›® Linly-Talker æ­£åœ¨è¿›è¡Œä¸­ - æ¬¢è¿æå‡ºPRè¯·æ±‚ï¼å¦‚æœæ‚¨æœ‰ä»»ä½•å…³äºæ–°çš„æ¨¡å‹æ–¹æ³•ã€ç ”ç©¶ã€æŠ€æœ¯æˆ–å‘ç°è¿è¡Œé”™è¯¯çš„å»ºè®®ï¼Œè¯·éšæ—¶ç¼–è¾‘å¹¶æäº¤ PRã€‚æ‚¨ä¹Ÿå¯ä»¥æ‰“å¼€ä¸€ä¸ªé—®é¢˜æˆ–é€šè¿‡ç”µå­é‚®ä»¶ç›´æ¥è”ç³»æˆ‘ã€‚ğŸ“©â­ å¦‚æœæ‚¨å‘ç°è¿™ä¸ªGithub Projectæœ‰ç”¨ï¼Œè¯·ç»™å®ƒç‚¹ä¸ªæ˜Ÿï¼ğŸ¤©

## ç¤ºä¾‹

|                        æ–‡å­—/è¯­éŸ³å¯¹è¯                         |                          æ•°å­—äººå›ç­”                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|                 åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ                 | <video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f1deb189-b682-4175-9dea-7eeb0fb392ca"></video> |
|                      å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ                      | <video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/968b5c43-4dce-484b-b6c6-0fd4d621ac03"></video> |
|  æ’°å†™ä¸€ç¯‡äº¤å“ä¹éŸ³ä¹ä¼šè¯„è®ºï¼Œè®¨è®ºä¹å›¢çš„è¡¨æ¼”å’Œè§‚ä¼—çš„æ•´ä½“ä½“éªŒã€‚  | <video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/f052820f-6511-4cf0-a383-daf8402630db"></video> |
| ç¿»è¯‘æˆä¸­æ–‡ï¼šLuck is a dividend of sweat. The more you sweat, the luckier you get. | <video src="https://github.com/Kedreamix/Linly-Talker/assets/61195303/118eec13-a9f7-4c38-b4ad-044d36ba9776"></video> |

## åˆ›å»ºç¯å¢ƒ

```bash
conda create -n linly python=3.9 
conda activate linly

pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113

# pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113

# conda install pytorch==1.12.0 torchvision==0.13.0 torchaudio==0.12.0 cudatoolkit=11.3 -c pytorch

conda install -q ffmpeg # ffmpeg==4.2.2

pip install -r requirements_app.txt
```

ä¸ºäº†å¤§å®¶çš„éƒ¨ç½²ä½¿ç”¨æ–¹ä¾¿ï¼Œæ›´æ–°äº†ä¸€ä¸ª`configs.py`æ–‡ä»¶ï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œä¸€äº›è¶…å‚æ•°ä¿®æ”¹å³å¯

```bash
# è®¾å¤‡è¿è¡Œç«¯å£ (Device running port)
port = 7860
# apiè¿è¡Œç«¯å£åŠIP (API running port and IP)
mode = 'api' # api éœ€è¦å…ˆè¿è¡ŒLinly-api-fast.pyï¼Œæš‚æ—¶ä»…ä»…é€‚ç”¨äºLinly
ip = '127.0.0.1' 
api_port = 7871

# Læ¨¡å‹è·¯å¾„ (Linly model path)
mode = 'offline'
model_path = 'Qwen/Qwen-1_8B-Chat'

# sslè¯ä¹¦ (SSL certificate) éº¦å…‹é£å¯¹è¯éœ€è¦æ­¤å‚æ•°
# æœ€å¥½è°ƒæ•´ä¸ºç»å¯¹è·¯å¾„
ssl_certfile = "./https_cert/cert.pem"
ssl_keyfile = "./https_cert/key.pem"
```

## ASR - Whisper

å€Ÿé‰´OpenAIçš„Whisper,å…·ä½“ä½¿ç”¨æ–¹æ³•å‚è€ƒ[https://github.com/openai/whisper](https://github.com/openai/whisper)

## TTS - Edge TTS

ä½¿ç”¨å¾®è½¯è¯­éŸ³æœåŠ¡,å…·ä½“ä½¿ç”¨æ–¹æ³•å‚è€ƒ[https://github.com/rany2/edge-tts](https://github.com/rany2/edge-tts)

æˆ‘ç¼–å†™äº†ä¸€ä¸ª `EdgeTTS` çš„ç±»ï¼Œèƒ½å¤Ÿæ›´å¥½çš„ä½¿ç”¨ï¼Œå¹¶ä¸”å¢åŠ äº†ä¿å­˜å­—å¹•æ–‡ä»¶çš„åŠŸèƒ½

```python
class EdgeTTS:
    def __init__(self, list_voices = False, proxy = None) -> None:
        voices = list_voices_fn(proxy=proxy)
        self.SUPPORTED_VOICE = [item['ShortName'] for item in voices]
        self.SUPPORTED_VOICE.sort(reverse=True)
        if list_voices:
            print(", ".join(self.SUPPORTED_VOICE))

    def preprocess(self, rate, volume, pitch):
        if rate >= 0:
            rate = f'+{rate}%'
        else:
            rate = f'{rate}%'
        if pitch >= 0:
            pitch = f'+{pitch}Hz'
        else:
            pitch = f'{pitch}Hz'
        volume = 100 - volume
        volume = f'-{volume}%'
        return rate, volume, pitch

    def predict(self,TEXT, VOICE, RATE, VOLUME, PITCH, OUTPUT_FILE='result.wav', OUTPUT_SUBS='result.vtt', words_in_cue = 8):
        async def amain() -> None:
            """Main function"""
            rate, volume, pitch = self.preprocess(rate = RATE, volume = VOLUME, pitch = PITCH)
            communicate = Communicate(TEXT, VOICE, rate = rate, volume = volume, pitch = pitch)
            subs: SubMaker = SubMaker()
            sub_file: Union[TextIOWrapper, TextIO] = (
                open(OUTPUT_SUBS, "w", encoding="utf-8")
            )
            async for chunk in communicate.stream():
                if chunk["type"] == "audio":
                    # audio_file.write(chunk["data"])
                    pass
                elif chunk["type"] == "WordBoundary":
                    # print((chunk["offset"], chunk["duration"]), chunk["text"])
                    subs.create_sub((chunk["offset"], chunk["duration"]), chunk["text"])
            sub_file.write(subs.generate_subs(words_in_cue))
            await communicate.save(OUTPUT_FILE)
            
        
        # loop = asyncio.get_event_loop_policy().get_event_loop()
        # try:
        #     loop.run_until_complete(amain())
        # finally:
        #     loop.close()
        asyncio.run(amain())
        with open(OUTPUT_SUBS, 'r', encoding='utf-8') as file:
            vtt_lines = file.readlines()

        # å»æ‰æ¯ä¸€è¡Œæ–‡å­—ä¸­çš„ç©ºæ ¼
        vtt_lines_without_spaces = [line.replace(" ", "") if "-->" not in line else line for line in vtt_lines]
        # print(vtt_lines_without_spaces)
        with open(OUTPUT_SUBS, 'w', encoding='utf-8') as output_file:
            output_file.writelines(vtt_lines_without_spaces)
        return OUTPUT_FILE, OUTPUT_SUBS
```

åŒæ—¶åœ¨`src`æ–‡ä»¶å¤¹ä¸‹ï¼Œå†™äº†ä¸€ä¸ªç®€æ˜“çš„`WebUI`

```bash
python TTS_app.py
```

![TTS](docs/TTS.png)

## THG - SadTalker

è¯´è¯å¤´ç”Ÿæˆä½¿ç”¨SadTalkerï¼ˆCVPR 2023ï¼‰,è¯¦æƒ…è§[https://sadtalker.github.io](https://sadtalker.github.io)

ä¸‹è½½SadTalkeræ¨¡å‹:

```bash
bash scripts/download_models.sh  
```

[Baidu (ç™¾åº¦äº‘ç›˜)](https://pan.baidu.com/s/1eF13O-8wyw4B3MtesctQyg?pwd=linl) (Password: `linl`)

## LLM - Conversation

### Linly-AI

Linlyæ¥è‡ªæ·±åœ³å¤§å­¦æ•°æ®å·¥ç¨‹å›½å®¶é‡ç‚¹å®éªŒå®¤,å‚è€ƒ[https://github.com/CVI-SZU/Linly](https://github.com/CVI-SZU/Linly)

ä¸‹è½½Linlyæ¨¡å‹:[https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf](https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf)

å¯ä»¥ä½¿ç”¨`git`ä¸‹è½½

```bash
git lfs install
git clone https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf
```

æˆ–è€…ä½¿ç”¨`huggingface`çš„ä¸‹è½½å·¥å…·`huggingface-cli`

```bash
pip install -U huggingface_hub

# è®¾ç½®é•œåƒåŠ é€Ÿ
# Linux
export HF_ENDPOINT="https://hf-mirror.com"
# windows powershell
$env:HF_ENDPOINT="https://hf-mirror.com"

huggingface-cli download --resume-download Linly-AI/Chinese-LLaMA-2-7B-hf --local-dir Linly-AI/Chinese-LLaMA-2-7B-hf
```

æˆ–ä½¿ç”¨API:

```bash
# å‘½ä»¤è¡Œ
curl -X POST -H "Content-Type: application/json" -d '{"question": "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹?"}' http://url:port  

# Python
import requests

url = "http://url:port"
headers = {
  "Content-Type": "application/json"
}

data = {
  "question": "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹?" 
}

response = requests.post(url, headers=headers, json=data)
# response_text = response.content.decode("utf-8")
answer, tag = response.json()
# print(answer)
if tag == 'success':
    response_text =  answer[0]
else:
    print("fail")
print(response_text)
```

APIéƒ¨ç½²æ¨è**FastAPI**ï¼Œç°åœ¨æ›´æ–°äº† FastAPI çš„APIä½¿ç”¨ç‰ˆæœ¬ï¼ŒFastAPI æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€æ˜“ç”¨ä¸”ç°ä»£çš„Python Web æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä½¿ç”¨æœ€æ–°çš„Python ç‰¹æ€§å’Œå¼‚æ­¥ç¼–ç¨‹ï¼Œæä¾›äº†å¿«é€Ÿå¼€å‘Web API çš„èƒ½åŠ›ã€‚ è¯¥æ¡†æ¶ä¸ä»…æ˜“äºå­¦ä¹ å’Œä½¿ç”¨ï¼Œè¿˜å…·æœ‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ã€æ•°æ®éªŒè¯ç­‰å¼ºå¤§åŠŸèƒ½ã€‚ æ— è®ºæ˜¯æ„å»ºå°å‹é¡¹ç›®è¿˜æ˜¯å¤§å‹åº”ç”¨ç¨‹åºï¼ŒFastAPI éƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œæœ‰æ•ˆçš„å·¥å…·ã€‚

é¦–å…ˆå®‰è£…éƒ¨ç½²APIæ‰€ä½¿ç”¨çš„åº“

```bash
pip install fastapi==0.104.1
pip install uvicorn==0.24.0.post1
```

å…¶ä»–ä½¿ç”¨æ–¹æ³•å¤§è‡´ç›¸åŒï¼Œä¸»è¦æ˜¯ä¸åŒä»£ç å®ç°æ–¹å¼ï¼Œä¼šæ›´åŠ ç®€å•è¾¹ç•Œï¼Œå¹¶ä¸”å¤„ç†å¹¶å‘ä¹Ÿä¼šæ›´å¥½

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import uvicorn
import json
import datetime
import torch
from configs import model_path, api_port
# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    history = json_post_list.get('history')  # è·å–è¯·æ±‚ä¸­çš„å†å²è®°å½•
    max_length = json_post_list.get('max_length')  # è·å–è¯·æ±‚ä¸­çš„æœ€å¤§é•¿åº¦
    top_p = json_post_list.get('top_p')  # è·å–è¯·æ±‚ä¸­çš„top_på‚æ•°
    temperature = json_post_list.get('temperature')  # è·å–è¯·æ±‚ä¸­çš„æ¸©åº¦å‚æ•°
    
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    prompt = f"è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜ ### Instruction:{prompt}  ### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
    generate_ids = model.generate(inputs.input_ids, 
                                  max_new_tokens=max_length if max_length else 2048,
                                  do_sample=True, 
                                  top_k=20,
                                  top_p=top_p,
                                  temperature=temperature if temperature else 0.84,
                                  repetition_penalty=1.15, eos_token_id=2, bos_token_id=1,pad_token_id=0)
    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    response = response.split("### Response:")[-1]
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        # "history": history,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="cuda:0",
                                                    torch_dtype=torch.bfloat16, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    # å¯åŠ¨FastAPIåº”ç”¨
    uvicorn.run(app, host='0.0.0.0', port=api_port, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```

é»˜è®¤éƒ¨ç½²åœ¨ 7871 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨curlè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
curl -X POST "http://127.0.0.1:7871" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "å¦‚ä½•åº”å¯¹å‹åŠ›"}'
```

ä¹Ÿå¯ä»¥ä½¿ç”¨pythonä¸­çš„requestsåº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt}
    response = requests.post(url='http://127.0.0.1:7871', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½å¦‚ä½•åº”å¯¹å‹åŠ›'))
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
{
  "response":"å¯»æ±‚æ”¯æŒå’Œæ”¾æ¾ï¼Œå¹¶é‡‡å–ç§¯æçš„æªæ–½è§£å†³é—®é¢˜ã€‚",
  "status":200,
  "time":"2024-01-12 01:43:37"
}
```



### Qwen

æ¥è‡ªé˜¿é‡Œäº‘çš„Qwenï¼ŒæŸ¥çœ‹ [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)

å¦‚æœæƒ³è¦å¿«é€Ÿä½¿ç”¨ï¼Œå¯ä»¥é€‰1.8Bçš„æ¨¡å‹ï¼Œå‚æ•°æ¯”è¾ƒå°‘ï¼Œåœ¨è¾ƒå°çš„æ˜¾å­˜ä¹Ÿå¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼Œå½“ç„¶è¿™ä¸€éƒ¨åˆ†å¯ä»¥æ›¿æ¢

ä¸‹è½½ Qwen1.8B æ¨¡å‹: [https://huggingface.co/Qwen/Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat)

å¯ä»¥ä½¿ç”¨`git`ä¸‹è½½

```bash
git lfs install
git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat
```

æˆ–è€…ä½¿ç”¨`huggingface`çš„ä¸‹è½½å·¥å…·`huggingface-cli`

```bash
pip install -U huggingface_hub

# è®¾ç½®é•œåƒåŠ é€Ÿ
# Linux
export HF_ENDPOINT="https://hf-mirror.com"
# windows powershell
$env:HF_ENDPOINT="https://hf-mirror.com"

huggingface-cli download --resume-download Qwen/Qwen-1_8B-Chat --local-dir Qwen/Qwen-1_8B-Chat
```

å¦‚æœå‡ºç°äº†ä¸€äº›ç½‘ç»œé—®é¢˜ï¼Œå¤§å®¶å…¶å®å¯ä»¥ç”¨é­”æ­ç¤¾åŒºè¿›è¡Œä¸‹è½½ï¼Œé€Ÿåº¦å¾ˆå¿«ï¼Œæœ€åä¿®æ”¹è·¯å¾„å³å¯ [https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/files](https://modelscope.cn/models/qwen/Qwen-1_8B-Chat/files)

```python
# æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('qwen/Qwen-1_8B-Chat')
```

### Gemini-Pro

æ¥è‡ª Google çš„ Gemini-Proï¼Œäº†è§£æ›´å¤šè¯·è®¿é—® [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/)

è¯·æ±‚ API å¯†é’¥: [https://makersuite.google.com/](https://makersuite.google.com/)



### LLM æ¨¡å‹é€‰æ‹©

åœ¨ app.py æ–‡ä»¶ä¸­ï¼Œè½»æ¾é€‰æ‹©æ‚¨éœ€è¦çš„æ¨¡å‹ã€‚

```python
# å¯ä»¥æ³¨é‡Šæ‰é€‰æ‹©æ¨¡å‹
# llm = LLM(mode='offline').init_model('Linly', 'Linly-AI/Chinese-LLaMA-2-7B-hf')
# llm = LLM(mode='offline').init_model('Gemini', 'gemini-pro', api_key = "your api key")
# llm = LLM(mode='offline').init_model('Qwen', 'Qwen/Qwen-1_8B-Chat')

# å¯ä»¥é€šè¿‡configæ¥è®¾ç½®æ¨¡å‹
llm = LLM(mode=mode).init_model('Qwen', model_path)
```



## ä¼˜åŒ–

ä¸€äº›ä¼˜åŒ–:

- ä½¿ç”¨å›ºå®šçš„è¾“å…¥äººè„¸å›¾åƒ,æå‰æå–ç‰¹å¾,é¿å…æ¯æ¬¡è¯»å–
- ç§»é™¤ä¸å¿…è¦çš„åº“,ç¼©çŸ­æ€»æ—¶é—´
- åªä¿å­˜æœ€ç»ˆè§†é¢‘è¾“å‡º,ä¸ä¿å­˜ä¸­é—´ç»“æœ,æé«˜æ€§èƒ½
- ä½¿ç”¨OpenCVç”Ÿæˆæœ€ç»ˆè§†é¢‘,æ¯”mimwriteæ›´å¿«

## Gradio

Gradioæ˜¯ä¸€ä¸ªPythonåº“,æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼å°†æœºå™¨å­¦ä¹ æ¨¡å‹ä½œä¸ºäº¤äº’å¼Webåº”ç”¨ç¨‹åºæ¥éƒ¨ç½²ã€‚

å¯¹Linly-Talkerè€Œè¨€,ä½¿ç”¨Gradioæœ‰ä¸¤ä¸ªä¸»è¦ç›®çš„:

1. **å¯è§†åŒ–ä¸æ¼”ç¤º**:Gradioä¸ºæ¨¡å‹æä¾›ä¸€ä¸ªç®€å•çš„Web GUI,ä¸Šä¼ å›¾ç‰‡å’Œæ–‡æœ¬åå¯ä»¥ç›´è§‚åœ°çœ‹åˆ°ç»“æœã€‚è¿™æ˜¯å±•ç¤ºç³»ç»Ÿèƒ½åŠ›çš„æœ‰æ•ˆæ–¹å¼ã€‚

2. **ç”¨æˆ·äº¤äº’**:Gradioçš„GUIå¯ä»¥ä½œä¸ºå‰ç«¯,å…è®¸ç”¨æˆ·ä¸Linly-Talkerè¿›è¡Œäº¤äº’å¯¹è¯ã€‚ç”¨æˆ·å¯ä»¥ä¸Šä¼ è‡ªå·±çš„å›¾ç‰‡å¹¶è¾“å…¥é—®é¢˜,å®æ—¶è·å–å›ç­”ã€‚è¿™æä¾›äº†æ›´è‡ªç„¶çš„è¯­éŸ³äº¤äº’æ–¹å¼ã€‚

å…·ä½“æ¥è¯´,æˆ‘ä»¬åœ¨app.pyä¸­åˆ›å»ºäº†ä¸€ä¸ªGradioçš„Interface,æ¥æ”¶å›¾ç‰‡å’Œæ–‡æœ¬è¾“å…¥,è°ƒç”¨å‡½æ•°ç”Ÿæˆå›åº”è§†é¢‘,åœ¨GUIä¸­æ˜¾ç¤ºå‡ºæ¥ã€‚è¿™æ ·å°±å®ç°äº†æµè§ˆå™¨äº¤äº’è€Œä¸éœ€è¦ç¼–å†™å¤æ‚çš„å‰ç«¯ã€‚

æ€»ä¹‹,Gradioä¸ºLinly-Talkeræä¾›äº†å¯è§†åŒ–å’Œç”¨æˆ·äº¤äº’çš„æ¥å£,æ˜¯å±•ç¤ºç³»ç»ŸåŠŸèƒ½å’Œè®©æœ€ç»ˆç”¨æˆ·ä½¿ç”¨ç³»ç»Ÿçš„æœ‰æ•ˆé€”å¾„ã€‚

## å¯åŠ¨

ç°åœ¨çš„å¯åŠ¨ä¸€å…±æœ‰å‡ ç§æ¨¡å¼ï¼Œå¯ä»¥é€‰æ‹©ç‰¹å®šçš„åœºæ™¯è¿›è¡Œè®¾ç½®

ç¬¬ä¸€ç§åªæœ‰å›ºå®šäº†äººç‰©é—®ç­”ï¼Œè®¾ç½®å¥½äº†äººç‰©ï¼Œçœå»äº†é¢„å¤„ç†æ—¶é—´

```bash
python app.py
```

![](docs/UI.png)

ç¬¬äºŒç§æ˜¯å¯ä»¥ä»»æ„ä¸Šä¼ å›¾ç‰‡è¿›è¡Œå¯¹è¯

```bash
python app_img.py
```

![](docs/UI2.png)

ç¬¬ä¸‰ç§æ˜¯åœ¨ç¬¬ä¸€ç§çš„åŸºç¡€ä¸ŠåŠ å…¥äº†å¤§è¯­è¨€æ¨¡å‹ï¼ŒåŠ å…¥äº†å¤šè½®çš„GPTå¯¹è¯

```bash
python app_multi.py
```

![](docs/UI3.png)

æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹

[Baidu (ç™¾åº¦äº‘ç›˜)](https://pan.baidu.com/s/1eF13O-8wyw4B3MtesctQyg?pwd=linl) (Password: `linl`)

```bash
Linly-Talker/ 
â”œâ”€â”€ app.py
â”œâ”€â”€ app_img.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ Linly-api.py
â”œâ”€â”€ Linly-api-fast.py
â”œâ”€â”€ Linly-example.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ README_zh.md
â”œâ”€â”€ request-Linly-api.py
â”œâ”€â”€ requirements_app.txt
â”œâ”€â”€ scripts
â”‚   â””â”€â”€ download_models.sh
â”œâ”€â”€	src
â”‚	â””â”€â”€ .....
â”œâ”€â”€ inputs
â”‚   â”œâ”€â”€ example.png
â”‚   â””â”€â”€ first_frame_dir
â”‚       â”œâ”€â”€ example_landmarks.txt
â”‚       â”œâ”€â”€ example.mat
â”‚       â””â”€â”€ example.png
â”œâ”€â”€ examples
â”‚   â””â”€â”€ source_image
â”‚       â”œâ”€â”€ art_0.png
â”‚       â”œâ”€â”€ ......
â”‚       â””â”€â”€ sad.png
â”œâ”€â”€ checkpoints // SadTalker æƒé‡è·¯å¾„
â”‚   â”œâ”€â”€ mapping_00109-model.pth.tar
â”‚   â”œâ”€â”€ mapping_00229-model.pth.tar
â”‚   â”œâ”€â”€ SadTalker_V0.0.2_256.safetensors
â”‚   â””â”€â”€ SadTalker_V0.0.2_512.safetensors
â”œâ”€â”€ gfpgan // GFPGAN æƒé‡è·¯å¾„
â”‚   â””â”€â”€ weights
â”‚       â”œâ”€â”€ alignment_WFLW_4HG.pth
â”‚       â””â”€â”€ detection_Resnet50_Final.pth
â”œâ”€â”€ Linly-AI // Linly æƒé‡è·¯å¾„
â”‚   â””â”€â”€ Chinese-LLaMA-2-7B-hf 
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ pytorch_model-00001-of-00002.bin
â”‚       â”œâ”€â”€ pytorch_model-00002-of-00002.bin
â”‚       â”œâ”€â”€ pytorch_model.bin.index.json
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ special_tokens_map.json
â”‚       â”œâ”€â”€ tokenizer_config.json
â”‚       â””â”€â”€ tokenizer.model
â”œâ”€â”€ Qwen // Qwen æƒé‡è·¯å¾„
â”‚   â””â”€â”€ Qwen-1_8B-Chat
â”‚       â”œâ”€â”€ cache_autogptq_cuda_256.cpp
â”‚       â”œâ”€â”€ cache_autogptq_cuda_kernel_256.cu
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ configuration_qwen.py
â”‚       â”œâ”€â”€ cpp_kernels.py
â”‚       â”œâ”€â”€ examples
â”‚       â”‚   â””â”€â”€ react_prompt.md
â”‚       â”œâ”€â”€ generation_config.json
â”‚       â”œâ”€â”€ LICENSE
â”‚       â”œâ”€â”€ model-00001-of-00002.safetensors
â”‚       â”œâ”€â”€ model-00002-of-00002.safetensors
â”‚       â”œâ”€â”€ modeling_qwen.py
â”‚       â”œâ”€â”€ model.safetensors.index.json
â”‚       â”œâ”€â”€ NOTICE
â”‚       â”œâ”€â”€ qwen_generation_utils.py
â”‚       â”œâ”€â”€ qwen.tiktoken
â”‚       â”œâ”€â”€ README.md
â”‚       â”œâ”€â”€ tokenization_qwen.py
â”‚       â””â”€â”€ tokenizer_config.json
```



## å‚è€ƒ

- [https://github.com/openai/whisper](https://github.com/openai/whisper)
- [https://github.com/rany2/edge-tts](https://github.com/rany2/edge-tts)  
- [https://github.com/CVI-SZU/Linly](https://github.com/CVI-SZU/Linly)
- [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)
- [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/)
- [https://github.com/OpenTalker/SadTalker](https://github.com/OpenTalker/SadTalker)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Kedreamix/Linly-Talker&type=Date)](https://star-history.com/#Kedreamix/Linly-Talker&Date)

