import os
import random 
import gradio as gr
import time
import torch
import gc
import warnings
warnings.filterwarnings('ignore')
from zhconv import convert
from LLM import LLM
from TTS import EdgeTTS
from src.cost_time import calculate_time

from configs import *
os.environ["GRADIO_TEMP_DIR"]= './temp'
os.environ["WEBUI"] = "true"
def get_title(title = 'Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker)'):
    description = f"""
    <p style="text-align: center; font-weight: bold;">
        <span style="font-size: 28px;">{title}</span>
        <br>
        <span style="font-size: 18px;" id="paper-info">
            [<a href="https://zhuanlan.zhihu.com/p/671006998" target="_blank">çŸ¥ä¹</a>]
            [<a href="https://www.bilibili.com/video/BV1rN4y1a76x/" target="_blank">bilibili</a>]
            [<a href="https://github.com/Kedreamix/Linly-Talker" target="_blank">GitHub</a>]
            [<a herf="https://kedreamix.github.io/" target="_blank">ä¸ªäººä¸»é¡µ</a>]
        </span>
        <br> 
        <span>Linly-Talkeræ˜¯ä¸€æ¬¾åˆ›æ–°çš„æ•°å­—äººå¯¹è¯ç³»ç»Ÿï¼Œå®ƒèåˆäº†æœ€æ–°çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ğŸ¤–ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ğŸ™ï¸ã€æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆTTSï¼‰ğŸ—£ï¸å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯ğŸ¤ã€‚</span>
    </p>
    """
    return description


# è®¾ç½®é»˜è®¤system
default_system = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'
# è®¾ç½®é»˜è®¤çš„prompt
prefix_prompt = '''è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'''

edgetts = EdgeTTS()

# è®¾å®šé»˜è®¤å‚æ•°å€¼ï¼Œå¯ä¿®æ”¹
blink_every = True
size_of_image = 256
preprocess_type = 'crop'
facerender = 'facevid2vid'
enhancer = False
is_still_mode = False
exp_weight = 1
use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
        gr.Warning(question)
    return question

def clear_memory():
    """
    æ¸…ç†PyTorchçš„æ˜¾å­˜å’Œç³»ç»Ÿå†…å­˜ç¼“å­˜ã€‚
    """
    # 1. æ¸…ç†ç¼“å­˜çš„å˜é‡
    gc.collect()  # è§¦å‘Pythonåƒåœ¾å›æ”¶
    torch.cuda.empty_cache()  # æ¸…ç†PyTorchçš„æ˜¾å­˜ç¼“å­˜
    torch.cuda.ipc_collect()  # æ¸…ç†PyTorchçš„è·¨è¿›ç¨‹é€šä¿¡ç¼“å­˜
    
    # 2. æ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼‰
    print(f"Memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB")
    print(f"Max memory allocated: {torch.cuda.max_memory_allocated() / (1024 ** 2):.2f} MB")
    print(f"Cached memory: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB")
    print(f"Max cached memory: {torch.cuda.max_memory_reserved() / (1024 ** 2):.2f} MB")

@calculate_time
def TTS_response(text, 
                 voice, rate, volume, pitch,
                 am, voc, lang, male,
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, 
                 question_audio, question, use_mic_voice,
                 tts_method = 'PaddleTTS', save_path = 'answer.wav'):
    # print(text, voice, rate, volume, pitch, am, voc, lang, male, tts_method, save_path)
    if tts_method == 'Edge-TTS':
        if not edgetts.network:
            gr.Warning("è¯·æ£€æŸ¥ç½‘ç»œæˆ–è€…ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚PaddleTTS") 
            return None, None
        try:
            edgetts.predict(text, voice, rate, volume, pitch , 'answer.wav', 'answer.vtt')
        except:
            os.system(f'edge-tts --text "{text}" --voice {voice} --write-media answer.wav --write-subtitles answer.vtt')
        return 'answer.wav', 'answer.vtt'
    elif tts_method == 'PaddleTTS':
        tts.predict(text, am, voc, lang = lang, male=male, save_path = save_path)
        return save_path, None
    elif tts_method == 'GPT-SoVITSå…‹éš†å£°éŸ³':
        if use_mic_voice:
            try:
                vits.predict(ref_wav_path = question_audio,
                                prompt_text = question,
                                prompt_language = "ä¸­æ–‡",
                                text = text, # å›ç­”
                                text_language = "ä¸­æ–‡",
                                how_to_cut = "å‡‘å››å¥ä¸€åˆ‡",
                                save_path = 'answer.wav')
                return 'answer.wav', None
            except Exception as e:
                gr.Warning("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
                return None, None
        else:
            try:
                vits.predict(ref_wav_path = inp_ref,
                                prompt_text = prompt_text,
                                prompt_language = prompt_language,
                                text = text, # å›ç­”
                                text_language = text_language,
                                how_to_cut = how_to_cut,
                                save_path = 'answer.wav')
                return 'answer.wav', None
            except Exception as e:
                gr.Warning("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
                return None, None
    return None, None
@calculate_time
def LLM_response(question_audio, question, 
                 voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0,
                 am='fastspeech2', voc='pwgan',lang='zh', male=False, 
                 inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                 tts_method = 'Edge-TTS'):
    answer = llm.generate(question, default_system)
    print(answer)
    driven_audio, driven_vtt = TTS_response(answer, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, question, use_mic_voice,
                 tts_method)
    return driven_audio, driven_vtt, answer

@calculate_time
def Talker_response(question_audio = None, method = 'SadTalker', text = '',
                    voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, 
                    am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                    inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                    tts_method = 'Edge-TTS',batch_size = 2, character = 'å¥³æ€§è§’è‰²', 
                    progress=gr.Progress(track_tqdm=True)):
    default_voice = None
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image, pic_path = r'inputs/girl.png', r'inputs/girl.png'
        crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
        first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
        crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
        pic_path = "./inputs/boy.png"
        crop_pic_path = "./inputs/first_frame_dir_boy/boy.png"
        first_coeff_path = "./inputs/first_frame_dir_boy/boy.mat"
        crop_info = ((876, 747), (0, 0, 886, 838), [10.382158280494476, 0, 886, 747.7078990925525])
        default_voice = 'zh-CN-YunyangNeural'
    else:
        gr.Warning('æœªçŸ¥è§’è‰²')
        return None
    
    voice = default_voice if not voice else voice
    
    if not voice:
        gr.Warning('è¯·é€‰æ‹©å£°éŸ³')
    
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, 
                                               voice, rate, volume, pitch, 
                                               am, voc, lang, male, 
                                               inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                               tts_method)
    if driven_audio is None:
        gr.Warning("éŸ³é¢‘æ²¡æœ‰æ­£å¸¸ç”Ÿæˆï¼Œè¯·æ£€æŸ¥TTSæ˜¯å¦æ­£ç¡®")
        return None
    if method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)
    elif method == 'Wav2Lip':
        video = talker.predict(crop_pic_path, driven_audio, batch_size, enhancer)
    elif method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + method)
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

@calculate_time
def Talker_response_img(question_audio, method, text, voice, rate, volume, pitch, 
                        am, voc, lang, male, 
                        inp_ref , prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                        tts_method,
                        source_image,
                        preprocess_type, 
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps, progress=gr.Progress(track_tqdm=True)
                    ):
    if enhancer:
        gr.Warning("è®°å¾—è¯·å…ˆå®‰è£…GFPGANåº“ï¼Œpip install gfpgan, å·²å®‰è£…å¯å¿½ç•¥")
    if not voice:
        gr.Warning("è¯·å…ˆé€‰æ‹©å£°éŸ³")
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, voice, rate, volume, pitch, 
                                               am, voc, lang, male, 
                                               inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                               tts_method = tts_method)
    if method == 'SadTalker':
        video = talker.test2(source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=fps)
    elif method == 'Wav2Lip':
        video = talker.predict(source_image, driven_audio, batch_size)
    elif method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

@calculate_time
def Talker_Say(preprocess_type, 
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps,source_image = None, source_video = None, question_audio = None, method = 'SadTalker', text = '', 
                    voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, 
                    am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                    inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                    tts_method = 'Edge-TTS', character = 'å¥³æ€§è§’è‰²',
                    progress=gr.Progress(track_tqdm=True)):
    if source_video:
        source_image = source_video
    default_voice = None
    
    voice = default_voice if not voice else voice
    
    if not voice:
        gr.Warning('è¯·é€‰æ‹©å£°éŸ³')
    
    driven_audio, driven_vtt = TTS_response(text, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, text, use_mic_voice,
                 tts_method)
    
    if method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test2(source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=fps)
    elif method == 'Wav2Lip':
        video = talker.predict(source_image, driven_audio, batch_size, enhancer)
    elif method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + method)
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video


def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history

def modify_system_session(system: str) -> str:
    if system is None or len(system) == 0:
        system = default_system
    llm.clear_history()
    return system, system, []

def clear_session():
    # clear history
    llm.clear_history()
    return '', []


def human_response(history, question_audio, talker_method, 
                   voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0, batch_size = 2, 
                  am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                  inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                  tts_method = 'Edge-TTS', character = 'å¥³æ€§è§’è‰²', progress=gr.Progress(track_tqdm=True)):
    response = history[-1][1]
    qusetion = history[-1][0]
    # driven_audio, video_vtt = 'answer.wav', 'answer.vtt'
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image, pic_path = r'./inputs/girl.png', r"./inputs/girl.png"
        crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
        first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
        crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
        pic_path = "./inputs/boy.png"
        crop_pic_path = "./inputs/first_frame_dir_boy/boy.png"
        first_coeff_path = "./inputs/first_frame_dir_boy/boy.mat"
        crop_info = ((876, 747), (0, 0, 886, 838), [10.382158280494476, 0, 886, 747.7078990925525])
        default_voice = 'zh-CN-YunyangNeural'
    voice = default_voice if not voice else voice
    # tts.predict(response, voice, rate, volume, pitch, driven_audio, video_vtt)
    driven_audio, driven_vtt = TTS_response(response, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, qusetion, use_mic_voice,
                 tts_method)
    
    if talker_method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)
    elif talker_method == 'Wav2Lip':
        video = talker.predict(crop_pic_path, driven_audio, batch_size, enhancer)
    elif talker_method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + talker_method)
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video


@calculate_time
def MuseTalker_response(source_video, bbox_shift, question_audio = None, text = '',
                    voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, 
                    am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                    inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                    tts_method = 'Edge-TTS', batch_size = 4,
                    progress=gr.Progress(track_tqdm=True)):
    default_voice = None    
    voice = default_voice if not voice else voice
    
    if not voice:
        gr.Warning('è¯·é€‰æ‹©å£°éŸ³')
    
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, 
                                               voice, rate, volume, pitch, 
                                               am, voc, lang, male, 
                                               inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                               tts_method)
    print(driven_audio, driven_vtt)
    video = musetalker.inference_noprepare(driven_audio, 
                                            source_video, 
                                            bbox_shift,
                                            batch_size,
                                            fps = 25) 
    
    if driven_vtt:
        return (video, driven_vtt)
    else:
        return video 
GPT_SoVITS_ckpt = "GPT_SoVITS/pretrained_models"
def load_vits_model(gpt_path, sovits_path, progress=gr.Progress(track_tqdm=True)):
    global vits
    print("æ¨¡å‹åŠ è½½ä¸­...", gpt_path, sovits_path)
    all_gpt_path, all_sovits_path = os.path.join(GPT_SoVITS_ckpt, gpt_path), os.path.join(GPT_SoVITS_ckpt, sovits_path)
    vits.load_model(all_gpt_path, all_sovits_path)
    gr.Info("æ¨¡å‹åŠ è½½æˆåŠŸ")
    return gpt_path, sovits_path

def list_models(dir, endwith = ".pth"):
    list_folder = os.listdir(dir)
    list_folder = [i for i in list_folder if i.endswith(endwith)]
    return list_folder

def character_change(character):
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image = r'./inputs/girl.png'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
    elif character == 'è‡ªå®šä¹‰è§’è‰²':
        # gr.Warnings("è‡ªå®šä¹‰è§’è‰²æš‚æœªæ›´æ–°ï¼Œè¯·ç»§ç»­å…³æ³¨åç»­ï¼Œå¯é€šè¿‡è‡ªç”±ä¸Šä¼ å›¾ç‰‡æ¨¡å¼è¿›è¡Œè‡ªå®šä¹‰è§’è‰²")
        source_image = None
    return source_image

def webui_setting(talk = True):
    if not talk:
        with gr.Tabs():
            with gr.TabItem('æ•°å­—äººå½¢è±¡è®¾å®š'):
                source_image = gr.Image(label="Source image", type="filepath")
    else:
        source_image = None
    with gr.Tabs("TTS Method"):
        with gr.Accordion("TTS Methodè¯­éŸ³æ–¹æ³•è°ƒèŠ‚ ", open=False):
            with gr.Tab("Edge-TTS"):
                voice = gr.Dropdown(edgetts.SUPPORTED_VOICE, 
                                    value='zh-CN-XiaoxiaoNeural', 
                                    label="Voice")
                rate = gr.Slider(minimum=-100,
                                    maximum=100,
                                    value=0,
                                    step=1.0,
                                    label='Rate')
                volume = gr.Slider(minimum=0,
                                        maximum=100,
                                        value=100,
                                        step=1,
                                        label='Volume')
                pitch = gr.Slider(minimum=-100,
                                    maximum=100,
                                    value=0,
                                    step=1,
                                    label='Pitch')
            with gr.Tab("PaddleTTS"):
                am = gr.Dropdown(["FastSpeech2"], label="å£°å­¦æ¨¡å‹é€‰æ‹©", value = 'FastSpeech2')
                voc = gr.Dropdown(["PWGan", "HifiGan"], label="å£°ç å™¨é€‰æ‹©", value = 'PWGan')
                lang = gr.Dropdown(["zh", "en", "mix", "canton"], label="è¯­è¨€é€‰æ‹©", value = 'zh')
                male = gr.Checkbox(label="ç”·å£°(Male)", value=False)
            with gr.Tab('GPT-SoVITS'):
                with gr.Row():
                    gpt_path = gr.FileExplorer(root = GPT_SoVITS_ckpt, glob = "*.ckpt", value = "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt", file_count='single', label="GPTæ¨¡å‹è·¯å¾„")
                    sovits_path = gr.FileExplorer(root = GPT_SoVITS_ckpt, glob = "*.pth", value = "s2G488k.pth", file_count='single', label="SoVITSæ¨¡å‹è·¯å¾„")
                    # gpt_path = gr.Dropdown(choices=list_models(GPT_SoVITS_ckpt, 'ckpt'))
                    # sovits_path = gr.Dropdown(choices=list_models(GPT_SoVITS_ckpt, 'pth'))
                    # gpt_path = gr.Textbox(label="GPTæ¨¡å‹è·¯å¾„", 
                    #                       value="GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt")
                    # sovits_path = gr.Textbox(label="SoVITSæ¨¡å‹è·¯å¾„", 
                    #                          value="GPT_SoVITS/pretrained_models/s2G488k.pth")
                button = gr.Button("åŠ è½½æ¨¡å‹")
                button.click(fn = load_vits_model, 
                             inputs=[gpt_path, sovits_path], 
                             outputs=[gpt_path, sovits_path])
                
                with gr.Row():
                    inp_ref = gr.Audio(label="è¯·ä¸Šä¼ 3~10ç§’å†…å‚è€ƒéŸ³é¢‘ï¼Œè¶…è¿‡ä¼šæŠ¥é”™ï¼", sources=["microphone", "upload"], type="filepath")
                    use_mic_voice = gr.Checkbox(label="ä½¿ç”¨è¯­éŸ³é—®ç­”çš„éº¦å…‹é£")
                    prompt_text = gr.Textbox(label="å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬", value="")
                    prompt_language = gr.Dropdown(
                        label="å‚è€ƒéŸ³é¢‘çš„è¯­ç§", choices=["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡"], value="ä¸­æ–‡"
                    )
                asr_button = gr.Button("è¯­éŸ³è¯†åˆ« - å…‹éš†å‚è€ƒéŸ³é¢‘")
                asr_button.click(fn=Asr,inputs=[inp_ref],outputs=[prompt_text])
                with gr.Row():
                    text_language = gr.Dropdown(
                        label="éœ€è¦åˆæˆçš„è¯­ç§", choices=["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "ä¸­è‹±æ··åˆ", "æ—¥è‹±æ··åˆ", "å¤šè¯­ç§æ··åˆ"], value="ä¸­æ–‡"
                    )
                    
                    how_to_cut = gr.Dropdown(
                        label="æ€ä¹ˆåˆ‡",
                        choices=["ä¸åˆ‡", "å‡‘å››å¥ä¸€åˆ‡", "å‡‘50å­—ä¸€åˆ‡", "æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡", "æŒ‰è‹±æ–‡å¥å·.åˆ‡", "æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡" ],
                        value="å‡‘å››å¥ä¸€åˆ‡",
                        interactive=True,
                    )
            
            with gr.Column(variant='panel'): 
                batch_size = gr.Slider(minimum=1,
                                    maximum=10,
                                    value=2,
                                    step=1,
                                    label='Talker Batch size')

    character = gr.Radio(['å¥³æ€§è§’è‰²', 
                          'ç”·æ€§è§’è‰²', 
                          'è‡ªå®šä¹‰è§’è‰²'], 
                         label="è§’è‰²é€‰æ‹©", value='è‡ªå®šä¹‰è§’è‰²')
    # character.change(fn = character_change, inputs=[character], outputs = [source_image])
    tts_method = gr.Radio(['Edge-TTS', 'PaddleTTS', 'GPT-SoVITSå…‹éš†å£°éŸ³', 'Comming Soon!!!'], label="Text To Speech Method", 
                                              value = 'Edge-TTS')
    tts_method.change(fn = tts_model_change, inputs=[tts_method], outputs = [tts_method])
    asr_method = gr.Radio(choices = ['Whisper-tiny', 'Whisper-base', 'FunASR', 'Comming Soon!!!'], value='Whisper-base', label = 'è¯­éŸ³è¯†åˆ«æ¨¡å‹é€‰æ‹©')
    asr_method.change(fn = asr_model_change, inputs=[asr_method], outputs = [asr_method])
    talker_method = gr.Radio(choices = ['SadTalker', 'Wav2Lip', 'ER-NeRF', 'Comming Soon!!!'], 
                      value = 'SadTalker', label = 'æ•°å­—äººæ¨¡å‹é€‰æ‹©')
    talker_method.change(fn = talker_model_change, inputs=[talker_method], outputs = [talker_method])
    llm_method = gr.Dropdown(choices = ['Qwen', 'Qwen2', 'Linly', 'Gemini', 'ChatGLM', 'ChatGPT', 'GPT4Free', 'ç›´æ¥å›å¤ Direct Reply', 'Comming Soon!!!'], value = 'ç›´æ¥å›å¤ Direct Reply', label = 'LLM æ¨¡å‹é€‰æ‹©')
    llm_method.change(fn = llm_model_change, inputs=[llm_method], outputs = [llm_method])
    return  (source_image, voice, rate, volume, pitch, 
             am, voc, lang, male, 
             inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
             tts_method, batch_size, character, talker_method, asr_method, llm_method)


def exmaple_setting(asr, text, character, talk , tts, voice, llm):
    # é»˜è®¤textçš„Example
    examples =  [
        ['Whisper-base', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-XiaoxiaoNeural', 'ç›´æ¥å›å¤ Direct Reply'],
        ['Whisper-tiny', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'PaddleTTS', 'None', 'ç›´æ¥å›å¤ Direct Reply'],
        ['Whisper-base', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-XiaoxiaoNeural', 'Qwen'],
        ['FunASR', 'å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ','ç”·æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-YunyangNeural', 'Qwen'],
        ['Whisper-tiny', 'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ','å¥³æ€§è§’è‰²', 'Wav2Lip', 'PaddleTTS', 'None', 'Qwen'],
        ]

    with gr.Row(variant='panel'):
        with gr.Column(variant='panel'):
            gr.Markdown("## Test Examples")
            gr.Examples(
                examples = examples,
                inputs = [asr, text, character, talk , tts, voice, llm],
            )
def app():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) æ–‡æœ¬/è¯­éŸ³å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                (source_image, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
             
            
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                # with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                #     with gr.Accordion("Advanced Settings",
                #                     open=False):
                #         gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                #         with gr.Column(variant='panel'):
                #             # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                #             # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                #             with gr.Row():
                #                 pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                #                 exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                #                 blink_every = gr.Checkbox(label="use eye blink", value=True)

                #             with gr.Row():
                #                 size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                #                 preprocess_type = gr.Radio(['crop', 'resize','full'], value='full', label='preprocess', info="How to handle input image?")
                            
                #             with gr.Row():
                #                 is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                #                 facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                
                #             with gr.Row():
                #                 # batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                #                 fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                #                 enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")       
                with gr.Tabs():
                    with gr.TabItem('æ•°å­—äººé—®ç­”'):
                        gen_video = gr.Video(label="ç”Ÿæˆè§†é¢‘", format="mp4", autoplay=False)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘", variant='primary')
            video_button.click(fn=Talker_response,inputs=[question_audio, talker_method, input_text, voice, rate, volume, pitch,
                                                          am, voc, lang, male, 
                                                          inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                                          tts_method, batch_size, character],outputs=[gen_video])
        exmaple_setting(asr_method, input_text, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_multi():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) å¤šè½®GPTå¯¹è¯"))
        with gr.Row():
            with gr.Column():
                (source_image, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
                video = gr.Video(label = 'æ•°å­—äººé—®ç­”', scale = 0.5)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯¹è¯åï¼‰", variant = 'primary')
            
            with gr.Column():
                with gr.Row():
                    with gr.Column(scale=3):
                        system_input = gr.Textbox(value=default_system, lines=1, label='System (è®¾å®šè§’è‰²)')
                    with gr.Column(scale=1):
                        modify_system = gr.Button("ğŸ› ï¸ è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯", scale=2)
                    system_state = gr.Textbox(value=default_system, visible=False)

                chatbot = gr.Chatbot(height=400, show_copy_button=True)
                with gr.Group():
                    question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label='è¯­éŸ³å¯¹è¯', autoplay=False)
                    asr_text = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                
                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                msg = gr.Textbox(label="Prompt/é—®é¢˜")
                asr_text.click(fn=Asr,inputs=[question_audio],outputs=[msg])
                
                with gr.Row():
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    sumbit = gr.Button("ğŸš€ å‘é€", variant = 'primary')
                    
            # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
            sumbit.click(chat_response, inputs=[system_input, msg, chatbot], 
                         outputs=[msg, chatbot])
            
            # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
            clear_history.click(fn = clear_session, outputs = [msg, chatbot])
            
            # è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯
            modify_system.click(fn=modify_system_session,
                        inputs=[system_input],
                        outputs=[system_state, system_input, chatbot])
            
            video_button.click(fn = human_response, inputs = [chatbot, question_audio, talker_method, 
                                                              voice, rate, volume, pitch, batch_size,
                                                             am, voc, lang, male, 
                                                             inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  
                                                             use_mic_voice, tts_method, character], outputs = [video])
            
        exmaple_setting(asr_method, msg, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_img():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) ä»»æ„å›¾ç‰‡å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'):
                with gr.Tabs(elem_id="sadtalker_source_image"):
                        with gr.TabItem('Source image'):
                            with gr.Row():
                                source_image = gr.Image(label="Source image", type="filepath", elem_id="img2img_image", width=512)
                                
                (_, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
                            
                
                    
            # driven_audio = 'answer.wav'           
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                with gr.Tabs(elem_id="text_examples"): 
                    gr.Markdown("## Text Examples")
                    examples =  [
                        ['åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ'],
                        ['å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ'],
                        ['ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ'],
                    ]
                    gr.Examples(
                        examples = examples,
                        inputs = [input_text],
                    )
                with gr.Tabs(elem_id="sadtalker_checkbox"):
                    with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings",
                                        open=False):
                            gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                            with gr.Column(variant='panel'):
                                # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                                # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)

                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                                    preprocess_type = gr.Radio(['crop', 'resize','full', 'extcrop', 'extfull'], value='crop', label='preprocess', info="How to handle input image?")
                                
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                    
                                with gr.Row():
                                    batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")                                               

                with gr.Tabs(elem_id="sadtalker_genearted"):
                    gen_video = gr.Video(label="Generated video", format="mp4")

                submit = gr.Button('ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘', elem_id="sadtalker_generate", variant='primary')
            submit.click(
                fn=Talker_response_img,
                inputs=[question_audio,
                        talker_method, 
                        input_text,
                        voice, rate, volume, pitch,
                        am, voc, lang, male, 
                        inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                        tts_method,
                        source_image, 
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps], 
                outputs=[gen_video]
                )
        
        with gr.Row():
            examples = [
                [
                    'examples/source_image/full_body_2.png', 'SadTalker',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/full_body_1.png', 'SadTalker',
                    'full',
                    True,
                    False
                ],
                [
                    'examples/source_image/full4.jpeg', 'SadTalker',
                    'crop',
                    False,
                    True
                ],
            ]
            gr.Examples(examples=examples,
                        inputs=[
                            source_image, talker_method,
                            preprocess_type,
                            is_still_mode,
                            enhancer], 
                        outputs=[gen_video],
                        # cache_examples=True,
                        )
    return inference

def app_vits():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) è¯­éŸ³å…‹éš†"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                (source_image, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                with gr.Tabs():
                    with gr.TabItem('æ•°å­—äººé—®ç­”'):
                        gen_video = gr.Video(label="Generated video", format="mp4", autoplay=False)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘", variant='primary')
            video_button.click(fn=Talker_response,inputs=[question_audio, talker_method, input_text, voice, rate, volume, pitch, am, voc, lang, male, 
                            inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                            tts_method, batch_size, character],outputs=[gen_video])
        exmaple_setting(asr_method, input_text, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_talk():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) æ•°å­—äººæ’­æŠ¥"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.Tab("å›¾ç‰‡äººç‰©"):
                        source_image = gr.Image(label='Source image', type = 'filepath')
                        
                    with gr.Tab("è§†é¢‘äººç‰©"):
                        source_video = gr.Video(label="Source video")
               
                (_, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
        
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text]) 
                with gr.Tabs():
                    with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings",
                                        open=False):
                            gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                            with gr.Column(variant='panel'):
                                # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                                # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)

                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                                    preprocess_type = gr.Radio(['crop', 'resize','full'], value='full', label='preprocess', info="How to handle input image?")
                                
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                    
                                with gr.Row():
                                    # batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")                                               

                with gr.Tabs():
                    gen_video = gr.Video(label="Generated video", format="mp4")

                video_button = gr.Button('ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘', elem_id="sadtalker_generate", variant='primary')

                video_button.click(fn=Talker_Say,inputs=[preprocess_type, is_still_mode, enhancer, batch_size, size_of_image,
                                pose_style, facerender, exp_weight, blink_every, fps,
                                source_image, source_video, question_audio, talker_method, input_text, voice, rate, volume, pitch, am, voc, lang, male, 
                                inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                                tts_method, character],outputs=[gen_video])
            
        with gr.Row():
            with gr.Column(variant='panel'):
                gr.Markdown("## Test Examples")
                gr.Examples(
                    examples = [
                        [
                            'examples/source_image/full_body_2.png',
                            'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ',
                        ],
                        [
                            'examples/source_image/full_body_1.png',
                            'å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ',
                        ],
                        [
                            'examples/source_image/full3.png',
                            'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ',
                        ],
                    ],
                    fn = Talker_Say,
                    inputs = [source_image, input_text],
                )   
    return inference

def load_musetalk_model():
    gr.Info("MuseTalkæ¨¡å‹å¯¼å…¥ä¸­...")
    musetalker.init_model()
    gr.Info("MuseTalkæ¨¡å‹å¯¼å…¥æˆåŠŸ")
    return "MuseTalkæ¨¡å‹å¯¼å…¥æˆåŠŸ"
def musetalk_prepare_material(source_video, bbox_shift):
    if musetalker.load is False:
        gr.Warning("è¯·å…ˆåŠ è½½MuseTalkæ¨¡å‹åé‡æ–°ä¸Šä¼ æ–‡ä»¶")
        return source_video, None
    return musetalker.prepare_material(source_video, bbox_shift)
def app_muse():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) MuseTalkeræ•°å­—äººå®æ—¶å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                with gr.TabItem('MuseV Video'):
                    gr.Markdown("MuseV: need help? please visit MuseVDemo to generate Video https://huggingface.co/spaces/AnchorFake/MuseVDemo")
                    with gr.Row():
                        source_video = gr.Video(label="Reference Video",sources=['upload'])
                    gr.Markdown("BBox_shift æ¨èå€¼ä¸‹é™ï¼Œåœ¨ç”Ÿæˆåˆå§‹ç»“æœåç”Ÿæˆç›¸åº”çš„ bbox èŒƒå›´ã€‚å¦‚æœç»“æœä¸ç†æƒ³ï¼Œå¯ä»¥æ ¹æ®è¯¥å‚è€ƒå€¼è¿›è¡Œè°ƒæ•´ã€‚\nä¸€èˆ¬æ¥è¯´ï¼Œåœ¨æˆ‘ä»¬çš„å®éªŒè§‚å¯Ÿä¸­ï¼Œæˆ‘ä»¬å‘ç°æ­£å€¼ï¼ˆå‘ä¸‹åŠéƒ¨åˆ†ç§»åŠ¨ï¼‰é€šå¸¸ä¼šå¢åŠ å˜´å·´çš„å¼ å¼€åº¦ï¼Œè€Œè´Ÿå€¼ï¼ˆå‘ä¸ŠåŠéƒ¨åˆ†ç§»åŠ¨ï¼‰é€šå¸¸ä¼šå‡å°‘å˜´å·´çš„å¼ å¼€åº¦ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™å¹¶ä¸æ˜¯ç»å¯¹çš„è§„åˆ™ï¼Œç”¨æˆ·å¯èƒ½éœ€è¦æ ¹æ®ä»–ä»¬çš„å…·ä½“éœ€æ±‚å’ŒæœŸæœ›æ•ˆæœæ¥è°ƒæ•´è¯¥å‚æ•°ã€‚")
                    with gr.Row():
                        bbox_shift = gr.Number(label="BBox_shift value, px", value=0)
                        bbox_shift_scale = gr.Textbox(label="bbox_shift_scale", 
                                                        value="",interactive=False)
                load_musetalk = gr.Button("åŠ è½½MuseTalkæ¨¡å‹(ä¼ å…¥è§†é¢‘å‰å…ˆåŠ è½½)", variant='primary')
                load_musetalk.click(fn=load_musetalk_model, outputs=bbox_shift_scale)

                (_, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
            
            source_video.change(fn=musetalk_prepare_material, inputs=[source_video, bbox_shift], outputs=[source_video, bbox_shift_scale])
            
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text]) 
            
                with gr.TabItem("MuseTalk Video"):
                    gen_video = gr.Video(label="Generated video", format="mp4")
                submit = gr.Button('Generate', elem_id="sadtalker_generate", variant='primary')
                examples = [os.path.join('Musetalk/data/video', video) for video in os.listdir("Musetalk/data/video")]
                # ['Musetalk/data/video/yongen_musev.mp4', 'Musetalk/data/video/musk_musev.mp4', 'Musetalk/data/video/monalisa_musev.mp4', 'Musetalk/data/video/sun_musev.mp4', 'Musetalk/data/video/seaside4_musev.mp4', 'Musetalk/data/video/sit_musev.mp4', 'Musetalk/data/video/man_musev.mp4']
                
                gr.Markdown("## MuseV Video Examples")
                gr.Examples(
                    examples=[
                        ['Musetalk/data/video/yongen_musev.mp4', 5],
                        ['Musetalk/data/video/musk_musev.mp4', 5],
                        ['Musetalk/data/video/monalisa_musev.mp4', 5],
                        ['Musetalk/data/video/sun_musev.mp4', 5],
                        ['Musetalk/data/video/seaside4_musev.mp4', 5],
                        ['Musetalk/data/video/sit_musev.mp4', 5],
                        ['Musetalk/data/video/man_musev.mp4', 5]
                        ],
                    inputs =[source_video, bbox_shift], 
                )

            submit.click(
                fn=MuseTalker_response,
                inputs=[source_video, bbox_shift, question_audio, input_text, voice, rate, volume, pitch, am, voc, lang, male, 
                            inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                            tts_method, batch_size], 
                outputs=[gen_video]
                )
    return inference
def asr_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global asr

    # æ¸…ç†æ˜¾å­˜ï¼Œåœ¨åŠ è½½æ–°çš„æ¨¡å‹ä¹‹å‰é‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜
    clear_memory()

    if model_name == "Whisper-tiny":
        try:
            asr = WhisperASR('tiny')
            # asr = WhisperASR('Whisper/tiny.pt')
            gr.Info("Whisper-tinyæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Whisper-tinyæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == "Whisper-base":
        try:
            asr = WhisperASR('base')
            # asr = WhisperASR('Whisper/base.pt')
            gr.Info("Whisper-baseæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Whisper-baseæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'FunASR':
        try:
            from ASR import FunASR
            asr = FunASR()
            gr.Info("FunASRæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"FunASRæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    else:
        gr.Warning("æœªçŸ¥ASRæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name

def llm_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global llm
    gemini_apikey = ""
    openai_apikey = ""
    proxy_url = None

    # æ¸…ç†æ˜¾å­˜ï¼Œåœ¨åŠ è½½æ–°çš„æ¨¡å‹ä¹‹å‰é‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜
    clear_memory()

    if model_name == 'Linly':
        try:
            llm = llm_class.init_model('Linly', 'Linly-AI/Chinese-LLaMA-2-7B-hf', prefix_prompt=prefix_prompt)
            gr.Info("Linlyæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Linlyæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'Qwen':
        try:
            llm = llm_class.init_model('Qwen', 'Qwen/Qwen-1_8B-Chat', prefix_prompt=prefix_prompt)
            gr.Info("Qwenæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Qwenæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'Qwen2':
        try:
            llm = llm_class.init_model('Qwen2', 'Qwen/Qwen1.5-0.5B-Chat', prefix_prompt=prefix_prompt)
            gr.Info("Qwen2æ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Qwen2æ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'Gemini':
        if gemini_apikey:
            llm = llm_class.init_model('Gemini', 'gemini-pro', gemini_apikey, proxy_url)
            gr.Info("Geminiæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        else:
            gr.Warning("è¯·å¡«å†™Geminiçš„api_key")
    elif model_name == 'ChatGLM':
        try:
            llm = llm_class.init_model('ChatGLM', 'THUDM/chatglm3-6b', prefix_prompt=prefix_prompt)
            gr.Info("ChatGLMæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"ChatGLMæ¨¡å‹å¯¼å…¥å¤±è´¥ {e}")
    elif model_name == 'ChatGPT':
        if openai_apikey:
            llm = llm_class.init_model('ChatGPT', api_key=openai_apikey, proxy_url=proxy_url, prefix_prompt=prefix_prompt)
        else:
            gr.Warning("è¯·å¡«å†™OpenAIçš„api_key")
    elif model_name == 'ç›´æ¥å›å¤ Direct Reply':
        llm =llm_class.init_model(model_name)
        gr.Info("ç›´æ¥å›å¤ï¼Œä¸å®ç”¨LLMæ¨¡å‹")
    elif model_name == 'GPT4Free':
        try:
            llm = llm_class.init_model('GPT4Free', prefix_prompt=prefix_prompt)
            gr.Info("GPT4Freeæ¨¡å‹å¯¼å…¥æˆåŠŸ, è¯·æ³¨æ„GPT4Freeå¯èƒ½ä¸ç¨³å®š")
        except Exception as e:
            gr.Warning(f"GPT4Freeæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    else:
        gr.Warning("æœªçŸ¥LLMæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name
    
def talker_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global talker

    # æ¸…ç†æ˜¾å­˜ï¼Œåœ¨åŠ è½½æ–°çš„æ¨¡å‹ä¹‹å‰é‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜
    clear_memory()

    if model_name not in ['SadTalker', 'Wav2Lip', 'ER-NeRF']:
        gr.Warning("å…¶ä»–æ¨¡å‹è¿˜æœªé›†æˆï¼Œè¯·ç­‰å¾…")
    if model_name == 'SadTalker':
        try:
            from TFG import SadTalker
            talker = SadTalker(lazy_load=True)
            gr.Info("SadTalkeræ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning("SadTalkeræ¨¡å‹ä¸‹è½½å¤±è´¥", e)
    elif model_name == 'Wav2Lip':
        try:
            from TFG import Wav2Lip
            clear_memory()
            talker = Wav2Lip("checkpoints/wav2lip_gan.pth")
            gr.Info("Wav2Lipæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning("Wav2Lipæ¨¡å‹ä¸‹è½½å¤±è´¥", e)
    elif model_name == 'ER-NeRF':
        try:
            from TFG import ERNeRF
            talker = ERNeRF()
            talker.init_model('checkpoints/Obama_ave.pth', 'checkpoints/Obama.json')
            gr.Info("ER-NeRFæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning("ER-NeRFæ¨¡å‹ä¸‹è½½å¤±è´¥", e)
    else:
        gr.Warning("æœªçŸ¥TFGæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name

def tts_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global tts

    # æ¸…ç†æ˜¾å­˜ï¼Œåœ¨åŠ è½½æ–°çš„æ¨¡å‹ä¹‹å‰é‡Šæ”¾ä¸å¿…è¦çš„æ˜¾å­˜
    clear_memory()

    if model_name == 'Edge-TTS':
        # tts = EdgeTTS()
        if edgetts.network:
            gr.Info("EdgeTTSæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        else:
            gr.Warning("EdgeTTSæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ­£å¸¸è¿æ¥ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨")
    elif model_name == 'PaddleTTS':
        try:
            from TTS import PaddleTTS
            tts = PaddleTTS()
            gr.Info("PaddleTTSæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"PaddleTTSæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'GPT-SoVITSå…‹éš†å£°éŸ³':
        try:
            gpt_path = "GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
            sovits_path = "GPT_SoVITS/pretrained_models/s2G488k.pth"
            vits.load_model(gpt_path, sovits_path)
            gr.Info("æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"æ¨¡å‹åŠ è½½å¤±è´¥ {e}")
        gr.Warning("æ³¨æ„æ³¨æ„âš ï¸ï¼šGPT-SoVITSè¦ä¸Šä¼ å‚è€ƒéŸ³é¢‘è¿›è¡Œå…‹éš†ï¼Œè¯·ç‚¹å‡»TTS Methodè¯­éŸ³æ–¹æ³•è°ƒèŠ‚æ“ä½œ")
    else:
        gr.Warning("æœªçŸ¥TTSæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name

def success_print(text):
    print(f"\033[1;32;40m{text}\033[0m")

def error_print(text):
    print(f"\033[1;31;40m{text}\033[0m")

if __name__ == "__main__":
    llm_class = LLM(mode='offline')
    llm = llm_class.init_model('ç›´æ¥å›å¤ Direct Reply')
    success_print("é»˜è®¤ä¸ä½¿ç”¨LLMæ¨¡å‹ï¼Œç›´æ¥å›å¤é—®é¢˜ï¼ŒåŒæ—¶å‡å°‘æ˜¾å­˜å ç”¨ï¼")
    
    try:
        from VITS import *
        vits = GPT_SoVITS()
        success_print("Success!!! GPT-SoVITSæ¨¡å—åŠ è½½æˆåŠŸï¼Œè¯­éŸ³å…‹éš†é»˜è®¤ä½¿ç”¨GPT-SoVITSæ¨¡å‹")
        # gpt_path = "GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
        # sovits_path = "GPT_SoVITS/pretrained_models/s2G488k.pth"
        # vits.load_model(gpt_path, sovits_path)
    except Exception as e:
        error_print(f"GPT-SoVITS Error: {e}")
        error_print("å¦‚æœä½¿ç”¨VITSï¼Œè¯·å…ˆä¸‹è½½GPT-SoVITSæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    
    try:
        from TFG import SadTalker
        talker = SadTalker(lazy_load=True)
        success_print("Success!!! SadTalkeræ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨SadTalkeræ¨¡å‹")
    except Exception as e:
        error_print(f"SadTalker Error: {e}")
        error_print("å¦‚æœä½¿ç”¨SadTalkerï¼Œè¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")
    
    try:
        from ASR import WhisperASR
        asr = WhisperASR('base')
        success_print("Success!!! WhisperASRæ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨Whisper-baseæ¨¡å‹")
    except Exception as e:
        error_print(f"ASR Error: {e}")
        error_print("å¦‚æœä½¿ç”¨FunASRï¼Œè¯·å…ˆä¸‹è½½WhisperASRæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    
    # åˆ¤æ–­æ˜¾å­˜æ˜¯å¦8gï¼Œè‹¥å°äº8gä¸å»ºè®®ä½¿ç”¨MuseTalkåŠŸèƒ½
    # Check if GPU is available and has at least 8GB of memory
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert bytes to GB
        if gpu_memory < 8:
            error_print("è­¦å‘Š: æ‚¨çš„æ˜¾å¡æ˜¾å­˜å°äº8GBï¼Œä¸å»ºè®®ä½¿ç”¨MuseTalkåŠŸèƒ½")
    
    try:
        from TFG import MuseTalk_RealTime
        musetalker = MuseTalk_RealTime()
        success_print("Success!!! MuseTalkæ¨¡å—åŠ è½½æˆåŠŸ")
    except Exception as e:
        error_print(f"MuseTalk Error: {e}")
        error_print("å¦‚æœä½¿ç”¨MuseTalkï¼Œè¯·å…ˆä¸‹è½½MuseTalkæ¨¡å‹")

    tts = edgetts
    if not tts.network:
        error_print("EdgeTTSæ¨¡å—åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ­£å¸¸è¿æ¥ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨")

    gr.close_all()
    demo_app = app()
    demo_img = app_img()
    demo_multi = app_multi()
    demo_vits = app_vits()
    demo_talk = app_talk()
    demo_muse = app_muse()
    demo = gr.TabbedInterface(interface_list = [demo_app, 
                                                demo_img, 
                                                demo_multi, 
                                                demo_vits, 
                                                demo_talk,
                                                demo_muse,
                                                ], 
                              tab_names = ["æ–‡æœ¬/è¯­éŸ³å¯¹è¯", 
                                           "ä»»æ„å›¾ç‰‡å¯¹è¯", 
                                           "å¤šè½®GPTå¯¹è¯", 
                                           "è¯­éŸ³å…‹éš†æ•°å­—äººå¯¹è¯", 
                                           "æ•°å­—äººæ–‡æœ¬/è¯­éŸ³æ’­æŠ¥",
                                           "MuseTalkæ•°å­—äººå®æ—¶å¯¹è¯"
                                           ],
                              title = "Linly-Talker WebUI")
    demo.queue()
    demo.launch(server_name=ip, # æœ¬åœ°ç«¯å£localhost:127.0.0.1 å…¨å±€ç«¯å£è½¬å‘:"0.0.0.0"
                server_port=port,
                # ä¼¼ä¹åœ¨Gradio4.0ä»¥ä¸Šç‰ˆæœ¬å¯ä»¥ä¸ä½¿ç”¨è¯ä¹¦ä¹Ÿå¯ä»¥è¿›è¡Œéº¦å…‹é£å¯¹è¯
                # ssl_certfile=ssl_certfile,
                # ssl_keyfile=ssl_keyfile,
                # ssl_verify=False,
                # share=True,
                debug=True,
                )