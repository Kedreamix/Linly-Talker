# æ•°å­—äººå¯¹è¯ç³»ç»Ÿ - Linly-Talker â€”â€” â€œæ•°å­—äººäº¤äº’ï¼Œä¸è™šæ‹Ÿçš„è‡ªå·±äº’åŠ¨â€

[English](./README_en.md) [ç®€ä½“ä¸­æ–‡](./README.md)

**2023.12 æ›´æ–°** ğŸ“†

**ç”¨æˆ·å¯ä»¥ä¸Šä¼ ä»»æ„å›¾ç‰‡è¿›è¡Œå¯¹è¯**

**2024.01 æ›´æ–°** ğŸ“†

- **ä»¤äººå…´å¥‹çš„æ¶ˆæ¯ï¼æˆ‘ç°åœ¨å·²ç»å°†å¼ºå¤§çš„GeminiProå’ŒQwenå¤§æ¨¡å‹èå…¥åˆ°æˆ‘ä»¬çš„å¯¹è¯åœºæ™¯ä¸­ã€‚ç”¨æˆ·ç°åœ¨å¯ä»¥åœ¨å¯¹è¯ä¸­ä¸Šä¼ ä»»ä½•å›¾ç‰‡ï¼Œä¸ºæˆ‘ä»¬çš„äº’åŠ¨å¢æ·»äº†å…¨æ–°çš„å±‚é¢ã€‚**
-  **æ›´æ–°äº†FastAPIçš„éƒ¨ç½²è°ƒç”¨æ–¹æ³•ã€‚** 
- **æ›´æ–°äº†å¾®è½¯TTSçš„é«˜çº§è®¾ç½®é€‰é¡¹ï¼Œå¢åŠ å£°éŸ³ç§ç±»çš„å¤šæ ·æ€§ï¼Œä»¥åŠåŠ å…¥è§†é¢‘å­—å¹•åŠ å¼ºå¯è§†åŒ–ã€‚**

## ä»‹ç»

Linly-Talkeræ˜¯ä¸€ä¸ªå°†å¤§å‹è¯­è¨€æ¨¡å‹ä¸è§†è§‰æ¨¡å‹ç›¸ç»“åˆçš„æ™ºèƒ½AIç³»ç»Ÿ,åˆ›å»ºäº†ä¸€ç§å…¨æ–°çš„äººæœºäº¤äº’æ–¹å¼ã€‚å®ƒé›†æˆäº†å„ç§æŠ€æœ¯,ä¾‹å¦‚Whisperã€Linlyã€å¾®è½¯è¯­éŸ³æœåŠ¡å’ŒSadTalkerä¼šè¯´è¯çš„ç”Ÿæˆç³»ç»Ÿã€‚è¯¥ç³»ç»Ÿéƒ¨ç½²åœ¨Gradioä¸Š,å…è®¸ç”¨æˆ·é€šè¿‡æä¾›å›¾åƒä¸AIåŠ©æ‰‹è¿›è¡Œäº¤è°ˆã€‚ç”¨æˆ·å¯ä»¥æ ¹æ®è‡ªå·±çš„å–œå¥½è¿›è¡Œè‡ªç”±çš„å¯¹è¯æˆ–å†…å®¹ç”Ÿæˆã€‚

![The system architecture of multimodal humanâ€“computer interaction.](docs/HOI.png)

## TO DO LIST

- [x] åŸºæœ¬å®Œæˆå¯¹è¯ç³»ç»Ÿæµç¨‹ï¼Œèƒ½å¤Ÿè¯­éŸ³å¯¹è¯
- [x] åŠ å…¥äº†LLMå¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬Linlyï¼ŒQwenå’ŒGeminiProçš„ä½¿ç”¨
- [x] å¯ä¸Šä¼ ä»»æ„æ•°å­—äººç…§ç‰‡è¿›è¡Œå¯¹è¯
- [x] LinlyåŠ å…¥FastAPIè°ƒç”¨æ–¹å¼
- [x] åˆ©ç”¨å¾®è½¯TTSåŠ å…¥é«˜çº§é€‰é¡¹ï¼Œå¯è®¾ç½®å¯¹åº”äººå£°ä»¥åŠéŸ³è°ƒç­‰å‚æ•°ï¼Œå¢åŠ å£°éŸ³çš„å¤šæ ·æ€§
- [x] è§†é¢‘ç”ŸæˆåŠ å…¥å­—å¹•ï¼Œèƒ½å¤Ÿæ›´å¥½çš„è¿›è¡Œå¯è§†åŒ–
- [ ] è¯­éŸ³å…‹éš†æŠ€æœ¯ï¼ˆè¯­éŸ³å…‹éš†åˆæˆè‡ªå·±å£°éŸ³ï¼Œæé«˜æ•°å­—äººåˆ†èº«çš„çœŸå®æ„Ÿå’Œäº’åŠ¨ä½“éªŒï¼‰
- [ ] å®æ—¶è¯­éŸ³è¯†åˆ«ï¼ˆäººä¸æ•°å­—äººä¹‹é—´å°±å¯ä»¥é€šè¿‡è¯­éŸ³è¿›è¡Œå¯¹è¯äº¤æµ)
- [ ] GPTå¤šè½®å¯¹è¯ç³»ç»Ÿï¼ˆæé«˜æ•°å­—äººçš„äº¤äº’æ€§å’ŒçœŸå®æ„Ÿï¼Œå¢å¼ºæ•°å­—äººçš„æ™ºèƒ½ï¼‰

ğŸ”† è¯¥é¡¹ç›® Linly-Talker æ­£åœ¨è¿›è¡Œä¸­ - æ¬¢è¿æå‡ºPRè¯·æ±‚ï¼å¦‚æœæ‚¨æœ‰ä»»ä½•å…³äºæ–°çš„æ¨¡å‹æ–¹æ³•ã€ç ”ç©¶ã€æŠ€æœ¯æˆ–å‘ç°è¿è¡Œé”™è¯¯çš„å»ºè®®ï¼Œè¯·éšæ—¶ç¼–è¾‘å¹¶æäº¤ PRã€‚æ‚¨ä¹Ÿå¯ä»¥æ‰“å¼€ä¸€ä¸ªé—®é¢˜æˆ–é€šè¿‡ç”µå­é‚®ä»¶ç›´æ¥è”ç³»æˆ‘ã€‚ğŸ“©â­ å¦‚æœæ‚¨å‘ç°è¿™ä¸ªGithub Projectæœ‰ç”¨ï¼Œè¯·ç»™å®ƒç‚¹ä¸ªæ˜Ÿï¼ğŸ¤©

## åˆ›å»ºç¯å¢ƒ

```bash
conda create -n linly python=3.8 
conda activate linly

pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113

conda install ffmpeg 

pip install -r requirements_app.txt
```

ä¸ºäº†å¤§å®¶çš„éƒ¨ç½²ä½¿ç”¨æ–¹ä¾¿ï¼Œæ›´æ–°äº†ä¸€ä¸ª`configs.py`æ–‡ä»¶ï¼Œå¯ä»¥å¯¹å…¶è¿›è¡Œä¸€äº›è¶…å‚æ•°ä¿®æ”¹å³å¯

```bash
# è®¾å¤‡è¿è¡Œç«¯å£ (Device running port)
port = 7870
# apiè¿è¡Œç«¯å£ (API running port)
api_port = 7871
# Linlyæ¨¡å‹è·¯å¾„ (Linly model path)
mode = 'api' # api éœ€è¦å…ˆè¿è¡ŒLinly-api-fast.py
mode = 'offline'
model_path = 'Linly-AI/Chinese-LLaMA-2-7B-hf'
# sslè¯ä¹¦ (SSL certificate) éº¦å…‹é£å¯¹è¯éœ€è¦æ­¤å‚æ•°
ssl_certfile = "/path/to/Linly-Talker/cert.pem"
ssl_keyfile = "/path/to/Linly-Talker/key.pem"
```

## ASR - Whisper

å€Ÿé‰´OpenAIçš„Whisper,å…·ä½“ä½¿ç”¨æ–¹æ³•å‚è€ƒ[https://github.com/openai/whisper](https://github.com/openai/whisper)

## TTS - Edge TTS

ä½¿ç”¨å¾®è½¯è¯­éŸ³æœåŠ¡,å…·ä½“ä½¿ç”¨æ–¹æ³•å‚è€ƒ[https://github.com/rany2/edge-tts](https://github.com/rany2/edge-tts)

## THG - SadTalker

è¯´è¯å¤´ç”Ÿæˆä½¿ç”¨SadTalkerï¼ˆCVPR 2023ï¼‰,è¯¦æƒ…è§[https://sadtalker.github.io](https://sadtalker.github.io)

ä¸‹è½½SadTalkeræ¨¡å‹:

```bash
bash scripts/download_models.sh  
```

## LLM - Conversation

### Linly-AI

Linlyæ¥è‡ªæ·±åœ³å¤§å­¦æ•°æ®å·¥ç¨‹å›½å®¶é‡ç‚¹å®éªŒå®¤,å‚è€ƒ[https://github.com/CVI-SZU/Linly](https://github.com/CVI-SZU/Linly)

ä¸‹è½½Linlyæ¨¡å‹:[https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf](https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf)

```bash
git lfs install
git clone https://huggingface.co/Linly-AI/Chinese-LLaMA-2-7B-hf
```

æˆ–ä½¿ç”¨API:

```bash
# å‘½ä»¤è¡Œ
curl -X POST -H "Content-Type: application/json" -d '{"question": "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹?"}' http://url:port  

# Python
import requests

url = "http://url:port"
headers = {
  "Content-Type": "application/json"
}

data = {
  "question": "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹?" 
}

response = requests.post(url, headers=headers, json=data)
# response_text = response.content.decode("utf-8")
answer, tag = response.json()
# print(answer)
if tag == 'success':
    response_text =  answer[0]
else:
    print("fail")
print(response_text)
```

APIéƒ¨ç½²æ¨è**FastAPI**ï¼Œç°åœ¨æ›´æ–°äº† FastAPI çš„APIä½¿ç”¨ç‰ˆæœ¬ï¼ŒFastAPI æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€æ˜“ç”¨ä¸”ç°ä»£çš„Python Web æ¡†æ¶ï¼Œå®ƒé€šè¿‡ä½¿ç”¨æœ€æ–°çš„Python ç‰¹æ€§å’Œå¼‚æ­¥ç¼–ç¨‹ï¼Œæä¾›äº†å¿«é€Ÿå¼€å‘Web API çš„èƒ½åŠ›ã€‚ è¯¥æ¡†æ¶ä¸ä»…æ˜“äºå­¦ä¹ å’Œä½¿ç”¨ï¼Œè¿˜å…·æœ‰è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ã€æ•°æ®éªŒè¯ç­‰å¼ºå¤§åŠŸèƒ½ã€‚ æ— è®ºæ˜¯æ„å»ºå°å‹é¡¹ç›®è¿˜æ˜¯å¤§å‹åº”ç”¨ç¨‹åºï¼ŒFastAPI éƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§è€Œæœ‰æ•ˆçš„å·¥å…·ã€‚

é¦–å…ˆå®‰è£…éƒ¨ç½²APIæ‰€ä½¿ç”¨çš„åº“
```bash
pip install fastapi==0.104.1
pip install uvicorn==0.24.0.post1
```

å…¶ä»–ä½¿ç”¨æ–¹æ³•å¤§è‡´ç›¸åŒï¼Œä¸»è¦æ˜¯ä¸åŒä»£ç å®ç°æ–¹å¼ï¼Œä¼šæ›´åŠ ç®€å•è¾¹ç•Œï¼Œå¹¶ä¸”å¤„ç†å¹¶å‘ä¹Ÿä¼šæ›´å¥½

```python
from fastapi import FastAPI, Request
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import uvicorn
import json
import datetime
import torch
from configs import model_path, api_port
# è®¾ç½®è®¾å¤‡å‚æ•°
DEVICE = "cuda"  # ä½¿ç”¨CUDA
DEVICE_ID = "0"  # CUDAè®¾å¤‡IDï¼Œå¦‚æœæœªè®¾ç½®åˆ™ä¸ºç©º
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE  # ç»„åˆCUDAè®¾å¤‡ä¿¡æ¯

# æ¸…ç†GPUå†…å­˜å‡½æ•°
def torch_gc():
    if torch.cuda.is_available():  # æ£€æŸ¥æ˜¯å¦å¯ç”¨CUDA
        with torch.cuda.device(CUDA_DEVICE):  # æŒ‡å®šCUDAè®¾å¤‡
            torch.cuda.empty_cache()  # æ¸…ç©ºCUDAç¼“å­˜
            torch.cuda.ipc_collect()  # æ”¶é›†CUDAå†…å­˜ç¢ç‰‡

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI()

# å¤„ç†POSTè¯·æ±‚çš„ç«¯ç‚¹
@app.post("/")
async def create_item(request: Request):
    global model, tokenizer  # å£°æ˜å…¨å±€å˜é‡ä»¥ä¾¿åœ¨å‡½æ•°å†…éƒ¨ä½¿ç”¨æ¨¡å‹å’Œåˆ†è¯å™¨
    json_post_raw = await request.json()  # è·å–POSTè¯·æ±‚çš„JSONæ•°æ®
    json_post = json.dumps(json_post_raw)  # å°†JSONæ•°æ®è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    json_post_list = json.loads(json_post)  # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPythonå¯¹è±¡
    prompt = json_post_list.get('prompt')  # è·å–è¯·æ±‚ä¸­çš„æç¤º
    history = json_post_list.get('history')  # è·å–è¯·æ±‚ä¸­çš„å†å²è®°å½•
    max_length = json_post_list.get('max_length')  # è·å–è¯·æ±‚ä¸­çš„æœ€å¤§é•¿åº¦
    top_p = json_post_list.get('top_p')  # è·å–è¯·æ±‚ä¸­çš„top_på‚æ•°
    temperature = json_post_list.get('temperature')  # è·å–è¯·æ±‚ä¸­çš„æ¸©åº¦å‚æ•°
    
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œå¯¹è¯ç”Ÿæˆ
    prompt = f"è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜ ### Instruction:{prompt}  ### Response:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda:0")
    generate_ids = model.generate(inputs.input_ids, 
                                  max_new_tokens=max_length if max_length else 2048,
                                  do_sample=True, 
                                  top_k=20,
                                  top_p=top_p,
                                  temperature=temperature if temperature else 0.84,
                                  repetition_penalty=1.15, eos_token_id=2, bos_token_id=1,pad_token_id=0)
    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    response = response.split("### Response:")[-1]
    now = datetime.datetime.now()  # è·å–å½“å‰æ—¶é—´
    time = now.strftime("%Y-%m-%d %H:%M:%S")  # æ ¼å¼åŒ–æ—¶é—´ä¸ºå­—ç¬¦ä¸²
    # æ„å»ºå“åº”JSON
    answer = {
        "response": response,
        # "history": history,
        "status": 200,
        "time": time
    }
    # æ„å»ºæ—¥å¿—ä¿¡æ¯
    log = "[" + time + "] " + '", prompt:"' + prompt + '", response:"' + repr(response) + '"'
    print(log)  # æ‰“å°æ—¥å¿—
    torch_gc()  # æ‰§è¡ŒGPUå†…å­˜æ¸…ç†
    return answer  # è¿”å›å“åº”

# ä¸»å‡½æ•°å…¥å£
if __name__ == '__main__':
    # åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨å’Œæ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(model_path, device_map="cuda:0",
                                                    torch_dtype=torch.bfloat16, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    # å¯åŠ¨FastAPIåº”ç”¨
    uvicorn.run(app, host='0.0.0.0', port=api_port, workers=1)  # åœ¨æŒ‡å®šç«¯å£å’Œä¸»æœºä¸Šå¯åŠ¨åº”ç”¨
```

é»˜è®¤éƒ¨ç½²åœ¨ 7871 ç«¯å£ï¼Œé€šè¿‡ POST æ–¹æ³•è¿›è¡Œè°ƒç”¨ï¼Œå¯ä»¥ä½¿ç”¨curlè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
curl -X POST "http://127.0.0.1:7871" \
     -H 'Content-Type: application/json' \
     -d '{"prompt": "å¦‚ä½•åº”å¯¹å‹åŠ›"}'
```

ä¹Ÿå¯ä»¥ä½¿ç”¨pythonä¸­çš„requestsåº“è¿›è¡Œè°ƒç”¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
import requests
import json

def get_completion(prompt):
    headers = {'Content-Type': 'application/json'}
    data = {"prompt": prompt}
    response = requests.post(url='http://127.0.0.1:7871', headers=headers, data=json.dumps(data))
    return response.json()['response']

if __name__ == '__main__':
    print(get_completion('ä½ å¥½å¦‚ä½•åº”å¯¹å‹åŠ›
```

å¾—åˆ°çš„è¿”å›å€¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```bash
{
  "response":"å¯»æ±‚æ”¯æŒå’Œæ”¾æ¾ï¼Œå¹¶é‡‡å–ç§¯æçš„æªæ–½è§£å†³é—®é¢˜ã€‚",
  "status":200,
  "time":"2024-01-12 01:43:37"
}
```



### Qwen

æ¥è‡ªé˜¿é‡Œäº‘çš„Qwenï¼ŒæŸ¥çœ‹ [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)

ä¸‹è½½ Qwen æ¨¡å‹: [https://huggingface.co/Qwen/Qwen-7B-Chat-Int4](https://huggingface.co/Qwen/Qwen-7B-Chat-Int4)

```bash
git lfs install
git clone https://huggingface.co/Qwen/Qwen-1_8B-Chat
```



### Gemini-Pro

æ¥è‡ª Google çš„ Gemini-Proï¼Œäº†è§£æ›´å¤šè¯·è®¿é—® [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/)

è¯·æ±‚ API å¯†é’¥: [https://makersuite.google.com/](https://makersuite.google.com/)



### æ¨¡å‹é€‰æ‹©

åœ¨ app.py æ–‡ä»¶ä¸­ï¼Œè½»æ¾é€‰æ‹©æ‚¨éœ€è¦çš„æ¨¡å‹ã€‚

```python
# å–æ¶ˆæ³¨é‡Šå¹¶è®¾ç½®æ‚¨é€‰æ‹©çš„æ¨¡å‹:

# llm = Gemini(model_path='gemini-pro', api_key=None, proxy_url=None) # ä¸è¦å¿˜è®°åŠ å…¥æ‚¨è‡ªå·±çš„ Google API å¯†é’¥
# llm = Qwen(mode='offline', model_path="Qwen/Qwen-1_8B-Chat")
# è‡ªåŠ¨ä¸‹è½½
# llm = Linly(mode='offline', model_path="Linly-AI/Chinese-LLaMA-2-7B-hf")
# æ‰‹åŠ¨ä¸‹è½½åˆ°æŒ‡å®šè·¯å¾„
llm = Linly(mode='offline', model_path="./Chinese-LLaMA-2-7B-hf")
```



## ä¼˜åŒ–

ä¸€äº›ä¼˜åŒ–:

- ä½¿ç”¨å›ºå®šçš„è¾“å…¥äººè„¸å›¾åƒ,æå‰æå–ç‰¹å¾,é¿å…æ¯æ¬¡è¯»å–
- ç§»é™¤ä¸å¿…è¦çš„åº“,ç¼©çŸ­æ€»æ—¶é—´
- åªä¿å­˜æœ€ç»ˆè§†é¢‘è¾“å‡º,ä¸ä¿å­˜ä¸­é—´ç»“æœ,æé«˜æ€§èƒ½
- ä½¿ç”¨OpenCVç”Ÿæˆæœ€ç»ˆè§†é¢‘,æ¯”mimwriteæ›´å¿«

## Gradio

Gradioæ˜¯ä¸€ä¸ªPythonåº“,æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹å¼å°†æœºå™¨å­¦ä¹ æ¨¡å‹ä½œä¸ºäº¤äº’å¼Webåº”ç”¨ç¨‹åºæ¥éƒ¨ç½²ã€‚

å¯¹Linly-Talkerè€Œè¨€,ä½¿ç”¨Gradioæœ‰ä¸¤ä¸ªä¸»è¦ç›®çš„:

1. **å¯è§†åŒ–ä¸æ¼”ç¤º**:Gradioä¸ºæ¨¡å‹æä¾›ä¸€ä¸ªç®€å•çš„Web GUI,ä¸Šä¼ å›¾ç‰‡å’Œæ–‡æœ¬åå¯ä»¥ç›´è§‚åœ°çœ‹åˆ°ç»“æœã€‚è¿™æ˜¯å±•ç¤ºç³»ç»Ÿèƒ½åŠ›çš„æœ‰æ•ˆæ–¹å¼ã€‚

2. **ç”¨æˆ·äº¤äº’**:Gradioçš„GUIå¯ä»¥ä½œä¸ºå‰ç«¯,å…è®¸ç”¨æˆ·ä¸Linly-Talkerè¿›è¡Œäº¤äº’å¯¹è¯ã€‚ç”¨æˆ·å¯ä»¥ä¸Šä¼ è‡ªå·±çš„å›¾ç‰‡å¹¶è¾“å…¥é—®é¢˜,å®æ—¶è·å–å›ç­”ã€‚è¿™æä¾›äº†æ›´è‡ªç„¶çš„è¯­éŸ³äº¤äº’æ–¹å¼ã€‚

å…·ä½“æ¥è¯´,æˆ‘ä»¬åœ¨app.pyä¸­åˆ›å»ºäº†ä¸€ä¸ªGradioçš„Interface,æ¥æ”¶å›¾ç‰‡å’Œæ–‡æœ¬è¾“å…¥,è°ƒç”¨å‡½æ•°ç”Ÿæˆå›åº”è§†é¢‘,åœ¨GUIä¸­æ˜¾ç¤ºå‡ºæ¥ã€‚è¿™æ ·å°±å®ç°äº†æµè§ˆå™¨äº¤äº’è€Œä¸éœ€è¦ç¼–å†™å¤æ‚çš„å‰ç«¯ã€‚

æ€»ä¹‹,Gradioä¸ºLinly-Talkeræä¾›äº†å¯è§†åŒ–å’Œç”¨æˆ·äº¤äº’çš„æ¥å£,æ˜¯å±•ç¤ºç³»ç»ŸåŠŸèƒ½å’Œè®©æœ€ç»ˆç”¨æˆ·ä½¿ç”¨ç³»ç»Ÿçš„æœ‰æ•ˆé€”å¾„ã€‚

## å¯åŠ¨

é¦–å…ˆè¯´æ˜ä¸€ä¸‹çš„æ–‡ä»¶å¤¹ç»“æ„å¦‚ä¸‹

```bash
Linly-Talker/ 
â”œâ”€â”€ app.py
â”œâ”€â”€ app_img.py
â”œâ”€â”€ utils.py
â”œâ”€â”€ Linly-api.py
â”œâ”€â”€ Linly-api-fast.py
â”œâ”€â”€ Linly-example.ipynb
â”œâ”€â”€ README.md
â”œâ”€â”€ README_zh.md
â”œâ”€â”€ request-Linly-api.py
â”œâ”€â”€ requirements_app.txt
â”œâ”€â”€ scripts
â”‚   â””â”€â”€ download_models.sh
â”œâ”€â”€	src
â”‚	â””â”€â”€ .....
â”œâ”€â”€ inputs
â”‚   â”œâ”€â”€ example.png
â”‚   â””â”€â”€ first_frame_dir
â”‚       â”œâ”€â”€ example_landmarks.txt
â”‚       â”œâ”€â”€ example.mat
â”‚       â””â”€â”€ example.png
â”œâ”€â”€ examples
â”‚   â”œâ”€â”€ driven_audio
â”‚   â”‚   â”œâ”€â”€ bus_chinese.wav
â”‚   â”‚   â”œâ”€â”€ ......
â”‚   â”‚   â””â”€â”€ RD_Radio40_000.wav
â”‚   â”œâ”€â”€ ref_video
â”‚   â”‚   â”œâ”€â”€ WDA_AlexandriaOcasioCortez_000.mp4
â”‚   â”‚   â””â”€â”€ WDA_KatieHill_000.mp4
â”‚   â””â”€â”€ source_image
â”‚       â”œâ”€â”€ art_0.png
â”‚       â”œâ”€â”€ ......
â”‚       â””â”€â”€ sad.png
â”œâ”€â”€ checkpoints // SadTalker æƒé‡è·¯å¾„
â”‚   â”œâ”€â”€ mapping_00109-model.pth.tar
â”‚   â”œâ”€â”€ mapping_00229-model.pth.tar
â”‚   â”œâ”€â”€ SadTalker_V0.0.2_256.safetensors
â”‚   â””â”€â”€ SadTalker_V0.0.2_512.safetensors
â”œâ”€â”€ gfpgan // GFPGAN æƒé‡è·¯å¾„
â”‚   â””â”€â”€ weights
â”‚       â”œâ”€â”€ alignment_WFLW_4HG.pth
â”‚       â””â”€â”€ detection_Resnet50_Final.pth
â”œâ”€â”€ Linly-AI
    â”œâ”€â”€ Chinese-LLaMA-2-7B-hf // Linly æƒé‡è·¯å¾„
        â”œâ”€â”€ config.json
        â”œâ”€â”€ generation_config.json
        â”œâ”€â”€ pytorch_model-00001-of-00002.bin
        â”œâ”€â”€ pytorch_model-00002-of-00002.bin
        â”œâ”€â”€ pytorch_model.bin.index.json
        â”œâ”€â”€ README.md
        â”œâ”€â”€ special_tokens_map.json
        â”œâ”€â”€ tokenizer_config.json
        â””â”€â”€ tokenizer.model
```

æ¥ä¸‹æ¥è¿›è¡Œå¯åŠ¨

```bash
python app.py
```

![](docs/UI.png)

å¯ä»¥ä»»æ„ä¸Šä¼ å›¾ç‰‡è¿›è¡Œå¯¹è¯

```bash
python app_img.py
```

![](docs/UI2.png)



## å‚è€ƒ

- [https://github.com/openai/whisper](https://github.com/openai/whisper)
- [https://github.com/rany2/edge-tts](https://github.com/rany2/edge-tts)  
- [https://github.com/CVI-SZU/Linly](https://github.com/CVI-SZU/Linly)
- [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)
- [https://deepmind.google/technologies/gemini/](https://deepmind.google/technologies/gemini/)
- [https://github.com/OpenTalker/SadTalker](https://github.com/OpenTalker/SadTalker)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=Kedreamix/Linly-Talker&type=Date)](https://star-history.com/#Kedreamix/Linly-Talker&Date)

