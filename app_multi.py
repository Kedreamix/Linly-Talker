import os
import random 
import time
import gradio as gr
from zhconv import convert
from LLM import LLM
from ASR import WhisperASR
from TFG import SadTalker 
from TTS import EdgeTTS

from src.cost_time import calculate_time
from configs import *
os.environ["GRADIO_TEMP_DIR"]= './temp'

description = """<p style="text-align: center; font-weight: bold;">
    <span style="font-size: 28px;">Linly æ™ºèƒ½å¤šè½®å¯¹è¯ç³»ç»Ÿ (Linly-Talker)</span>
    <br>
    <span style="font-size: 18px;" id="paper-info">
        [<a href="https://zhuanlan.zhihu.com/p/671006998" target="_blank">çŸ¥ä¹</a>]
        [<a href="https://www.bilibili.com/video/BV1rN4y1a76x/" target="_blank">bilibili</a>]
        [<a href="https://github.com/Kedreamix/Linly-Talker" target="_blank">GitHub</a>]
        [<a herf="https://kedreamix.github.io/" target="_blank">ä¸ªäººä¸»é¡µ</a>]
    </span>
    <br> 
    <span>Linly-Talker æ˜¯ä¸€æ¬¾æ™ºèƒ½ AI å¯¹è¯ç³»ç»Ÿï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸è§†è§‰æ¨¡å‹ï¼Œæ˜¯ä¸€ç§æ–°é¢–çš„äººå·¥æ™ºèƒ½äº¤äº’æ–¹å¼ã€‚</span>
</p>
"""
# è®¾ç½®é»˜è®¤system
default_system = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'

# è®¾å®šé»˜è®¤å‚æ•°å€¼ï¼Œå¯ä¿®æ”¹
source_image = r'example.png'
blink_every = True
size_of_image = 256
preprocess_type = 'crop'
facerender = 'facevid2vid'
enhancer = False
is_still_mode = False
# pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0)
pic_path = "./inputs/girl.png"
crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])

# exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1)
exp_weight = 1

use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
        gr.Warning(question)
    return question

@calculate_time
def LLM_response(question, voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0):
    answer = llm.generate(question)
    print(answer)
    try:
        tts.predict(answer, voice, rate, volume, pitch , 'answer.wav', 'answer.vtt')
    except:
        os.system(f'edge-tts --text "{answer}" --voice {voice} --write-media answer.wav')
    return 'answer.wav', 'answer.vtt', answer

@calculate_time
def Talker_response(text, voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, batch_size = 2):
    voice = 'zh-CN-XiaoxiaoNeural' if voice not in tts.SUPPORTED_VOICE else voice
    talker = SadTalker(lazy_load=True)
    driven_audio, driven_vtt, _ = LLM_response(text, voice, rate, volume, pitch)
    pose_style = random.randint(0, 45)
    video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history

def human_respone(history, voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0, batch_size = 2):
    response = history[-1][1]
    driven_audio, video_vtt = 'answer.wav', 'answer.vtt'
    voice = 'zh-CN-XiaoxiaoNeural' if voice not in tts.SUPPORTED_VOICE else voice
    tts.predict(response, voice, rate, volume, pitch, driven_audio, video_vtt)
    pose_style = random.randint(0, 45) # éšæœºé€‰æ‹©
    video_path = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)

    return video_path, video_vtt

def modify_system_session(system: str) -> str:
    if system is None or len(system) == 0:
        system = default_system
    llm.clear_history()
    return system, system, []

def clear_session():
    # clear history
    llm.clear_history()
    return '', []

def main():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(description)
        with gr.Row():   
            with gr.Column():
                with gr.Accordion("Advanced Settings(é«˜çº§è®¾ç½®) ",
                                        open=False):
                    voice = gr.Dropdown(tts.SUPPORTED_VOICE, 
                                        value='zh-CN-XiaoxiaoNeural', 
                                        label="Voice")
                    rate = gr.Slider(minimum=-100,
                                        maximum=100,
                                        value=0,
                                        step=1.0,
                                        label='Rate')
                    volume = gr.Slider(minimum=0,
                                            maximum=100,
                                            value=100,
                                            step=1,
                                            label='Volume')
                    pitch = gr.Slider(minimum=-100,
                                        maximum=100,
                                        value=0,
                                        step=1,
                                        label='Pitch')
                    batch_size = gr.Slider(minimum=1,
                                        maximum=10,
                                        value=1,
                                        step=1,
                                        label='Talker Batch size')
                video = gr.Video(label = 'æ•°å­—äººé—®ç­”', scale = 0.5)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯¹è¯åï¼‰", variant = 'primary')
            
            with gr.Column():
                with gr.Row():
                    with gr.Column(scale=3):
                        system_input = gr.Textbox(value=default_system, lines=1, label='System (è®¾å®šè§’è‰²)')
                    with gr.Column(scale=1):
                        modify_system = gr.Button("ğŸ› ï¸ è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯", scale=2)
                    system_state = gr.Textbox(value=default_system, visible=False)

                chatbot = gr.Chatbot(height=400, show_copy_button=True)
                audio = gr.Audio(sources=['microphone','upload'], type="filepath", label='è¯­éŸ³å¯¹è¯', autoplay=True)
                asr_text = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                msg = gr.Textbox(label="Prompt/é—®é¢˜")
                asr_text.click(fn=Asr,inputs=[audio],outputs=[msg])
                
                with gr.Row():
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    sumbit = gr.Button("ğŸš€ å‘é€", variant = 'primary')
                    
            # # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
            sumbit.click(chat_response, inputs=[system_input, msg, chatbot], 
                         outputs=[msg, chatbot])
            
            # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
            clear_history.click(fn = clear_session, outputs = [msg, chatbot])
            
            # è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯
            modify_system.click(fn=modify_system_session,
                        inputs=[system_input],
                        outputs=[system_state, system_input, chatbot])
            
            video_button.click(fn = human_respone, inputs = [chatbot, voice, rate, volume, pitch, batch_size], outputs = [video])
            
        with gr.Row(variant='panel'):
            with gr.Column():
                gr.Markdown("## Text Examples")
                examples =  ['åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ',
                    'å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ',
                    'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ',
                    'è¿‘æ—¥ï¼Œè‹¹æœå…¬å¸èµ·è¯‰é«˜é€šå…¬å¸ï¼ŒçŠ¶å‘Šå…¶æœªæŒ‰ç…§ç›¸å…³åˆçº¦è¿›è¡Œåˆä½œï¼Œé«˜é€šæ–¹é¢å°šæœªå›åº”ã€‚è¿™å¥è¯ä¸­â€œå…¶â€æŒ‡çš„æ˜¯è°ï¼Ÿ',
                    'ä¸‰å¹´çº§åŒå­¦ç§æ ‘80é¢—ï¼Œå››ã€äº”å¹´çº§ç§çš„æ£µæ ‘æ¯”ä¸‰å¹´çº§ç§çš„2å€å¤š14æ£µï¼Œä¸‰ä¸ªå¹´çº§å…±ç§æ ‘å¤šå°‘æ£µ?',
                    'æ’°å†™ä¸€ç¯‡äº¤å“ä¹éŸ³ä¹ä¼šè¯„è®ºï¼Œè®¨è®ºä¹å›¢çš„è¡¨æ¼”å’Œè§‚ä¼—çš„æ•´ä½“ä½“éªŒã€‚',
                    'ç¿»è¯‘æˆä¸­æ–‡ï¼šLuck is a dividend of sweat. The more you sweat, the luckier you get.',
                    ]
                gr.Examples(
                    examples = examples,
                    # fn = Talker_response,
                    inputs = [msg],
                    # outputs=[gen_video],
                    # cache_examples = True,
                )
    return inference


    
if __name__ == "__main__":
    # llm = LLM(mode='offline').init_model('Linly', 'Linly-AI/Chinese-LLaMA-2-7B-hf')
    # llm = LLM(mode='offline').init_model('Gemini', 'gemini-pro', api_key = "your api key")
    # llm = LLM(mode='offline').init_model('Qwen', 'Qwen/Qwen-1_8B-Chat')
    llm = LLM(mode=mode).init_model('Qwen', 'Qwen/Qwen-1_8B-Chat')
    talker = SadTalker(lazy_load=True)
    asr = WhisperASR('base')
    tts = EdgeTTS()
    gr.close_all()
    demo = main()
    demo.queue()
    # demo.launch()
    demo.launch(server_name=ip, # æœ¬åœ°ç«¯å£localhost:127.0.0.1 å…¨å±€ç«¯å£è½¬å‘:"0.0.0.0"
                server_port=port,
                # ä¼¼ä¹åœ¨Gradio4.0ä»¥ä¸Šç‰ˆæœ¬å¯ä»¥ä¸ä½¿ç”¨è¯ä¹¦ä¹Ÿå¯ä»¥è¿›è¡Œéº¦å…‹é£å¯¹è¯
                ssl_certfile=ssl_certfile,
                ssl_keyfile=ssl_keyfile,
                ssl_verify=False,                
                debug=True)