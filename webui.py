import os
import random 
import gradio as gr
import time
from zhconv import convert
from LLM import LLM
from ASR import WhisperASR
from TFG import SadTalker 
from TTS import EdgeTTS
from src.cost_time import calculate_time

from configs import *
os.environ["GRADIO_TEMP_DIR"]= './temp'

def get_title(title = 'Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker)'):
    description = f"""
    <p style="text-align: center; font-weight: bold;">
        <span style="font-size: 28px;">{title}</span>
        <br>
        <span style="font-size: 18px;" id="paper-info">
            [<a href="https://zhuanlan.zhihu.com/p/671006998" target="_blank">çŸ¥ä¹</a>]
            [<a href="https://www.bilibili.com/video/BV1rN4y1a76x/" target="_blank">bilibili</a>]
            [<a href="https://github.com/Kedreamix/Linly-Talker" target="_blank">GitHub</a>]
            [<a herf="https://kedreamix.github.io/" target="_blank">ä¸ªäººä¸»é¡µ</a>]
        </span>
        <br> 
        <span>Linly-Talker æ˜¯ä¸€æ¬¾æ™ºèƒ½ AI å¯¹è¯ç³»ç»Ÿï¼Œç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹ (LLMs) ä¸è§†è§‰æ¨¡å‹ï¼Œæ˜¯ä¸€ç§æ–°é¢–çš„äººå·¥æ™ºèƒ½äº¤äº’æ–¹å¼ã€‚</span>
    </p>
    """
    return description

# é»˜è®¤textçš„Example
examples =  [
    ['åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'zh-CN-XiaoxiaoNeural'],
    ['å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ','ç”·æ€§è§’è‰²', 'SadTalker', 'zh-CN-YunyangNeural'],
    ['ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ','å¥³æ€§è§’è‰²', 'SadTalker', 'zh-HK-HiuMaanNeural'],
    ['è¿‘æ—¥ï¼Œè‹¹æœå…¬å¸èµ·è¯‰é«˜é€šå…¬å¸ï¼ŒçŠ¶å‘Šå…¶æœªæŒ‰ç…§ç›¸å…³åˆçº¦è¿›è¡Œåˆä½œï¼Œé«˜é€šæ–¹é¢å°šæœªå›åº”ã€‚è¿™å¥è¯ä¸­â€œå…¶â€æŒ‡çš„æ˜¯è°ï¼Ÿ', 'ç”·æ€§è§’è‰²', 'SadTalker', 'zh-TW-YunJheNeural'],
    ['æ’°å†™ä¸€ç¯‡äº¤å“ä¹éŸ³ä¹ä¼šè¯„è®ºï¼Œè®¨è®ºä¹å›¢çš„è¡¨æ¼”å’Œè§‚ä¼—çš„æ•´ä½“ä½“éªŒã€‚', 'ç”·æ€§è§’è‰²', 'Wav2Lip', 'zh-CN-YunyangNeural'],
    ['ç¿»è¯‘æˆä¸­æ–‡ï¼šLuck is a dividend of sweat. The more you sweat, the luckier you get.', 'å¥³æ€§è§’è‰²', 'SadTalker', 'zh-CN-XiaoxiaoNeural'],
    ]

# è®¾ç½®é»˜è®¤system
default_system = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'

# è®¾å®šé»˜è®¤å‚æ•°å€¼ï¼Œå¯ä¿®æ”¹
blink_every = True
size_of_image = 256
preprocess_type = 'crop'
facerender = 'facevid2vid'
enhancer = False
is_still_mode = False
exp_weight = 1
use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
        gr.Warning(question)
    return question

@calculate_time
def LLM_response(question_audio, question, voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0):
    answer = llm.generate(question)
    print(answer)
    if voice in tts.SUPPORTED_VOICE:
        try:
            tts.predict(answer, voice, rate, volume, pitch , 'answer.wav', 'answer.vtt')
        except:
            os.system(f'edge-tts --text "{answer}" --voice {voice} --write-media answer.wav')
        return 'answer.wav', 'answer.vtt', answer
    elif voice == "å…‹éš†çƒŸå—“éŸ³":
        try:
            gpt_path = "../GPT-SoVITS/GPT_weights/yansang-e15.ckpt"
            sovits_path = "../GPT-SoVITS/SoVITS_weights/yansang_e16_s144.pth"
            vits.load_model(gpt_path, sovits_path)
            vits.predict(ref_wav_path = "examples/slicer_opt/vocal_output.wav_10.wav_0000846400_0000957760.wav",
                        prompt_text = "ä½ ä¸ºä»€ä¹ˆè¦ä¸€æ¬¡ä¸€æ¬¡çš„ä¼¤æˆ‘çš„å¿ƒå•Šï¼Ÿ",
                        prompt_language = "ä¸­æ–‡",
                        text = answer,
                        text_language = "ä¸­è‹±æ··åˆ",
                        how_to_cut = "æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡",
                        save_path = 'answer.wav')
            return 'answer.wav', None, answer
        except Exception as e:
            gr.Error("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
            return None, None, None
    elif voice == "å…‹éš†å£°éŸ³":
        try:
            if question_audio is None:
                gr.Error("æ— å£°éŸ³è¾“å…¥ï¼Œæ— æ³•å…‹éš†å£°éŸ³")
                # print("æ— å£°éŸ³è¾“å…¥ï¼Œæ— æ³•å…‹éš†å£°éŸ³")
                return None, None, None
            gpt_path = "GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
            sovits_path = "GPT_SoVITS/pretrained_models/s2G488k.pth"
            vits.load_model(gpt_path, sovits_path)
            vits.predict(ref_wav_path = question_audio, 
                        prompt_text = question,
                        prompt_language = "ä¸­æ–‡",
                        text = answer,
                        text_language = "ä¸­è‹±æ··åˆ",
                        how_to_cut = "å‡‘å››å¥ä¸€åˆ‡",
                        save_path = 'answer.wav')
            return 'answer.wav', None, answer
        except Exception as e:
            gr.Error("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
            return None, None, None

@calculate_time
def Talker_response(question_audio = None, method = 'SadTalker', text = '', voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, batch_size = 2, character = 'å¥³æ€§è§’è‰²'):
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image, pic_path = r'inputs/girl.png', r'inputs/girl.png'
        crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
        first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
        crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
        pic_path = "./inputs/boy.png"
        crop_pic_path = "./inputs/first_frame_dir_boy/boy.png"
        first_coeff_path = "./inputs/first_frame_dir_boy/boy.mat"
        crop_info = ((876, 747), (0, 0, 886, 838), [10.382158280494476, 0, 886, 747.7078990925525])
        default_voice = 'zh-CN-YunyangNeural'
    else:
        gr.Error('æœªçŸ¥è§’è‰²')
        return None
    voice = default_voice if voice not in tts.SUPPORTED_VOICE+["å…‹éš†çƒŸå—“éŸ³", "å…‹éš†å£°éŸ³"] else voice
    print(voice, character)
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, voice, rate, volume, pitch)
    pose_style = random.randint(0, 45)
    if method == 'SadTalker':
        video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)
    elif method == 'Wav2Lip':
        video = wav2lip.predict(crop_pic_path, driven_audio, batch_size)
    else:
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history

def human_respone(history, voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0, batch_size = 2, character = 'å¥³æ€§è§’è‰²'):
    response = history[-1][1]
    driven_audio, video_vtt = 'answer.wav', 'answer.vtt'
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image, pic_path = r'./inputs/girl.png', r"./inputs/girl.png"
        crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
        first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
        crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
        pic_path = "./inputs/boy.png"
        crop_pic_path = "./inputs/first_frame_dir_boy/boy.png"
        first_coeff_path = "./inputs/first_frame_dir_boy/boy.mat"
        crop_info = ((876, 747), (0, 0, 886, 838), [10.382158280494476, 0, 886, 747.7078990925525])
        default_voice = 'zh-CN-YunyangNeural'
    voice = default_voice if voice not in tts.SUPPORTED_VOICE else voice
    tts.predict(response, voice, rate, volume, pitch, driven_audio, video_vtt)
    pose_style = random.randint(0, 45) # éšæœºé€‰æ‹©
    video_path = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)

    return video_path, video_vtt

def modify_system_session(system: str) -> str:
    if system is None or len(system) == 0:
        system = default_system
    llm.clear_history()
    return system, system, []

def clear_session():
    # clear history
    llm.clear_history()
    return '', []

def voice_setting(suport_voice):
    with gr.Accordion("Advanced Settings(é«˜çº§è®¾ç½®è¯­éŸ³å‚æ•°) ", open=False):
        voice = gr.Dropdown(suport_voice,  
                            label="å£°éŸ³é€‰æ‹© Voice", 
                            value = "å…‹éš†å£°éŸ³" if 'å…‹éš†å£°éŸ³' in suport_voice else None)
        rate = gr.Slider(minimum=-100,
                            maximum=100,
                            value=0,
                            step=1.0,
                            label='å£°éŸ³é€Ÿç‡ Rate')
        volume = gr.Slider(minimum=0,
                                maximum=100,
                                value=100,
                                step=1,
                                label='å£°éŸ³éŸ³é‡ Volume')
        pitch = gr.Slider(minimum=-100,
                            maximum=100,
                            value=0,
                            step=1,
                            label='å£°éŸ³éŸ³è°ƒ Pitch')
        batch_size = gr.Slider(minimum=1,
                            maximum=10,
                            value=2,
                            step=1,
                            label='æ¨¡å‹å‚æ•° è°ƒèŠ‚å¯ä»¥åŠ å¿«ç”Ÿæˆé€Ÿåº¦ Talker Batch size')

    character = gr.Radio(['å¥³æ€§è§’è‰²', 'ç”·æ€§è§’è‰²'], label="è§’è‰²é€‰æ‹©", value='å¥³æ€§è§’è‰²')
    method = gr.Radio(choices = ['SadTalker', 'Wav2Lip', 'ER-NeRF(Comming Soon!!!)'], value = 'SadTalker', label = 'æ¨¡å‹é€‰æ‹©')
    return  voice, rate, volume, pitch, batch_size, character, method

@calculate_time
def Talker_response_img(question_audio, method, text, voice, rate, volume, pitch, source_image,
                    preprocess_type, 
                    is_still_mode,
                    enhancer,
                    batch_size,                            
                    size_of_image,
                    pose_style,
                    facerender,
                    exp_weight,
                    blink_every,
                    fps):
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, voice, rate, volume, pitch)
    if method == 'SadTalker':
        video = talker.test2(source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=fps)
    elif method == 'Wav2Lip':
        video = wav2lip.predict(source_image, driven_audio, batch_size)
    else:
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

def app():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) æ–‡æœ¬/è¯­éŸ³å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                with gr.Tabs(elem_id="question_audio"):
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Column(variant='panel'):
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            voice, rate, volume, pitch, batch_size, character, method = voice_setting(tts.SUPPORTED_VOICE)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                            asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                        
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.TabItem('æ•°å­—äººé—®ç­”'):
                        gen_video = gr.Video(label="ç”Ÿæˆè§†é¢‘", format="mp4", scale=1, autoplay=False)
                video_button = gr.Button("æäº¤è§†é¢‘ç”Ÿæˆ", variant='primary')
            video_button.click(fn=Talker_response,inputs=[question_audio, method, input_text,voice, rate, volume, pitch, batch_size, character],outputs=[gen_video])

        with gr.Row():
            with gr.Column(variant='panel'):
                gr.Markdown("## Test Examples")
                gr.Examples(
                    examples = examples,
                    fn = Talker_response,
                    inputs = [input_text, character, method, voice],
                )
    return inference

def app_multi():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) å¤šè½®GPTå¯¹è¯"))
        with gr.Row():
            with gr.Column():
                voice, rate, volume, pitch, batch_size, character, method = voice_setting(tts.SUPPORTED_VOICE)
                video = gr.Video(label = 'æ•°å­—äººé—®ç­”', scale = 0.5)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯¹è¯åï¼‰", variant = 'primary')
            
            with gr.Column():
                with gr.Row():
                    with gr.Column(scale=3):
                        system_input = gr.Textbox(value=default_system, lines=1, label='System (è®¾å®šè§’è‰²)')
                    with gr.Column(scale=1):
                        modify_system = gr.Button("ğŸ› ï¸ è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯", scale=2)
                    system_state = gr.Textbox(value=default_system, visible=False)

                chatbot = gr.Chatbot(height=400, show_copy_button=True)
                audio = gr.Audio(sources=['microphone','upload'], type="filepath", label='è¯­éŸ³å¯¹è¯', autoplay=False)
                asr_text = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                
                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                msg = gr.Textbox(label="Prompt/é—®é¢˜")
                asr_text.click(fn=Asr,inputs=[audio],outputs=[msg])
                
                with gr.Row():
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    sumbit = gr.Button("ğŸš€ å‘é€", variant = 'primary')
                    
            # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
            sumbit.click(chat_response, inputs=[system_input, msg, chatbot], 
                         outputs=[msg, chatbot])
            
            # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
            clear_history.click(fn = clear_session, outputs = [msg, chatbot])
            
            # è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯
            modify_system.click(fn=modify_system_session,
                        inputs=[system_input],
                        outputs=[system_state, system_input, chatbot])
            
            video_button.click(fn = human_respone, inputs = [chatbot, voice, rate, volume, pitch, batch_size, character], outputs = [video])
            
        with gr.Row(variant='panel'):
            with gr.Column(variant='panel'):
                gr.Markdown("## Test Examples")
                gr.Examples(
                    examples = examples,
                    fn = Talker_response,
                    inputs = [msg, character, method, voice],
                )
    return inference

def app_img():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) ä»»æ„å›¾ç‰‡å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'):
                with gr.Tabs(elem_id="sadtalker_source_image"):
                        with gr.TabItem('Source image'):
                            with gr.Row():
                                source_image = gr.Image(label="Source image", type="filepath", elem_id="img2img_image", width=512)
                
                with gr.Tabs(elem_id="question_audio"):
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Column(variant='panel'):
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3, info = 'æ–‡å­—å¯¹è¯')
                            with gr.Accordion("Advanced Settings",
                                        open=False,
                                        visible=True) as parameter_article:
                                voice = gr.Dropdown(tts.SUPPORTED_VOICE, 
                                                    value='zh-CN-XiaoxiaoNeural', 
                                                    label="Voice")
                                rate = gr.Slider(minimum=-100,
                                                    maximum=100,
                                                    value=0,
                                                    step=1.0,
                                                    label='Rate')
                                volume = gr.Slider(minimum=0,
                                                        maximum=100,
                                                        value=100,
                                                        step=1,
                                                        label='Volume')
                                pitch = gr.Slider(minimum=-100,
                                                    maximum=100,
                                                    value=0,
                                                    step=1,
                                                    label='Pitch')

                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                            asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                
                # with gr.Tabs(elem_id="response_audio"):
                #     with gr.TabItem("è¯­éŸ³é€‰æ‹©"):
                #         with gr.Column(variant='panel'):
                #             voice = gr.Dropdown(VOICES, values='zh-CN-XiaoxiaoNeural')
                            
                            
                with gr.Tabs(elem_id="text_examples"): 
                    gr.Markdown("## Text Examples")
                    examples =  [
                        ['åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ'],
                        ['å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ'],
                        ['ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ'],
                        ['è¿‘æ—¥ï¼Œè‹¹æœå…¬å¸èµ·è¯‰é«˜é€šå…¬å¸ï¼ŒçŠ¶å‘Šå…¶æœªæŒ‰ç…§ç›¸å…³åˆçº¦è¿›è¡Œåˆä½œï¼Œé«˜é€šæ–¹é¢å°šæœªå›åº”ã€‚è¿™å¥è¯ä¸­â€œå…¶â€æŒ‡çš„æ˜¯è°ï¼Ÿ'],
                        ['ä¸‰å¹´çº§åŒå­¦ç§æ ‘80é¢—ï¼Œå››ã€äº”å¹´çº§ç§çš„æ£µæ ‘æ¯”ä¸‰å¹´çº§ç§çš„2å€å¤š14æ£µï¼Œä¸‰ä¸ªå¹´çº§å…±ç§æ ‘å¤šå°‘æ£µ?'],
                        ['æ’°å†™ä¸€ç¯‡äº¤å“ä¹éŸ³ä¹ä¼šè¯„è®ºï¼Œè®¨è®ºä¹å›¢çš„è¡¨æ¼”å’Œè§‚ä¼—çš„æ•´ä½“ä½“éªŒã€‚'],
                        ['ç¿»è¯‘æˆä¸­æ–‡ï¼šLuck is a dividend of sweat. The more you sweat, the luckier you get.'],
                    ]
                    gr.Examples(
                        examples = examples,
                        inputs = [input_text],
                    )
                    
            # driven_audio = 'answer.wav'           
            with gr.Column(variant='panel'): 
                method = gr.Radio(choices = ['SadTalker', 'Wav2Lip', 'ER-NeRF(Comming Soon!!!)'], value = 'SadTalker', label = 'æ¨¡å‹é€‰æ‹©')
                with gr.Tabs(elem_id="sadtalker_checkbox"):
                    with gr.TabItem('Settings'):
                        with gr.Accordion("Advanced Settings",
                                        open=False):
                            gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                            with gr.Column(variant='panel'):
                                # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                                # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)

                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                                    preprocess_type = gr.Radio(['crop', 'resize','full', 'extcrop', 'extfull'], value='crop', label='preprocess', info="How to handle input image?")
                                
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid', 'PIRender'], value='facevid2vid', label='facerender', info="which face render?")
                                    
                                with gr.Row():
                                    batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")                                               

                with gr.Tabs(elem_id="sadtalker_genearted"):
                    gen_video = gr.Video(label="Generated video", format="mp4",scale=0.8)

                submit = gr.Button('Generate', elem_id="sadtalker_generate", variant='primary')
            submit.click(
                fn=Talker_response_img,
                inputs=[question_audio,
                        method, 
                        input_text,
                        voice, rate, volume, pitch,
                        source_image, 
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps], 
                outputs=[gen_video]
                )
            
        with gr.Row():
            examples = [
                [
                    'examples/source_image/full_body_2.png',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/full_body_1.png',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/full3.png',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/full4.jpeg',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/art_13.png',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/art_5.png',
                    'crop',
                    False,
                    False
                ],
            ]
            gr.Examples(examples=examples,
                        fn=Talker_response,
                        inputs=[
                            source_image,
                            preprocess_type,
                            is_still_mode,
                            enhancer], 
                        outputs=[gen_video],
                        # cache_examples=True,
                        )
    return inference

def app_vits():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) è¯­éŸ³å…‹éš†"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                with gr.Tabs(elem_id="question_audio"):
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Column(variant='panel'):
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            voice, rate, volume, pitch, batch_size, character, method = voice_setting(["å…‹éš†å£°éŸ³", "å…‹éš†çƒŸå—“éŸ³"] + tts.SUPPORTED_VOICE)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                            asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.TabItem('æ•°å­—äººé—®ç­”'):
                        gen_video = gr.Video(label="Generated video", format="mp4", scale=1, autoplay=False)
                video_button = gr.Button("æäº¤", variant='primary')
            video_button.click(fn=Talker_response,inputs=[question_audio, method, input_text, voice, rate, volume, pitch, batch_size, character],outputs=[gen_video])

        with gr.Row():
            with gr.Column(variant='panel'):
                gr.Markdown("## Test Examples")
                gr.Examples(
                    examples = [["å¦‚ä½•åº”å¯¹å‹åŠ›", "ç”·æ€§è§’è‰²", "SadTalker", "å…‹éš†çƒŸå—“éŸ³"], ["åŒ—äº¬æœ‰ä»€ä¹ˆå¥½ç©çš„åœ°æ–¹", "ç”·æ€§è§’è‰²", "SadTalker", "å…‹éš†å£°éŸ³"]] + examples,
                    fn = Talker_response,
                    inputs = [input_text, character, method, voice],
                )
    return inference
    
if __name__ == "__main__":
    # llm = LLM(mode='offline').init_model('Linly', 'Linly-AI/Chinese-LLaMA-2-7B-hf')
    # llm = LLM(mode='offline').init_model('Gemini', 'gemini-pro', api_key = "your api key")
    # llm = LLM(mode='offline').init_model('Qwen', 'Qwen/Qwen-1_8B-Chat')
    llm = LLM(mode='offline').init_model('Qwen', 'Qwen/Qwen-1_8B-Chat')
    try:
        talker = SadTalker(lazy_load=True)
    except Exception as e:
        print("SadTalker Error: ", e)
        # print("å¦‚æœä½¿ç”¨SadTalkerï¼Œè¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")
        gr.Warning("å¦‚æœä½¿ç”¨SadTalkerï¼Œè¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")
    try:
        from TFG import Wav2Lip
        wav2lip = Wav2Lip("checkpoints/wav2lip_gan.pth")
    except Exception as e:
        print("Wav2Lip Error: ", e)
        print("å¦‚æœä½¿ç”¨Wav2Lipï¼Œè¯·å…ˆä¸‹è½½Wav2Lipæ¨¡å‹")
    try:
        from VITS import GPT_SoVITS
        vits = GPT_SoVITS()
    except Exception as e:
        print("GPT-SoVITS Error: ", e)
        print("å¦‚æœä½¿ç”¨VITSï¼Œè¯·å…ˆä¸‹è½½GPT-SoVITSæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    try:
        from ASR import FunASR
        asr = FunASR()
    except Exception as e:
        print("ASR Error: ", e)
        print("å¦‚æœä½¿ç”¨FunASRï¼Œè¯·å…ˆä¸‹è½½FunASRæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
        asr = WhisperASR('base')
    tts = EdgeTTS()
    gr.close_all()
    demo_app = app()
    demo_img = app_img()
    demo_multi = app_multi()
    demo_vits = app_vits()
    demo = gr.TabbedInterface(interface_list = [demo_app, demo_img, demo_multi, demo_vits], 
                              tab_names = ["æ–‡æœ¬/è¯­éŸ³å¯¹è¯", "ä»»æ„å›¾ç‰‡å¯¹è¯", "å¤šè½®GPTå¯¹è¯", "è¯­éŸ³å…‹éš†æ•°å­—äººå¯¹è¯"],
                              title = "Linly-Talker WebUI")
    demo.launch(server_name="127.0.0.1", # æœ¬åœ°ç«¯å£localhost:127.0.0.1 å…¨å±€ç«¯å£è½¬å‘:"0.0.0.0"
                server_port=port,
                # ä¼¼ä¹åœ¨Gradio4.0ä»¥ä¸Šç‰ˆæœ¬å¯ä»¥ä¸ä½¿ç”¨è¯ä¹¦ä¹Ÿå¯ä»¥è¿›è¡Œéº¦å…‹é£å¯¹è¯
                ssl_certfile=ssl_certfile,
                ssl_keyfile=ssl_keyfile,
                ssl_verify=False,
                debug=True,
                )