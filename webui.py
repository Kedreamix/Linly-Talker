import os
import random 
import gradio as gr
import time
from zhconv import convert
from LLM import LLM
from TFG import SadTalker
from TTS import EdgeTTS
from src.cost_time import calculate_time

from configs import *
os.environ["GRADIO_TEMP_DIR"]= './temp'
os.environ["WEBUI"] = "true"
def get_title(title = 'Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker)'):
    description = f"""
    <p style="text-align: center; font-weight: bold;">
        <span style="font-size: 28px;">{title}</span>
        <br>
        <span style="font-size: 18px;" id="paper-info">
            [<a href="https://zhuanlan.zhihu.com/p/671006998" target="_blank">çŸ¥ä¹</a>]
            [<a href="https://www.bilibili.com/video/BV1rN4y1a76x/" target="_blank">bilibili</a>]
            [<a href="https://github.com/Kedreamix/Linly-Talker" target="_blank">GitHub</a>]
            [<a herf="https://kedreamix.github.io/" target="_blank">ä¸ªäººä¸»é¡µ</a>]
        </span>
        <br> 
        <span>Linly-Talkeræ˜¯ä¸€æ¬¾åˆ›æ–°çš„æ•°å­—äººå¯¹è¯ç³»ç»Ÿï¼Œå®ƒèåˆäº†æœ€æ–°çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ğŸ¤–ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ğŸ™ï¸ã€æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆTTSï¼‰ğŸ—£ï¸å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯ğŸ¤ã€‚</span>
    </p>
    """
    return description


# è®¾ç½®é»˜è®¤system
default_system = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'
# è®¾ç½®é»˜è®¤çš„prompt
prefix_prompt = 'è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'

edgetts = EdgeTTS()

# è®¾å®šé»˜è®¤å‚æ•°å€¼ï¼Œå¯ä¿®æ”¹
blink_every = True
size_of_image = 256
preprocess_type = 'crop'
facerender = 'facevid2vid'
enhancer = False
is_still_mode = False
exp_weight = 1
use_ref_video = False
ref_video = None
ref_info = 'pose'
use_idle_mode = False
length_of_audio = 5

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        print("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
        gr.Warning(question)
    return question

@calculate_time
def TTS_response(text, 
                 voice, rate, volume, pitch,
                 am, voc, lang, male,
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, 
                 question_audio, question, use_mic_voice,
                 tts_method = 'PaddleTTS', save_path = 'answer.wav'):
    # print(text, voice, rate, volume, pitch, am, voc, lang, male, tts_method, save_path)
    if tts_method == 'Edge-TTS':
        if not edgetts.network:
            gr.Warning("è¯·æ£€æŸ¥ç½‘ç»œæˆ–è€…ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚PaddleTTS") 
            return None, None
        try:
            edgetts.predict(text, voice, rate, volume, pitch , 'answer.wav', 'answer.vtt')
        except:
            os.system(f'edge-tts --text "{text}" --voice {voice} --write-media answer.wav --write-subtitles answer.vtt')
        return 'answer.wav', 'answer.vtt'
    elif tts_method == 'PaddleTTS':
        tts.predict(text, am, voc, lang = lang, male=male, save_path = save_path)
        return save_path, None
    elif tts_method == 'GPT-SoVITSå…‹éš†å£°éŸ³':
        if use_mic_voice:
            try:
                vits.predict(ref_wav_path = question_audio,
                                prompt_text = question,
                                prompt_language = "ä¸­æ–‡",
                                text = text, # å›ç­”
                                text_language = "ä¸­æ–‡",
                                how_to_cut = "å‡‘å››å¥ä¸€åˆ‡",
                                save_path = 'answer.wav')
                return 'answer.wav', None
            except Exception as e:
                gr.Warning("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
                return None, None
        else:
            try:
                vits.predict(ref_wav_path = inp_ref,
                                prompt_text = prompt_text,
                                prompt_language = prompt_language,
                                text = text, # å›ç­”
                                text_language = text_language,
                                how_to_cut = how_to_cut,
                                save_path = 'answer.wav')
                return 'answer.wav', None
            except Exception as e:
                gr.Warning("æ— å…‹éš†ç¯å¢ƒæˆ–è€…æ— å…‹éš†æ¨¡å‹æƒé‡ï¼Œæ— æ³•å…‹éš†å£°éŸ³", e)
                return None, None
    return None, None
@calculate_time
def LLM_response(question_audio, question, 
                 voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0,
                 am='fastspeech2', voc='pwgan',lang='zh', male=False, 
                 inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                 tts_method = 'Edge-TTS'):
    answer = llm.generate(question, default_system)
    print(answer)
    driven_audio, driven_vtt = TTS_response(answer, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, question, use_mic_voice,
                 tts_method)
    return driven_audio, driven_vtt, answer

@calculate_time
def Talker_response(question_audio = None, method = 'SadTalker', text = '',
                    voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, 
                    am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                    inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                    tts_method = 'Edge-TTS',batch_size = 2, character = 'å¥³æ€§è§’è‰²', 
                    progress=gr.Progress(track_tqdm=True)):
    default_voice = None
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image, pic_path = r'inputs/girl.png', r'inputs/girl.png'
        crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
        first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
        crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
        pic_path = "./inputs/boy.png"
        crop_pic_path = "./inputs/first_frame_dir_boy/boy.png"
        first_coeff_path = "./inputs/first_frame_dir_boy/boy.mat"
        crop_info = ((876, 747), (0, 0, 886, 838), [10.382158280494476, 0, 886, 747.7078990925525])
        default_voice = 'zh-CN-YunyangNeural'
    else:
        gr.Warning('æœªçŸ¥è§’è‰²')
        return None
    
    voice = default_voice if not voice else voice
    
    if not voice:
        gr.Warning('è¯·é€‰æ‹©å£°éŸ³')
    
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, 
                                               voice, rate, volume, pitch, 
                                               am, voc, lang, male, 
                                               inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                               tts_method)
    
    if method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)
    elif method == 'Wav2Lip':
        video = talker.predict(crop_pic_path, driven_audio, batch_size, enhancer)
    elif method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + method)
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

@calculate_time
def Talker_response_img(question_audio, method, text, voice, rate, volume, pitch, 
                        am, voc, lang, male, 
                        inp_ref , prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                        tts_method,
                        source_image,
                        preprocess_type, 
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps, progress=gr.Progress(track_tqdm=True)
                    ):
    if enhancer:
        gr.Warning("è®°å¾—è¯·å…ˆå®‰è£…GFPGANåº“ï¼Œpip install gfpgan, å·²å®‰è£…å¯å¿½ç•¥")
    if not voice:
        gr.Warning("è¯·å…ˆé€‰æ‹©å£°éŸ³")
    driven_audio, driven_vtt, _ = LLM_response(question_audio, text, voice, rate, volume, pitch, 
                                               am, voc, lang, male, 
                                               inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                               tts_method = tts_method)
    if method == 'SadTalker':
        video = talker.test2(source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=fps)
    elif method == 'Wav2Lip':
        video = talker.predict(source_image, driven_audio, batch_size)
    elif method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video

@calculate_time
def Talker_Say(preprocess_type, 
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps,source_image = None, source_video = None, question_audio = None, method = 'SadTalker', text = '', 
                    voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 100, pitch = 0, 
                    am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                    inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                    tts_method = 'Edge-TTS', character = 'å¥³æ€§è§’è‰²',
                    progress=gr.Progress(track_tqdm=True)):
    if source_video:
        source_image = source_video
    default_voice = None
    
    voice = default_voice if not voice else voice
    
    if not voice:
        gr.Warning('è¯·é€‰æ‹©å£°éŸ³')
    
    driven_audio, driven_vtt = TTS_response(text, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, text, use_mic_voice,
                 tts_method)
    
    if method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test2(source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=fps)
    elif method == 'Wav2Lip':
        video = talker.predict(source_image, driven_audio, batch_size, enhancer)
    elif method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + method)
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video


def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history

def modify_system_session(system: str) -> str:
    if system is None or len(system) == 0:
        system = default_system
    llm.clear_history()
    return system, system, []

def clear_session():
    # clear history
    llm.clear_history()
    return '', []


def human_response(history, question_audio, talker_method, 
                   voice = 'zh-CN-XiaoxiaoNeural', rate = 0, volume = 0, pitch = 0, batch_size = 2, 
                  am = 'fastspeech2', voc = 'pwgan', lang = 'zh', male = False, 
                  inp_ref = None, prompt_text = "", prompt_language = "", text_language = "", how_to_cut = "", use_mic_voice = False,
                  tts_method = 'Edge-TTS', character = 'å¥³æ€§è§’è‰²', progress=gr.Progress(track_tqdm=True)):
    response = history[-1][1]
    qusetion = history[-1][0]
    # driven_audio, video_vtt = 'answer.wav', 'answer.vtt'
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image, pic_path = r'./inputs/girl.png', r"./inputs/girl.png"
        crop_pic_path = "./inputs/first_frame_dir_girl/girl.png"
        first_coeff_path = "./inputs/first_frame_dir_girl/girl.mat"
        crop_info = ((403, 403), (19, 30, 502, 513), [40.05956541381802, 40.17324339233366, 443.7892505041507, 443.9029284826663])
        default_voice = 'zh-CN-XiaoxiaoNeural'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
        pic_path = "./inputs/boy.png"
        crop_pic_path = "./inputs/first_frame_dir_boy/boy.png"
        first_coeff_path = "./inputs/first_frame_dir_boy/boy.mat"
        crop_info = ((876, 747), (0, 0, 886, 838), [10.382158280494476, 0, 886, 747.7078990925525])
        default_voice = 'zh-CN-YunyangNeural'
    voice = default_voice if not voice else voice
    # tts.predict(response, voice, rate, volume, pitch, driven_audio, video_vtt)
    driven_audio, driven_vtt = TTS_response(response, voice, rate, volume, pitch, 
                 am, voc, lang, male, 
                 inp_ref, prompt_text, prompt_language, text_language, how_to_cut, question_audio, qusetion, use_mic_voice,
                 tts_method)
    
    if talker_method == 'SadTalker':
        pose_style = random.randint(0, 45)
        video = talker.test(pic_path,
                        crop_pic_path,
                        first_coeff_path,
                        crop_info,
                        source_image,
                        driven_audio,
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        use_ref_video,
                        ref_video,
                        ref_info,
                        use_idle_mode,
                        length_of_audio,
                        blink_every,
                        fps=20)
    elif talker_method == 'Wav2Lip':
        video = talker.predict(crop_pic_path, driven_audio, batch_size, enhancer)
    elif talker_method == 'ER-NeRF':
        video = talker.predict(driven_audio)
    else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + talker_method)
        return None
    if driven_vtt:
        return video, driven_vtt
    else:
        return video
    
GPT_SoVITS_ckpt = "GPT_SoVITS/pretrained_models"
def load_vits_model(gpt_path, sovits_path, progress=gr.Progress(track_tqdm=True)):
    global vits
    print("æ¨¡å‹åŠ è½½ä¸­...", gpt_path, sovits_path)
    all_gpt_path, all_sovits_path = os.path.join(GPT_SoVITS_ckpt, gpt_path), os.path.join(GPT_SoVITS_ckpt, sovits_path)
    vits.load_model(all_gpt_path, all_sovits_path)
    gr.Info("æ¨¡å‹åŠ è½½æˆåŠŸ")
    return gpt_path, sovits_path

def list_models(dir, endwith = ".pth"):
    list_folder = os.listdir(dir)
    list_folder = [i for i in list_folder if i.endswith(endwith)]
    return list_folder

def character_change(character):
    if character == 'å¥³æ€§è§’è‰²':
        # å¥³æ€§è§’è‰²
        source_image = r'./inputs/girl.png'
    elif character == 'ç”·æ€§è§’è‰²':
        # ç”·æ€§è§’è‰²
        source_image = r'./inputs/boy.png'
    elif character == 'è‡ªå®šä¹‰è§’è‰²':
        # gr.Warnings("è‡ªå®šä¹‰è§’è‰²æš‚æœªæ›´æ–°ï¼Œè¯·ç»§ç»­å…³æ³¨åç»­ï¼Œå¯é€šè¿‡è‡ªç”±ä¸Šä¼ å›¾ç‰‡æ¨¡å¼è¿›è¡Œè‡ªå®šä¹‰è§’è‰²")
        source_image = None
    return source_image

def webui_setting(talk = True):
    if not talk:
        with gr.Tabs():
            with gr.TabItem('æ•°å­—äººå½¢è±¡è®¾å®š'):
                source_image = gr.Image(label="Source image", type="filepath")
    else:
        source_image = None
    with gr.Tabs("TTS Method"):
        with gr.Accordion("TTS Methodè¯­éŸ³æ–¹æ³•è°ƒèŠ‚ ", open=False):
            with gr.Tab("Edge-TTS"):
                voice = gr.Dropdown(edgetts.SUPPORTED_VOICE, 
                                    value='zh-CN-XiaoxiaoNeural', 
                                    label="Voice")
                rate = gr.Slider(minimum=-100,
                                    maximum=100,
                                    value=0,
                                    step=1.0,
                                    label='Rate')
                volume = gr.Slider(minimum=0,
                                        maximum=100,
                                        value=100,
                                        step=1,
                                        label='Volume')
                pitch = gr.Slider(minimum=-100,
                                    maximum=100,
                                    value=0,
                                    step=1,
                                    label='Pitch')
            with gr.Tab("PaddleTTS"):
                am = gr.Dropdown(["FastSpeech2"], label="å£°å­¦æ¨¡å‹é€‰æ‹©", value = 'FastSpeech2')
                voc = gr.Dropdown(["PWGan", "HifiGan"], label="å£°ç å™¨é€‰æ‹©", value = 'PWGan')
                lang = gr.Dropdown(["zh", "en", "mix", "canton"], label="è¯­è¨€é€‰æ‹©", value = 'zh')
                male = gr.Checkbox(label="ç”·å£°(Male)", value=False)
            with gr.Tab('GPT-SoVITS'):
                with gr.Row():
                    gpt_path = gr.FileExplorer(root = GPT_SoVITS_ckpt, glob = "*.ckpt", value = "s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt", file_count='single', label="GPTæ¨¡å‹è·¯å¾„")
                    sovits_path = gr.FileExplorer(root = GPT_SoVITS_ckpt, glob = "*.pth", value = "s2G488k.pth", file_count='single', label="SoVITSæ¨¡å‹è·¯å¾„")
                    # gpt_path = gr.Dropdown(choices=list_models(GPT_SoVITS_ckpt, 'ckpt'))
                    # sovits_path = gr.Dropdown(choices=list_models(GPT_SoVITS_ckpt, 'pth'))
                    # gpt_path = gr.Textbox(label="GPTæ¨¡å‹è·¯å¾„", 
                    #                       value="GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt")
                    # sovits_path = gr.Textbox(label="SoVITSæ¨¡å‹è·¯å¾„", 
                    #                          value="GPT_SoVITS/pretrained_models/s2G488k.pth")
                button = gr.Button("åŠ è½½æ¨¡å‹")
                button.click(fn = load_vits_model, 
                             inputs=[gpt_path, sovits_path], 
                             outputs=[gpt_path, sovits_path])
                
                with gr.Row():
                    inp_ref = gr.Audio(label="è¯·ä¸Šä¼ 3~10ç§’å†…å‚è€ƒéŸ³é¢‘ï¼Œè¶…è¿‡ä¼šæŠ¥é”™ï¼", sources=["microphone", "upload"], type="filepath")
                    use_mic_voice = gr.Checkbox(label="ä½¿ç”¨è¯­éŸ³é—®ç­”çš„éº¦å…‹é£")
                    prompt_text = gr.Textbox(label="å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬", value="")
                    prompt_language = gr.Dropdown(
                        label="å‚è€ƒéŸ³é¢‘çš„è¯­ç§", choices=["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡"], value="ä¸­æ–‡"
                    )
                asr_button = gr.Button("è¯­éŸ³è¯†åˆ« - å…‹éš†å‚è€ƒéŸ³é¢‘")
                asr_button.click(fn=Asr,inputs=[inp_ref],outputs=[prompt_text])
                with gr.Row():
                    text_language = gr.Dropdown(
                        label="éœ€è¦åˆæˆçš„è¯­ç§", choices=["ä¸­æ–‡", "è‹±æ–‡", "æ—¥æ–‡", "ä¸­è‹±æ··åˆ", "æ—¥è‹±æ··åˆ", "å¤šè¯­ç§æ··åˆ"], value="ä¸­æ–‡"
                    )
                    
                    how_to_cut = gr.Dropdown(
                        label="æ€ä¹ˆåˆ‡",
                        choices=["ä¸åˆ‡", "å‡‘å››å¥ä¸€åˆ‡", "å‡‘50å­—ä¸€åˆ‡", "æŒ‰ä¸­æ–‡å¥å·ã€‚åˆ‡", "æŒ‰è‹±æ–‡å¥å·.åˆ‡", "æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡" ],
                        value="å‡‘å››å¥ä¸€åˆ‡",
                        interactive=True,
                    )
            
            with gr.Column(variant='panel'): 
                batch_size = gr.Slider(minimum=1,
                                    maximum=10,
                                    value=2,
                                    step=1,
                                    label='Talker Batch size')

    character = gr.Radio(['å¥³æ€§è§’è‰²', 
                          'ç”·æ€§è§’è‰²', 
                          'è‡ªå®šä¹‰è§’è‰²'], 
                         label="è§’è‰²é€‰æ‹©", value='è‡ªå®šä¹‰è§’è‰²')
    # character.change(fn = character_change, inputs=[character], outputs = [source_image])
    tts_method = gr.Radio(['Edge-TTS', 'PaddleTTS', 'GPT-SoVITSå…‹éš†å£°éŸ³', 'Comming Soon!!!'], label="Text To Speech Method", 
                                              value = 'Edge-TTS')
    tts_method.change(fn = tts_model_change, inputs=[tts_method], outputs = [tts_method])
    asr_method = gr.Radio(choices = ['Whisper-tiny', 'Whisper-base', 'FunASR', 'Comming Soon!!!'], value='Whisper-base', label = 'è¯­éŸ³è¯†åˆ«æ¨¡å‹é€‰æ‹©')
    asr_method.change(fn = asr_model_change, inputs=[asr_method], outputs = [asr_method])
    talker_method = gr.Radio(choices = ['SadTalker', 'Wav2Lip', 'ER-NeRF', 'MuseTalk', 'Comming Soon!!!'], 
                      value = 'SadTalker', label = 'æ•°å­—äººæ¨¡å‹é€‰æ‹©')
    talker_method.change(fn = talker_model_change, inputs=[talker_method], outputs = [talker_method])
    llm_method = gr.Dropdown(choices = ['Qwen', 'Qwen2', 'Linly', 'Gemini', 'ChatGLM', 'ChatGPT', 'GPT4Free', 'Comming Soon!!!'], value = 'Qwen', label = 'LLM æ¨¡å‹é€‰æ‹©')
    llm_method.change(fn = llm_model_change, inputs=[llm_method], outputs = [llm_method])
    return  (source_image, voice, rate, volume, pitch, 
             am, voc, lang, male, 
             inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
             tts_method, batch_size, character, talker_method, asr_method, llm_method)


def exmaple_setting(asr, text, character, talk , tts, voice, llm):
    # é»˜è®¤textçš„Example
    examples =  [
        ['Whisper-base', 'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ', 'å¥³æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-XiaoxiaoNeural', 'Qwen'],
        ['FunASR', 'å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ','ç”·æ€§è§’è‰²', 'SadTalker', 'Edge-TTS', 'zh-CN-YunyangNeural', 'Qwen'],
        ['Whisper-tiny', 'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ','å¥³æ€§è§’è‰²', 'Wav2Lip', 'PaddleTTS', 'None', 'Qwen'],
        ]

    with gr.Row(variant='panel'):
        with gr.Column(variant='panel'):
            gr.Markdown("## Test Examples")
            gr.Examples(
                examples = examples,
                inputs = [asr, text, character, talk , tts, voice, llm],
            )
def app():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) æ–‡æœ¬/è¯­éŸ³å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                (source_image, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
             
            
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                # with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                #     with gr.Accordion("Advanced Settings",
                #                     open=False):
                #         gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                #         with gr.Column(variant='panel'):
                #             # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                #             # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                #             with gr.Row():
                #                 pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                #                 exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                #                 blink_every = gr.Checkbox(label="use eye blink", value=True)

                #             with gr.Row():
                #                 size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                #                 preprocess_type = gr.Radio(['crop', 'resize','full'], value='full', label='preprocess', info="How to handle input image?")
                            
                #             with gr.Row():
                #                 is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                #                 facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                
                #             with gr.Row():
                #                 # batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                #                 fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                #                 enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")       
                with gr.Tabs():
                    with gr.TabItem('æ•°å­—äººé—®ç­”'):
                        gen_video = gr.Video(label="ç”Ÿæˆè§†é¢‘", format="mp4", autoplay=False)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘", variant='primary')
            video_button.click(fn=Talker_response,inputs=[question_audio, talker_method, input_text, voice, rate, volume, pitch,
                                                          am, voc, lang, male, 
                                                          inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                                                          tts_method, batch_size, character],outputs=[gen_video])
        exmaple_setting(asr_method, input_text, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_multi():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) å¤šè½®GPTå¯¹è¯"))
        with gr.Row():
            with gr.Column():
                (source_image, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
                video = gr.Video(label = 'æ•°å­—äººé—®ç­”', scale = 0.5)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘ï¼ˆå¯¹è¯åï¼‰", variant = 'primary')
            
            with gr.Column():
                with gr.Row():
                    with gr.Column(scale=3):
                        system_input = gr.Textbox(value=default_system, lines=1, label='System (è®¾å®šè§’è‰²)')
                    with gr.Column(scale=1):
                        modify_system = gr.Button("ğŸ› ï¸ è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯", scale=2)
                    system_state = gr.Textbox(value=default_system, visible=False)

                chatbot = gr.Chatbot(height=400, show_copy_button=True)
                with gr.Group():
                    question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label='è¯­éŸ³å¯¹è¯', autoplay=False)
                    asr_text = gr.Button('ğŸ¤ è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                
                # åˆ›å»ºä¸€ä¸ªæ–‡æœ¬æ¡†ç»„ä»¶ï¼Œç”¨äºè¾“å…¥ promptã€‚
                msg = gr.Textbox(label="Prompt/é—®é¢˜")
                asr_text.click(fn=Asr,inputs=[question_audio],outputs=[msg])
                
                with gr.Row():
                    clear_history = gr.Button("ğŸ§¹ æ¸…é™¤å†å²å¯¹è¯")
                    sumbit = gr.Button("ğŸš€ å‘é€", variant = 'primary')
                    
            # è®¾ç½®æŒ‰é’®çš„ç‚¹å‡»äº‹ä»¶ã€‚å½“ç‚¹å‡»æ—¶ï¼Œè°ƒç”¨ä¸Šé¢å®šä¹‰çš„ å‡½æ•°ï¼Œå¹¶ä¼ å…¥ç”¨æˆ·çš„æ¶ˆæ¯å’ŒèŠå¤©å†å²è®°å½•ï¼Œç„¶åæ›´æ–°æ–‡æœ¬æ¡†å’ŒèŠå¤©æœºå™¨äººç»„ä»¶ã€‚
            sumbit.click(chat_response, inputs=[system_input, msg, chatbot], 
                         outputs=[msg, chatbot])
            
            # ç‚¹å‡»åæ¸…ç©ºåç«¯å­˜å‚¨çš„èŠå¤©è®°å½•
            clear_history.click(fn = clear_session, outputs = [msg, chatbot])
            
            # è®¾ç½®systemå¹¶æ¸…é™¤å†å²å¯¹è¯
            modify_system.click(fn=modify_system_session,
                        inputs=[system_input],
                        outputs=[system_state, system_input, chatbot])
            
            video_button.click(fn = human_response, inputs = [chatbot, question_audio, talker_method, 
                                                              voice, rate, volume, pitch, batch_size,
                                                             am, voc, lang, male, 
                                                             inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  
                                                             use_mic_voice, tts_method, character], outputs = [video])
            
        exmaple_setting(asr_method, msg, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_img():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) ä»»æ„å›¾ç‰‡å¯¹è¯"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'):
                with gr.Tabs(elem_id="sadtalker_source_image"):
                        with gr.TabItem('Source image'):
                            with gr.Row():
                                source_image = gr.Image(label="Source image", type="filepath", elem_id="img2img_image", width=512)
                                
                (_, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
                            
                
                    
            # driven_audio = 'answer.wav'           
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                with gr.Tabs(elem_id="text_examples"): 
                    gr.Markdown("## Text Examples")
                    examples =  [
                        ['åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ'],
                        ['å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ'],
                        ['ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ'],
                    ]
                    gr.Examples(
                        examples = examples,
                        inputs = [input_text],
                    )
                with gr.Tabs(elem_id="sadtalker_checkbox"):
                    with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings",
                                        open=False):
                            gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                            with gr.Column(variant='panel'):
                                # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                                # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)

                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                                    preprocess_type = gr.Radio(['crop', 'resize','full', 'extcrop', 'extfull'], value='crop', label='preprocess', info="How to handle input image?")
                                
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                    
                                with gr.Row():
                                    batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")                                               

                with gr.Tabs(elem_id="sadtalker_genearted"):
                    gen_video = gr.Video(label="Generated video", format="mp4")

                submit = gr.Button('ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘', elem_id="sadtalker_generate", variant='primary')
            submit.click(
                fn=Talker_response_img,
                inputs=[question_audio,
                        talker_method, 
                        input_text,
                        voice, rate, volume, pitch,
                        am, voc, lang, male, 
                        inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                        tts_method,
                        source_image, 
                        preprocess_type,
                        is_still_mode,
                        enhancer,
                        batch_size,                            
                        size_of_image,
                        pose_style,
                        facerender,
                        exp_weight,
                        blink_every,
                        fps], 
                outputs=[gen_video]
                )
        
        with gr.Row():
            examples = [
                [
                    'examples/source_image/full_body_2.png', 'SadTalker',
                    'crop',
                    False,
                    False
                ],
                [
                    'examples/source_image/full_body_1.png', 'SadTalker',
                    'full',
                    True,
                    False
                ],
                [
                    'examples/source_image/full4.jpeg', 'SadTalker',
                    'crop',
                    False,
                    True
                ],
            ]
            gr.Examples(examples=examples,
                        inputs=[
                            source_image, talker_method,
                            preprocess_type,
                            is_still_mode,
                            enhancer], 
                        outputs=[gen_video],
                        # cache_examples=True,
                        )
    return inference

def app_vits():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) è¯­éŸ³å…‹éš†"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                (source_image, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text])
                with gr.Tabs():
                    with gr.TabItem('æ•°å­—äººé—®ç­”'):
                        gen_video = gr.Video(label="Generated video", format="mp4", autoplay=False)
                video_button = gr.Button("ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘", variant='primary')
            video_button.click(fn=Talker_response,inputs=[question_audio, talker_method, input_text, voice, rate, volume, pitch, am, voc, lang, male, 
                            inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                            tts_method, batch_size, character],outputs=[gen_video])
        exmaple_setting(asr_method, input_text, character, talker_method, tts_method, voice, llm_method)
    return inference

def app_talk():
    with gr.Blocks(analytics_enabled=False, title = 'Linly-Talker') as inference:
        gr.HTML(get_title("Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker) æ•°å­—äººæ’­æŠ¥"))
        with gr.Row(equal_height=False):
            with gr.Column(variant='panel'): 
                with gr.Tabs():
                    with gr.Tab("å›¾ç‰‡äººç‰©"):
                        source_image = gr.Image(label='Source image', type = 'filepath')
                        
                    with gr.Tab("è§†é¢‘äººç‰©"):
                        source_video = gr.Video(label="Source video")
               
                (_, voice, rate, volume, pitch, 
                am, voc, lang, male, 
                inp_ref, prompt_text, prompt_language, text_language, how_to_cut, use_mic_voice,
                tts_method, batch_size, character, talker_method, asr_method, llm_method)= webui_setting()
        
            with gr.Column(variant='panel'):
                with gr.Tabs():
                    with gr.TabItem('å¯¹è¯'):
                        with gr.Group():
                            question_audio = gr.Audio(sources=['microphone','upload'], type="filepath", label = 'è¯­éŸ³å¯¹è¯')
                            input_text = gr.Textbox(label="Input Text", lines=3)
                            asr_text = gr.Button('è¯­éŸ³è¯†åˆ«ï¼ˆè¯­éŸ³å¯¹è¯åç‚¹å‡»ï¼‰')
                        asr_text.click(fn=Asr,inputs=[question_audio],outputs=[input_text]) 
                with gr.Tabs():
                    with gr.TabItem('SadTalkeræ•°å­—äººå‚æ•°è®¾ç½®'):
                        with gr.Accordion("Advanced Settings",
                                        open=False):
                            gr.Markdown("SadTalker: need help? please visit our [[best practice page](https://github.com/OpenTalker/SadTalker/blob/main/docs/best_practice.md)] for more detials")
                            with gr.Column(variant='panel'):
                                # width = gr.Slider(minimum=64, elem_id="img2img_width", maximum=2048, step=8, label="Manually Crop Width", value=512) # img2img_width
                                # height = gr.Slider(minimum=64, elem_id="img2img_height", maximum=2048, step=8, label="Manually Crop Height", value=512) # img2img_width
                                with gr.Row():
                                    pose_style = gr.Slider(minimum=0, maximum=45, step=1, label="Pose style", value=0) #
                                    exp_weight = gr.Slider(minimum=0, maximum=3, step=0.1, label="expression scale", value=1) # 
                                    blink_every = gr.Checkbox(label="use eye blink", value=True)

                                with gr.Row():
                                    size_of_image = gr.Radio([256, 512], value=256, label='face model resolution', info="use 256/512 model? 256 is faster") # 
                                    preprocess_type = gr.Radio(['crop', 'resize','full'], value='full', label='preprocess', info="How to handle input image?")
                                
                                with gr.Row():
                                    is_still_mode = gr.Checkbox(label="Still Mode (fewer head motion, works with preprocess `full`)")
                                    facerender = gr.Radio(['facevid2vid'], value='facevid2vid', label='facerender', info="which face render?")
                                    
                                with gr.Row():
                                    # batch_size = gr.Slider(label="batch size in generation", step=1, maximum=10, value=1)
                                    fps = gr.Slider(label='fps in generation', step=1, maximum=30, value =20)
                                    enhancer = gr.Checkbox(label="GFPGAN as Face enhancer(slow)")                                               

                with gr.Tabs():
                    gen_video = gr.Video(label="Generated video", format="mp4")

                video_button = gr.Button('ğŸ¬ ç”Ÿæˆæ•°å­—äººè§†é¢‘', elem_id="sadtalker_generate", variant='primary')

                video_button.click(fn=Talker_Say,inputs=[preprocess_type, is_still_mode, enhancer, batch_size, size_of_image,
                                pose_style, facerender, exp_weight, blink_every, fps,
                                source_image, source_video, question_audio, talker_method, input_text, voice, rate, volume, pitch, am, voc, lang, male, 
                                inp_ref, prompt_text, prompt_language, text_language, how_to_cut,  use_mic_voice,
                                tts_method, character],outputs=[gen_video])
            
        with gr.Row():
            with gr.Column(variant='panel'):
                gr.Markdown("## Test Examples")
                gr.Examples(
                    examples = [
                        [
                            'examples/source_image/full_body_2.png',
                            'åº”å¯¹å‹åŠ›æœ€æœ‰æ•ˆçš„æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿ',
                        ],
                        [
                            'examples/source_image/full_body_1.png',
                            'å¦‚ä½•è¿›è¡Œæ—¶é—´ç®¡ç†ï¼Ÿ',
                        ],
                        [
                            'examples/source_image/full3.png',
                            'ä¸ºä»€ä¹ˆæœ‰äº›äººé€‰æ‹©ä½¿ç”¨çº¸è´¨åœ°å›¾æˆ–å¯»æ±‚æ–¹å‘ï¼Œè€Œä¸æ˜¯ä¾èµ–GPSè®¾å¤‡æˆ–æ™ºèƒ½æ‰‹æœºåº”ç”¨ç¨‹åºï¼Ÿ',
                        ],
                    ],
                    fn = Talker_Say,
                    inputs = [source_image, input_text],
                )   
    return inference
            
def asr_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global asr
    if model_name == "Whisper-tiny":
        try:
            asr = WhisperASR('tiny')
            # asr = WhisperASR('Whisper/tiny.pt')
            gr.Info("Whisper-tinyæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Whisper-tinyæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == "Whisper-base":
        try:
            asr = WhisperASR('base')
            # asr = WhisperASR('Whisper/base.pt')
            gr.Info("Whisper-baseæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Whisper-baseæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'FunASR':
        try:
            from ASR import FunASR
            asr = FunASR()
            gr.Info("FunASRæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"FunASRæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    else:
        gr.Warning("æœªçŸ¥ASRæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name

def llm_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global llm
    gemini_apikey = ""
    openai_apikey = ""
    proxy_url = None
    if model_name == 'Linly':
        try:
            llm = llm_class.init_model('Linly', 'Linly-AI/Chinese-LLaMA-2-7B-hf', prefix_prompt=prefix_prompt)
            gr.Info("Linlyæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Linlyæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'Qwen':
        try:
            llm = llm_class.init_model('Qwen', 'Qwen/Qwen-1_8B-Chat', prefix_prompt=prefix_prompt)
            gr.Info("Qwenæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Qwenæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'Qwen2':
        try:
            llm = llm_class.init_model('Qwen2', 'Qwen/Qwen1.5-0.5B-Chat', prefix_prompt=prefix_prompt)
            gr.Info("Qwen2æ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"Qwen2æ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'Gemini':
        if gemini_apikey:
            llm = llm_class.init_model('Gemini', 'gemini-pro', gemini_apikey, proxy_url)
            gr.Info("Geminiæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        else:
            gr.Warning("è¯·å¡«å†™Geminiçš„api_key")
    elif model_name == 'ChatGLM':
        try:
            llm = llm_class.init_model('ChatGLM', 'THUDM/chatglm3-6b', prefix_prompt=prefix_prompt)
            gr.Info("ChatGLMæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"ChatGLMæ¨¡å‹å¯¼å…¥å¤±è´¥ {e}")
    elif model_name == 'ChatGPT':
        if openai_apikey:
            llm = llm_class.init_model('ChatGPT', api_key=openai_apikey, proxy_url=proxy_url, prefix_prompt=prefix_prompt)
        else:
            gr.Warning("è¯·å¡«å†™OpenAIçš„api_key")
    # elif model_name == 'Llama2Chinese':
    #     try:
    #         llm =llm_class.init_model('Llama2Chinese', 'Llama2-chat-13B-Chinese-50W')
    #         gr.Info("Llama2Chineseæ¨¡å‹å¯¼å…¥æˆåŠŸ")
    #     except Exception as e:
    #         gr.Warning(f"Llama2Chineseæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'GPT4Free':
        try:
            llm = llm_class.init_model('GPT4Free', prefix_prompt=prefix_prompt)
            gr.Info("GPT4Freeæ¨¡å‹å¯¼å…¥æˆåŠŸ, è¯·æ³¨æ„GPT4Freeå¯èƒ½ä¸ç¨³å®š")
        except Exception as e:
            gr.Warning(f"GPT4Freeæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    else:
        gr.Warning("æœªçŸ¥LLMæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name
    
def talker_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global talker
    if model_name not in ['SadTalker', 'Wav2Lip', 'ER-NeRF']:
        gr.Warning("å…¶ä»–æ¨¡å‹è¿˜æœªé›†æˆï¼Œè¯·ç­‰å¾…")
    if model_name == 'SadTalker':
        try:
            from TFG import SadTalker
            talker = SadTalker(lazy_load=True)
            gr.Info("SadTalkeræ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning("SadTalkeræ¨¡å‹ä¸‹è½½å¤±è´¥", e)
    elif model_name == 'Wav2Lip':
        try:
            from TFG import Wav2Lip
            talker = Wav2Lip("checkpoints/wav2lip_gan.pth")
            gr.Info("Wav2Lipæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning("Wav2Lipæ¨¡å‹ä¸‹è½½å¤±è´¥", e)
    elif model_name == 'ER-NeRF':
        try:
            from TFG import ERNeRF
            talker = ERNeRF()
            talker.init_model('checkpoints/Obama_ave.pth', 'checkpoints/Obama.json')
            gr.Info("ER-NeRFæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning("ER-NeRFæ¨¡å‹ä¸‹è½½å¤±è´¥", e)
    else:
        gr.Warning("æœªçŸ¥TFGæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name

def tts_model_change(model_name, progress=gr.Progress(track_tqdm=True)):
    global tts
    if model_name == 'Edge-TTS':
        # tts = EdgeTTS()
        if edgetts.network:
            gr.Info("EdgeTTSæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        else:
            gr.Warning("EdgeTTSæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ­£å¸¸è¿æ¥ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨")
    elif model_name == 'PaddleTTS':
        try:
            from TTS import PaddleTTS
            tts = PaddleTTS()
            gr.Info("PaddleTTSæ¨¡å‹å¯¼å…¥æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"PaddleTTSæ¨¡å‹ä¸‹è½½å¤±è´¥ {e}")
    elif model_name == 'GPT-SoVITSå…‹éš†å£°éŸ³':
        try:
            gpt_path = "GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
            sovits_path = "GPT_SoVITS/pretrained_models/s2G488k.pth"
            vits.load_model(gpt_path, sovits_path)
            gr.Info("æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            gr.Warning(f"æ¨¡å‹åŠ è½½å¤±è´¥ {e}")
    else:
        gr.Warning("æœªçŸ¥TTSæ¨¡å‹ï¼Œå¯æissueå’ŒPR æˆ–è€… å»ºè®®æ›´æ–°æ¨¡å‹")
    return model_name

if __name__ == "__main__":
    llm_class = LLM(mode='offline')
    try:
        llm = llm_class.init_model('Qwen', 'Qwen/Qwen-1_8B-Chat', prefix_prompt=prefix_prompt)
        print("Success!!! LLMæ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨Qwenæ¨¡å‹")
    except Exception as e:
        print("Qwen Error: ", e)
        print("å¦‚æœä½¿ç”¨Qwenï¼Œè¯·å…ˆä¸‹è½½Qwenæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    
    try:
        from VITS import *
        vits = GPT_SoVITS()
        print("Success!!! GPT-SoVITSæ¨¡å—åŠ è½½æˆåŠŸï¼Œè¯­éŸ³å…‹éš†é»˜è®¤ä½¿ç”¨GPT-SoVITSæ¨¡å‹")
        # gpt_path = "GPT_SoVITS/pretrained_models/s1bert25hz-2kh-longer-epoch=68e-step=50232.ckpt"
        # sovits_path = "GPT_SoVITS/pretrained_models/s2G488k.pth"
        # vits.load_model(gpt_path, sovits_path)
    except Exception as e:
        print("GPT-SoVITS Error: ", e)
        print("å¦‚æœä½¿ç”¨VITSï¼Œè¯·å…ˆä¸‹è½½GPT-SoVITSæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    
    try:
        talker = SadTalker(lazy_load=True)
        print("Success!!! SadTalkeræ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨SadTalkeræ¨¡å‹")
    except Exception as e:
        print("SadTalker Error: ", e)
        print("å¦‚æœä½¿ç”¨SadTalkerï¼Œè¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")
    
    try:
        from ASR import WhisperASR
        asr = WhisperASR('base')
        print("Success!!! WhisperASRæ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨Whisper-baseæ¨¡å‹")
    except Exception as e:
        print("ASR Error: ", e)
        print("å¦‚æœä½¿ç”¨FunASRï¼Œè¯·å…ˆä¸‹è½½WhisperASRæ¨¡å‹å’Œå®‰è£…ç¯å¢ƒ")
    
    tts = edgetts
    if not tts.network:
        print("EdgeTTSæ¨¡å—åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæ˜¯å¦æ­£å¸¸è¿æ¥ï¼Œå¦åˆ™æ— æ³•ä½¿ç”¨")

    gr.close_all()
    demo_app = app()
    demo_img = app_img()
    demo_multi = app_multi()
    demo_vits = app_vits()
    demo_talk = app_talk()
    demo = gr.TabbedInterface(interface_list = [demo_app, 
                                                demo_img, 
                                                demo_multi, 
                                                demo_vits, 
                                                demo_talk,
                                                ], 
                              tab_names = ["æ–‡æœ¬/è¯­éŸ³å¯¹è¯", 
                                           "ä»»æ„å›¾ç‰‡å¯¹è¯", 
                                           "å¤šè½®GPTå¯¹è¯", 
                                           "è¯­éŸ³å…‹éš†æ•°å­—äººå¯¹è¯", 
                                           "æ•°å­—äººæ–‡æœ¬/è¯­éŸ³æ’­æŠ¥",
                                           ],
                              title = "Linly-Talker WebUI")
    demo.queue()
    demo.launch(server_name=ip, # æœ¬åœ°ç«¯å£localhost:127.0.0.1 å…¨å±€ç«¯å£è½¬å‘:"0.0.0.0"
                server_port=port,
                # ä¼¼ä¹åœ¨Gradio4.0ä»¥ä¸Šç‰ˆæœ¬å¯ä»¥ä¸ä½¿ç”¨è¯ä¹¦ä¹Ÿå¯ä»¥è¿›è¡Œéº¦å…‹é£å¯¹è¯
                # ssl_certfile=ssl_certfile,
                # ssl_keyfile=ssl_keyfile,
                # ssl_verify=False,
                debug=True,
                )